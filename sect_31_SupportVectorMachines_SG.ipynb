{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 31: Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- online-ds-pt-100719\n",
    "- 04/15/20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MOD PROJECT QUESTIONS? (10-15min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  SVM Questions\n",
    "\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To revisit Linear (Algebra) Equations and revisit the relationship between $y=mx+b$ and $y= w^TX+B$.\n",
    "- To understand how support vector machine attempts to separate groups.\n",
    "- Discuss the advantages / disadvantages of SVMs\n",
    "\n",
    "- To understand the math notation of SVMs\n",
    "\n",
    "- Apply SVMs in the SVMs In scikit Learn Lab\n",
    "- Learn about using kernels with SVMs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BLOG POSTS/ARTICLES\n",
    "    - [Towards Data Science - SVM Simply Explained](https://towardsdatascience.com/support-vector-machine-simply-explained-fee28eba5496)\n",
    "- STUDY GROUP RECORDINGS:\n",
    "    - [Support Vector Machines - Victor](https://www.youtube.com/watch?v=_QmnoubpU3Q&list=PLVoXE6pv5LIg4WOllQ4rNPi9BtvtVMb78&index=5)\n",
    "    - [The Kernel Trick - Victor](https://www.youtube.com/watch?v=mnN74NI4Gqk&list=PLVoXE6pv5LIg4WOllQ4rNPi9BtvtVMb78&index=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/fsds_100719_cohort_notes/master/images/i_support_vector_machines.jpg\" width=50%>\n",
    "\n",
    "> Available now for [purchase](https://www.amazon.com/Support-Machines-Network-Machine-Learning/dp/B07XTLT7RL/ref=sr_1_2?dchild=1&keywords=i+support+vector+machines&qid=1579708346&s=apparel&sr=1-2) ðŸ˜‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Supervised learning**\n",
    "- Classification OR regression.\n",
    "- We can sacrifice accuracy to get _better_ boundaries (and protect against outliers)\n",
    "\n",
    "#### Advantages\n",
    "- Good for datasets with more variables than observations\n",
    "- Robust against outliers\n",
    "\n",
    "- Good performance\n",
    "- Good off-the-shelf model in general for several scenarios\n",
    "- Can approximate complex non-linear functions\n",
    "\n",
    "#### Disadvantages\n",
    "- Long training time required\n",
    "- Tuning required to determine optimal kernel for non-linear SVMs\n",
    "\n",
    "#### Requirements\n",
    "- Scaled features\n",
    "- Null values filled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T20:02:13.182410Z",
     "start_time": "2020-04-15T20:02:10.333001Z"
    }
   },
   "outputs": [],
   "source": [
    "from fsds_100719.imports import *\n",
    "import sklearn.datasets as datasets\n",
    "\n",
    "# Loading in an example dataset\n",
    "plt.style.use('seaborn-notebook')\n",
    "iris = datasets.load_iris()\n",
    "iris_data = iris.data\n",
    "\n",
    "# Only use two targets/classifications\n",
    "iris_targets = np.where(iris.target == 0, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we separate the data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T20:02:13.368087Z",
     "start_time": "2020-04-15T20:02:13.184538Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting different points\n",
    "def plot_iris():\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(x=iris_data[:,2], y=iris_data[:,1], c=iris_targets)\n",
    "    return fig,ax\n",
    "fig,ax= plot_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Look at these lines, which is a better model?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T20:02:13.564701Z",
     "start_time": "2020-04-15T20:02:13.370891Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting lines to separate points\n",
    "fig,ax=plot_iris()\n",
    "l1 = np.array([[1,2],[6.5,4.5]])\n",
    "ax.plot(l1[:,0], l1[:,1], linestyle='--',label='line 1')\n",
    "l2 = np.array([[2,2],[3.5,4.5]])\n",
    "ax.plot(l2[:,0], l2[:,1], linestyle='--',label='line 2')\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### A1: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Line 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Why is it better? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### A2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Maximizes the distance between the data points and the line (the margin).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy isn't everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Could say each line classifies the same (accuracy), so which of the following would be better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T20:02:13.778380Z",
     "start_time": "2020-04-15T20:02:13.566043Z"
    }
   },
   "outputs": [],
   "source": [
    "# Small margin\n",
    "margin_small = np.array([0.2,0])\n",
    "l2_margin_pos_small = l2 + margin_small\n",
    "l2_margin_neg_small = l2 - margin_small\n",
    "\n",
    "## Large margin\n",
    "margin_larger = np.array([0.5,0])\n",
    "l2_margin_pos_big = l2 + margin_larger\n",
    "l2_margin_neg_big = l2 - margin_larger\n",
    "\n",
    "# Plotting different points\n",
    "fig,ax = plot_iris()\n",
    "# Plotting lines to separate points\n",
    "ax.plot(l2[:,0], l2[:,1], linestyle='-', label='Decision Boundary')\n",
    "\n",
    "# Plot with margins\n",
    "ax.plot(l2_margin_pos_small[:,0], l2_margin_pos_small[:,1], \n",
    "        linestyle='--', color='orange', label='Small Margin')\n",
    "ax.plot(l2_margin_neg_small[:,0], l2_margin_neg_small[:,1], linestyle='--', color='orange')\n",
    "\n",
    "ax.plot(l2_margin_pos_big[:,0], l2_margin_pos_big[:,1], \n",
    "        linestyle='--', color='red',label='Large Margin')\n",
    "ax.plot(l2_margin_neg_big[:,0], l2_margin_neg_big[:,1], linestyle='--', color='red')\n",
    "ax.legend()\n",
    "ax.set_xlim(1,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---#### Line Definitions/Legend\n",
    "- Blue = Model\n",
    "- Possible Margins:\n",
    "    - orange\n",
    "    - red\n",
    "    \n",
    "- Left and Right margins are called negative and positive hyperplanes--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: Which margin is better? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T20:02:13.917711Z",
     "start_time": "2020-04-15T20:02:13.779811Z"
    }
   },
   "outputs": [],
   "source": [
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The larger margin. \n",
    "- The smaller the margin the more you're assuming your model is correct and the more likely it will be over-fit and not generalize well.\n",
    "<!--- > <img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-introduction-to-support-vector-machines-online-ds-pt-100719/master/images/new_SVM_test2.png\"> --->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So what is a \"support vector\" anyway?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperplane: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a $n$-dimensional line.\n",
    "- These hyperplanes are defined by two terms: $w_T$ and $b$. \n",
    "    - $w_T$ term is called the **weight vector** and contains the weights that are used in the classification.\n",
    "    - $b$ term is called the **bias** and functions as an offset term. \n",
    "        - If there were no bias term, the hyperplane would always go through the origin which would not be very generalizable! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sidebar: Linear Equation Notation vs Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **in Linear Regression, we predict $y$ using 2 parameters, m (slope) + b(intercept/constant):**\n",
    "$$ \\large y = mx+b $$\n",
    "where: \n",
    "- $x$ = input data for modeling\n",
    "- $y$ = model] predictions\n",
    "- $m$ = slope\n",
    "- $b$ = intercept\n",
    "\n",
    ">**In Linear Model Formulas, terminology/notation changes:**\n",
    "- slopes $(m)$ becomes **weights ($w$)**\n",
    "- constants $b$ becomes **biases ($b$)**\n",
    "$$ \\large y =  XW^T+B $$\n",
    "- $x$ = input data for modeling\n",
    "- $y$ = model] predictions\n",
    "- $w$ is the weight (slope)\n",
    "- $b$ is the bias (constant)\n",
    " <img src=\"https://raw.githubusercontent.com/jirvingphd/fsds_100719_cohort_notes/master/images/linear _model_multi_inputs_and_outputs.png\" width=40%>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/fsds_100719_cohort_notes/master/images/linear_model_multi_inputs.png\" width=40%>\n",
    "\n",
    "<!---<img src=\"https://raw.githubusercontent.com/jirvingphd/fsds_100719_cohort_notes/master/images/linear_model_multi_inputs_arrows.png\" width=40%>\n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminology Continued "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision boundary: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The hyperplane that divides/separates the classes\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-introduction-to-support-vector-machines-online-ds-pt-100719/master/images/new_SVM_2.png\" width=30%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The distance between the decision boundary and the closes datapoints\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-introduction-to-support-vector-machines-online-ds-pt-100719/master/images/new_SVM_4.png\" width=30%>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"Support vectors are the data points nearest to the hyperplane, the points of a data set that, if removed, would alter the position of the dividing hyperplane. Because of this, they can be considered the critical elements of a data set.\" - [KDnuggets - SVM simple explanation](https://www.kdnuggets.com/2016/07/support-vector-machines-simple-explanation.html) \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-introduction-to-support-vector-machines-online-ds-pt-100719/master/images/new_SVM_fin.png\" width=30%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positive/Negative Hyperplanes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: For SVMs, we do not represent our classes as 0 and 1, instead we use -1 and +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Positive Hyperplane:\n",
    "    - The line defined by the support vectors to the right (+) of the decision boundary\n",
    "    $$ b + w_Tx_{pos} =1$$\n",
    "\n",
    "    \n",
    "- Negative Hyperplane:\n",
    "    - The line defined support vector to the left (-) of the decision boundary\n",
    "    $$ b + w_Tx_{neg} =-1$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine - Max-Margin  Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Our goal is to **maximize the separation between the two hyperplanes.**\n",
    "\n",
    "- To do this, first subtract the negative hyperplane's equation from the positive hyperplane's equation:\n",
    "\n",
    "$$ \\large w_T(x_{pos}-x_{neg}) = 2$$\n",
    "\n",
    "- Normalize $w_T$ by dividing both sides of the equation by its norm, $||w||$\n",
    "    - Note: $ || w ||= \\sqrt{\\sum^m_{j-1}w_j^2} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **The equation becomes:**\n",
    "$$ \\large \\dfrac{w_T(x_{pos}-x_{neg})}{\\lVert w \\rVert} = \\dfrac{2}{\\lVert w \\rVert}$$\n",
    "\n",
    "- The left side of the equation = the distance between the positive and negative hyperplanes. (This is the **margin**)\n",
    "\n",
    "- The objective of the SVM is then maximizing $\\dfrac{2}{\\lVert w \\rVert}$ under the constraint/requirement that the samples are classified correctly. \n",
    "\n",
    "\n",
    "- Note that maximizing $\\dfrac{2}{\\lVert w \\rVert}$ means we're minimizing $\\lVert w \\rVert$, or\n",
    "    - as is done in practice because it seems to be easier to be minimized, $\\dfrac{1}{2}\\lVert w \\rVert^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Contraints expressed mathematically:\n",
    "- $ b + w_Tx^{(i)} \\geq 1$  if $y ^{(i)} = 1$\n",
    "- $ b + w_Tx^{(i)} \\leq -1$  if $y ^{(i)} = -1$<br>\n",
    "For $i= 1,\\ldots ,N$\n",
    "> These equations say that:\n",
    "- all negative samples should fall on the left side of the negative hyperplane\n",
    "- whereas all the positive samples should fall on the right of the positive hyperplane. \n",
    "\n",
    "<!---This can also be written in one line as follows:\n",
    "\n",
    "$y ^{(i)} (b + w_Tx^{(i)} )\\geq 1$  for each $i$\n",
    "--->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But what if my data isn't easily separable?\n",
    "- When does maximizing the margin cause problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/learn-co-students/dsc-introduction-to-support-vector-machines-online-ds-pt-100719/master/images/new_SVM_C.png\">\n",
    "\n",
    "- So Where do we go from here?/ What can we do about this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Types of SVM Max-Margin Classifier\n",
    "\n",
    "\n",
    "- Two kinds of max-margin classifiers:\n",
    "    - hard margin = no errors whatsoever\n",
    "    - soft margin = allows for errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Soft-Margin Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The linear constraints need to be relaxed for data that are not linearly separable.\n",
    "- We do this by adding slack variables $\\xi$ to our margins. \n",
    "    - By adding some slack ( allowance for misclassification), we improve model generalizability while sacrificing accuracy\n",
    "> Soft Margine Constraints:\n",
    ">- $ b + w_Tx^{(i)} \\geq 1-\\xi^{(i)}$  if $y ^{(i)} = 1$\n",
    ">- $ b + w_Tx^{(i)} \\leq -1+\\xi^{(i)}$  if $y ^{(i)} = -1$<br>For $i= 1,\\ldots ,N$\n",
    "\n",
    "\n",
    "- The objective function (AKA the function you want to minimize) is \n",
    "\n",
    " $$\\dfrac{1}{2}\\lVert w \\rVert^2+ C(\\sum_i \\xi^{(i)})$$\n",
    "- The hyperparameter $C$ is used to define how much slack is allowed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Hyperparameter $C$\n",
    "\n",
    "Q: What happens if $C$ is very large? (What errors do we care about more?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "        \n",
    "        \n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-students/dsc-introduction-to-support-vector-machines-online-ds-pt-100719/master/images/new_SVM_C.png\">\n",
    "\n",
    "- When C is large, we get left figure. \n",
    "    - Misclassifications are heavily punished.\n",
    "- When C is small, we get right figure.\n",
    "    - Misclassifications are accepted to maximize overall margin\n",
    "\n",
    "<!---\n",
    "## Classification error\n",
    "\n",
    "We minimize the two kinds of error:\n",
    " - how many are \"misclassified\" \n",
    " - how many are in bad boundary (within margin)\n",
    " \n",
    "\n",
    "This gives us something like this:\n",
    "\n",
    "$Error_{total} = Error_{classification} + Error_{margin}$\n",
    " \n",
    "\n",
    "- We start from our margin to count the error (instead of the center)\n",
    "\n",
    "## Margin error\n",
    "\n",
    "- $E = |W|^2 = ||W_1||+||W_2|| + â€¦$ \n",
    "    + big vs small margin (we want very large)\n",
    "- $M = \\frac{2}{||W||}$ \n",
    "    + inverse proportion, large margin â†’ small error\n",
    "\n",
    "Turns out to the same as the L2 Regularization!\n",
    "\n",
    "## Gradient Descent to minimize--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACTIVITY: SVM in scikit-learn  Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In Repo Folder > labs_from_class > sect_31_SVM > svm_sklearn_laba_SG.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# The Kernel Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> #### Q: But what do we do when don't have linearly separable data??\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-students/dsc-the-kernel-trick-online-ds-pt-100719/master/images/new_SVM_nonlin.png\" width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> #### A: When a simple model isn't good enough, extend to higher dimensions.\n",
    "\n",
    "> Use a kernel function to transform the data into a higher dimension and then separate the data in the higher dimension.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-students/dsc-the-kernel-trick-online-ds-pt-100719/master/images/new_SVM_kernel.png\" width=60%>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Kernel Function Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- **Blog Posts/Articles**\n",
    "    - [Blog Post on Hyperparameter Tuning](https://medium.com/all-things-ai/in-depth-parameter-tuning-for-svc-758215394769)\n",
    "    - \n",
    "\n",
    "- **Scikit-Learn Documentation:**\n",
    "    - [SVMs for Classification](https://scikit-learn.org/stable/modules/svm.html#classification)\n",
    "    - [Scikit-Learn Docs: Kernel Functions](https://scikit-learn.org/stable/modules/svm.html#svm-kernels)\n",
    "\n",
    "        - [Scikit-Learn SVM Math Equations](https://scikit-learn.org/stable/modules/svm.html#mathematical-formulation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T20:27:38.825498Z",
     "start_time": "2020-04-15T20:27:38.816259Z"
    },
    "hidden": true
   },
   "source": [
    "## Kernel Functions for sklearn's `SVC`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Probably the most important information worth reviewing is some of the various kernel functions that you can apply.\n",
    "\n",
    "\n",
    "- Recall that in general, `C` is the parameter for balancing standard accuracy metrics for tuning classifiers versus the decision boundary distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T20:27:38.825498Z",
     "start_time": "2020-04-15T20:27:38.816259Z"
    },
    "hidden": true
   },
   "source": [
    "#### 1. Radial Basis Functions (RBF) Kernel\n",
    "$$\\exp{(-\\gamma \\lVert  x -  x' \\rVert^2)} $$\n",
    "- [Hyperparameters](https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html):\n",
    "    1. `C`\n",
    "    2. $\\gamma$, which can be specified using `gamma` in scikit-learn (default='auto')\n",
    "        - Large gamma = overfitting\n",
    "        - Small gamma = underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 2. Polynomial Kernel\n",
    "$$(\\gamma \\langle  x -  x' \\rangle+r)^d $$\n",
    "- Hyperparameters:\n",
    "    1. $\\gamma$, which can be specified using `gamma` in scikit-learn\n",
    "    2. $r$, which can be specified using `coef0` in scikit-learn\n",
    "    3. $d$, which can be specified using `degree` in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 3. Sigmoid Kernel\n",
    "$$\\tanh ( \\gamma\\langle  x -  x' \\rangle+r) $$\n",
    "- Hyperparameters:\n",
    "    1. $\\gamma$, which can be specified using `gamma` in scikit-learn\n",
    "    2. $r$, which can be specified using `coef0` in scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Other Types of SVC Models in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### NuSVC\n",
    "- Like SVC, NuSVC but adds a parameter $v$ (see hyperparameters below). \n",
    "- NuSVC implements \"one-against-one\" approach when number of classes >2\n",
    "    - when there are n classes, $\\dfrac{n*(n-1)}{2}$ classifiers are created, and each one classifies samples in 2 classes. \n",
    "\n",
    "\n",
    "- Hyperparameters:\n",
    "    - $v$: controls the number of support vectors and training errors\n",
    "        - creates upper bound on training errors\n",
    "        - creates lower bound on support vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### LinearSVC\n",
    "- Like SVC but LinearSVC implements \"one-vs-rest\" so when there are $n$ classes, just $n$ classifiers are created and each one classifies samples in 2 classes (class of interest and all others)\n",
    "- LinearSVC generates more classifiers, so LinearSVC tends to scale better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### But there's sooo many options?!?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- To keep it simple:\n",
    "    - If your data looks linearly separable, use linear SVC, otherwise use RBF\n",
    "    - But I am sure you can find some technical papers discussing the tradeoffs of all of the options.\n",
    "     - (I wasn't able to find a good high-level comparison or use-case type of article.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Note re: predictions/probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> You can make predictions using support vector machines. The SVC decision function gives a probability score per class. However, this is not done by default. You'll need to set the `probability` argument equal to `True`. Scikit-learn internally performs cross-validation to compute the probabilities, so you can expect that setting `probability` to `True` makes the calculations longer. For large datasets, computation can take considerable time to execute.\n",
    "\n",
    "- In other words:\n",
    "    - If you want to get the probabiltiies (`.predict_proba`) for ROC AUC, you would have to instantiate your SVC with the parameter `SVC(probability=True)`\n",
    "    - https://www.kaggle.com/c/home-credit-default-risk/discussion/63499"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Scikit Learn Example of RBF Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T20:45:16.931594Z",
     "start_time": "2020-04-15T20:45:12.568100Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Utility function to move the midpoint of a colormap to be around\n",
    "# the values of interest.\n",
    "\n",
    "class MidpointNormalize(Normalize):\n",
    "\n",
    "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n",
    "        self.midpoint = midpoint\n",
    "        Normalize.__init__(self, vmin, vmax, clip)\n",
    "\n",
    "    def __call__(self, value, clip=None):\n",
    "        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n",
    "        return np.ma.masked_array(np.interp(value, x, y))\n",
    "\n",
    "# #############################################################################\n",
    "# Load and prepare data set\n",
    "#\n",
    "# dataset for grid search\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Dataset for decision function visualization: we only keep the first two\n",
    "# features in X and sub-sample the dataset to keep only 2 classes and\n",
    "# make it a binary classification problem.\n",
    "\n",
    "X_2d = X[:, :2]\n",
    "X_2d = X_2d[y > 0]\n",
    "y_2d = y[y > 0]\n",
    "y_2d -= 1\n",
    "\n",
    "# It is usually a good idea to scale the data for SVM training.\n",
    "# We are cheating a bit in this example in scaling all of the data,\n",
    "# instead of fitting the transformation on the training set and\n",
    "# just applying it on the test set.\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_2d = scaler.fit_transform(X_2d)\n",
    "\n",
    "# #############################################################################\n",
    "# Train classifiers\n",
    "#\n",
    "# For an initial search, a logarithmic grid with basis\n",
    "# 10 is often helpful. Using a basis of 2, a finer\n",
    "# tuning can be achieved but at a much higher cost.\n",
    "\n",
    "C_range = np.logspace(-2, 10, 13)\n",
    "gamma_range = np.logspace(-9, 3, 13)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(\"The best parameters are %s with a score of %0.2f\"\n",
    "      % (grid.best_params_, grid.best_score_))\n",
    "\n",
    "# Now we need to fit a classifier for all parameters in the 2d version\n",
    "# (we use a smaller set of parameters here because it takes a while to train)\n",
    "\n",
    "C_2d_range = [1e-2, 1, 1e2]\n",
    "gamma_2d_range = [1e-1, 1, 1e1]\n",
    "classifiers = []\n",
    "for C in C_2d_range:\n",
    "    for gamma in gamma_2d_range:\n",
    "        clf = SVC(C=C, gamma=gamma)\n",
    "        clf.fit(X_2d, y_2d)\n",
    "        classifiers.append((C, gamma, clf))\n",
    "\n",
    "# #############################################################################\n",
    "# Visualization\n",
    "#\n",
    "# draw visualization of parameter effects\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "xx, yy = np.meshgrid(np.linspace(-3, 3, 200), np.linspace(-3, 3, 200))\n",
    "for (k, (C, gamma, clf)) in enumerate(classifiers):\n",
    "    # evaluate decision function in a grid\n",
    "    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # visualize decision function for these parameters\n",
    "    plt.subplot(len(C_2d_range), len(gamma_2d_range), k + 1)\n",
    "#     plt.title(\"gamma=10^%d, C=10^%d\" % (np.log10(gamma), np.log10(C)),\n",
    "#               size='medium')\n",
    "    plt.title(f\"gamma={gamma}, C={C}\")\n",
    "\n",
    "    # visualize parameter's effect on decision function\n",
    "    plt.pcolormesh(xx, yy, -Z, cmap=plt.cm.RdBu)\n",
    "    plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y_2d, cmap=plt.cm.RdBu_r,\n",
    "                edgecolors='k')\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.axis('tight')\n",
    "\n",
    "scores = grid.cv_results_['mean_test_score'].reshape(len(C_range),\n",
    "                                                     len(gamma_range))\n",
    "\n",
    "# Draw heatmap of the validation accuracy as a function of gamma and C\n",
    "#\n",
    "# The score are encoded as colors with the hot colormap which varies from dark\n",
    "# red to bright yellow. As the most interesting scores are all located in the\n",
    "# 0.92 to 0.97 range we use a custom normalizer to set the mid-point to 0.92 so\n",
    "# as to make it easier to visualize the small variations of score values in the\n",
    "# interesting range while not brutally collapsing all the low score values to\n",
    "# the same color.\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)\n",
    "plt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot,\n",
    "           norm=MidpointNormalize(vmin=0.2, midpoint=0.92))\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel('C')\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(len(gamma_range)), gamma_range, rotation=45)\n",
    "plt.yticks(np.arange(len(C_range)), C_range)\n",
    "plt.title('Validation accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "279px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "3176a925b1da4812b6b9a08eaa9a47e5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "6cdfcbcf7d9e46788a093ca3e89884cf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7f45943170bc4076966bdad19d1993bb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "907fbf82d8fd4b0aa4bd1e20f61cd86c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "b1e379b187b447b6a38deec60ef877de": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b2c21f049d1c447ab6bdb2eb160fc01c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_b1e379b187b447b6a38deec60ef877de",
       "style": "IPY_MODEL_907fbf82d8fd4b0aa4bd1e20f61cd86c",
       "value": " 0/6497 [00:00&lt;?, ?it/s]"
      }
     },
     "b83511e5360247068a26da1552bce1b5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_fca5f875a10349d59c154faa80e49698",
        "IPY_MODEL_b2c21f049d1c447ab6bdb2eb160fc01c"
       ],
       "layout": "IPY_MODEL_7f45943170bc4076966bdad19d1993bb"
      }
     },
     "fca5f875a10349d59c154faa80e49698": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "description": "  0%",
       "layout": "IPY_MODEL_6cdfcbcf7d9e46788a093ca3e89884cf",
       "max": 6497,
       "style": "IPY_MODEL_3176a925b1da4812b6b9a08eaa9a47e5"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
