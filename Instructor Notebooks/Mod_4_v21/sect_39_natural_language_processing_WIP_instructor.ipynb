{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 40: Foundations of Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regex : wtf?!\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Introduce the field of Natural Language Processing\n",
    "- Learn about the extensive preprocessing involved with text data\n",
    "- Touch on some things like sentiment analysis \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP & Word Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_Natural Language Processing_**, or **_NLP_**, is the study of how computers can interact with humans through the use of human language.  Although this is a field that is quite important to Data Scientists, it does not belong to Data Science alone.  NLP has been around for quite a while, and sits at the intersection of *Computer Science*, *Artificial Intelligence*, *Linguistics*, and *Information Theory*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where is NLP Used?\n",
    "- Reviews (i.e. Amazon)\n",
    "- Stock market trading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Demonstration:**\n",
    "    - [Google Duplex AI Assistant](https://youtu.be/D5VN56jQMWM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preparing text data requires more processing than normal data.\n",
    "- In addition to cleaning the text itself to remove meaningless words, we have to convert our text data into numeric form for our machine learning models to analyze.\n",
    "- Text data must be cleaned and vectorized before we can use it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Vocabulary\n",
    "- Corpus\n",
    "    - Body of text\n",
    "    \n",
    "- Bag of Words\n",
    "    - Collection of all words from a corpus.\n",
    "\n",
    "    \n",
    "- Stopwords\n",
    "- Tokenization\n",
    "    - Separating long strings into single-words in a list.\n",
    "    \n",
    "- Stemming \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-students/dsc-nlp-and-word-vectorization-online-ds-ft-100719/master/images/new_stemming.png\" width=40%>\n",
    "\n",
    "- Lemmatization\n",
    "\n",
    "|   Word   |  Stem | Lemma |\n",
    "|:--------:|:-----:|:-----:|\n",
    "|  Studies | Studi | Study |\n",
    "| Studying | Study | Study |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context-Free Grammers and POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-context-free-grammars-and-POS-tagging-online-ds-ft-100719/master/images/new_LevelsOfLanguage-Graph.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Syntax and Meaning Can be Difficult for Computers \n",
    "\n",
    "In English, sentences consist of a **_Noun Phrase_** followed by a **_Verb Phrase_**, which may optionally be followed by a **_Prepositional Phrase_**.\n",
    "\n",
    "This ***seems simple, but it gets more tricky*** when we realize that there is a recursive structure to these phrases.\n",
    "\n",
    "- A noun phrase may consist of multiple smaller noun phrases, and in some cases, even a verb phrase. \n",
    "- Similarly, a verb phrase can consist of multiple smaller verb phrases and noun phrases, which can themselves be made up of smaller noun phrases and verb phrases. \n",
    "\n",
    "\n",
    "This leads levels of **_ambiguity_** that can be troublesome for computers. NLTK's documentation explains this by examining the classic Groucho Marx joke:\n",
    "\n",
    "> ***\"While hunting in Africa, I shot an elephant in my pajamas. How he got into my pajamas, I don't know.\"***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-context-free-grammars-and-POS-tagging-online-ds-ft-100719/master/images/parse_tree.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering for Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Do we remove stop words or not?    \n",
    "* Do we stem or lemmatize our text data, or leave the words as is?   \n",
    "* Is basic tokenization enough, or do we need to support special edge cases through the use of regex?  \n",
    "* Do we use the entire vocabulary, or just limit the model to a subset of the most frequently used words? If so, how many?  \n",
    "* Do we engineer other features, such as bigrams, or POS tags, or Mutual Information Scores?   \n",
    "* What sort of vectorization should we use in our model? Boolean Vectorization? Count Vectorization? TF-IDF? More advanced vectorization strategies such as Word2Vec?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:18:01.020482Z",
     "start_time": "2020-02-05T17:17:56.440354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fsds_1007219  v0.7.21 loaded.  Read the docs: https://fsds.readthedocs.io/en/latest/ \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_71f3a7d2_9580_11ea_a6ce_4865ee12e626\" ><caption>Loaded Packages and Handles</caption><thead>    <tr>        <th class=\"col_heading level0 col0\" >Handle</th>        <th class=\"col_heading level0 col1\" >Package</th>        <th class=\"col_heading level0 col2\" >Description</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                                <td id=\"T_71f3a7d2_9580_11ea_a6ce_4865ee12e626row0_col0\" class=\"data row0 col0\" >dp</td>\n",
       "                        <td id=\"T_71f3a7d2_9580_11ea_a6ce_4865ee12e626row0_col1\" class=\"data row0 col1\" >IPython.display</td>\n",
       "                        <td id=\"T_71f3a7d2_9580_11ea_a6ce_4865ee12e626row0_col2\" class=\"data row0 col2\" >Display modules with helpful display and clearing commands.</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_71f3a7d2_9580_11ea_a6ce_4865ee12e626row1_col0\" class=\"data row1 col0\" >fs</td>\n",
       "                        <td id=\"T_71f3a7d2_9580_11ea_a6ce_4865ee12e626row1_col1\" class=\"data row1 col1\" >fsds_100719</td>\n",
       "                        <td id=\"T_71f3a7d2_9580_11ea_a6ce_4865ee12e626row1_col2\" class=\"data row1 col2\" >Custom data science bootcamp student package</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_71f3a7d2_9580_11ea_a6ce_4865ee12e626row2_col0\" class=\"data row2 col0\" >mpl</td>\n",
       "                        <td id=\"T_71f3a7d2_9580_11ea_a6ce_4865ee12e626row2_col1\" class=\"data row2 col1\" >matplotlib</td>\n",
       "                        <td id=\"T_71f3a7d2_9580_11ea_a6ce_4865ee12e626row2_col2\" class=\"data row2 col2\" >Matplotlib's base OOP module with formatting artists</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_71f3a7d2_9580_11ea_a6ce_4865ee12e626row3_col0\" class=\"data row3 col0\" >plt</td>\n",
       "                        <td id=\"T_71f3a7d2_9580_11ea_a6ce_4865ee12e626row3_col1\" class=\"data row3 col1\" >matplotlib.pyplot</td>\n",
       "                        <td id=\"T_71f3a7d2_9580_11ea_a6ce_4865ee12e626row3_col2\" class=\"data row3 col2\" >Matplotlib's matlab-like plotting module</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_71f3a7d2_9580_11ea_a6ce_4865ee12e626row4_col0\" class=\"data row4 col0\" >np</td>\n",
       "                        <td id=\"T_71f3a7d2_9580_11ea_a6ce_4865ee12e626row4_col1\" class=\"data row4 col1\" >numpy</td>\n",
       "                        <td id=\"T_71f3a7d2_9580_11ea_a6ce_4865ee12e626row4_col2\" class=\"data row4 col2\" >scientific computing with Python</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_71f3a7d2_9580_11ea_a6ce_4865ee12e626row5_col0\" class=\"data row5 col0\" >pd</td>\n",
       "                        <td id=\"T_71f3a7d2_9580_11ea_a6ce_4865ee12e626row5_col1\" class=\"data row5 col1\" >pandas</td>\n",
       "                        <td id=\"T_71f3a7d2_9580_11ea_a6ce_4865ee12e626row5_col2\" class=\"data row5 col2\" >High performance data structures and tools</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_71f3a7d2_9580_11ea_a6ce_4865ee12e626row6_col0\" class=\"data row6 col0\" >sns</td>\n",
       "                        <td id=\"T_71f3a7d2_9580_11ea_a6ce_4865ee12e626row6_col1\" class=\"data row6 col1\" >seaborn</td>\n",
       "                        <td id=\"T_71f3a7d2_9580_11ea_a6ce_4865ee12e626row6_col2\" class=\"data row6 col2\" >High-level data visualization library based on matplotlib</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1c16891358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Pandas .iplot() method activated.\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U fsds_100719\n",
    "from fsds_100719.imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MacBeth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# macbeth = requests.get('http://www.gutenberg.org/cache/epub/2264/pg2264.txt').text\n",
    "# macbeth[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(macbeth[14000:18000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = macbeth.split('David Reed')[-1]\n",
    "# print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Capstone Excerpt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:18:35.144097Z",
     "start_time": "2020-02-05T17:18:34.711735Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>id_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Twitter Media Studio</td>\n",
       "      <td>https://t.co/EVAEYD1AgV</td>\n",
       "      <td>01-01-2020 03:12:07</td>\n",
       "      <td>25016</td>\n",
       "      <td>108830</td>\n",
       "      <td>False</td>\n",
       "      <td>1212209862094012416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>HAPPY NEW YEAR!</td>\n",
       "      <td>01-01-2020 01:30:35</td>\n",
       "      <td>85409</td>\n",
       "      <td>576045</td>\n",
       "      <td>False</td>\n",
       "      <td>1212184310389850119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Our fantastic First Lady! https://t.co/6iswto4WDI</td>\n",
       "      <td>01-01-2020 01:22:28</td>\n",
       "      <td>27567</td>\n",
       "      <td>132633</td>\n",
       "      <td>False</td>\n",
       "      <td>1212182267113680896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @DanScavino: https://t.co/CJRPySkF1Z</td>\n",
       "      <td>01-01-2020 01:18:47</td>\n",
       "      <td>10796</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1212181341078458369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @SenJohnKennedy: I think Speaker Pelosi is ...</td>\n",
       "      <td>01-01-2020 01:17:43</td>\n",
       "      <td>8893</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1212181071988703232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 source                                               text  \\\n",
       "0  Twitter Media Studio                            https://t.co/EVAEYD1AgV   \n",
       "1    Twitter for iPhone                                    HAPPY NEW YEAR!   \n",
       "2    Twitter for iPhone  Our fantastic First Lady! https://t.co/6iswto4WDI   \n",
       "3    Twitter for iPhone            RT @DanScavino: https://t.co/CJRPySkF1Z   \n",
       "4    Twitter for iPhone  RT @SenJohnKennedy: I think Speaker Pelosi is ...   \n",
       "\n",
       "            created_at  retweet_count  favorite_count is_retweet  \\\n",
       "0  01-01-2020 03:12:07          25016          108830      False   \n",
       "1  01-01-2020 01:30:35          85409          576045      False   \n",
       "2  01-01-2020 01:22:28          27567          132633      False   \n",
       "3  01-01-2020 01:18:47          10796               0       True   \n",
       "4  01-01-2020 01:17:43           8893               0       True   \n",
       "\n",
       "                id_str  \n",
       "0  1212209862094012416  \n",
       "1  1212184310389850119  \n",
       "2  1212182267113680896  \n",
       "3  1212181341078458369  \n",
       "4  1212181071988703232  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/jirvingphd/capstone-project-using-trumps-tweets-to-predict-stock-market/master/data/trump_tweets_12012016_to_01012020.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:18:03.952611Z",
     "start_time": "2020-02-05T17:18:03.945919Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://t.co/EVAEYD1AgV',\n",
       " 'HAPPY NEW YEAR!',\n",
       " 'Our fantastic First Lady! https://t.co/6iswto4WDI',\n",
       " 'RT @DanScavino: https://t.co/CJRPySkF1Z',\n",
       " 'RT @SenJohnKennedy: I think Speaker Pelosi is having 2nd thoughts about impeaching the President. The Senate should get back to work on USM‚Ä¶',\n",
       " 'Thank you Steve. The greatest Witch Hunt in U.S. history! https://t.co/I3bSNVp6gC',\n",
       " 'RT @ThisWeekABC: Sen. Ron Johnson says charges against Pres. Trump are \"pretty thin gruel\" and Speaker Nancy Pelosi\\'s decision to withhold‚Ä¶',\n",
       " \"RT @SenJohnKennedy: The Senate needs to reauthorize the Violence Against Women Act and I am proud to cosponsor @SenJoniErnst's bill that g‚Ä¶\",\n",
       " 'RT @LindseyGrahamSC: To our Iraqi allies:This is your moment to convince the American people the US-Iraq relationship is meaningful to yo‚Ä¶',\n",
       " 'RT @LindseyGrahamSC: President Trump unlike President Obama will hold you accountable for threats against Americans and hit you where it‚Ä¶']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = list(df['text'].values)\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Tweet Natural Language Processing\n",
    "\n",
    "To prepare Donal Trump's tweets for modeling, **it is essential to preprocess the text** and simplify its contents.\n",
    "<br><br>\n",
    "1. **At a minimum, things like:**\n",
    "    - punctuation\n",
    "    - numbers\n",
    "    - upper vs lowercase letters<br>\n",
    "    ***must*** be addressed before any initial analyses. I refer tho this initial cleaning as **\"minimal cleaning\"** of the text content<br>\n",
    "    \n",
    "> Version 1 of the tweet processing removes these items, as well as the removal of any urls in a tweet. The resulting data column is referred to here as \"content_min_clean\".\n",
    "\n",
    "<br><br>\n",
    "2. It is **always recommended** that go a step beyond this and<br> remove **commonly used words that contain little information** <br>for our machine learning algorithms. Words like: (the,was,he,she, it,etc.)<br> are called **\"stopwords\"**, and it is critical to address them as well.\n",
    "\n",
    "> Version 2 of the tweet processing removes these items and the resulting data column is referred here as `cleaned_stopped_content`\n",
    "\n",
    "<br>\n",
    "\n",
    "3. Additionally, many analyses **need the text tokenzied** into a list of words<br> and not in a natural sentence format. Instead, they are a list of words (**tokens**) separated by \",\", which tells the algorithm what should be considered one word.<br><br>For the tweet processing, I used a version of tokenization, called `regexp_tokenziation` <br>which uses pattern of letters and symbols (the `expression`) <br>that indicate what combination of alpha numeric characters should be considered a single token.<br><br>The pattern I used was `\"([a-zA-Z]+(?:'[a-z]+)?)\"`, which allows for words such as \"can't\" that contain \"'\" in the middle of word. This processes was actually applied in order to process Version 1 and 2 of the Tweets, but the resulting text was put back into sentence form. \n",
    "\n",
    "> Version 3 of the tweets keeps the text in their regexp-tokenized form and is reffered to as `cleaned_stopped_tokens`\n",
    "<br>\n",
    "\n",
    "4. While not always required, it is often a good idea to reduce similar words down to a shared core.\n",
    "There are often **multiple variants of the same word with the same/simiar meaning**,<br> but one may plural **(i.e. \"democrat\" and \"democrats\")**, or form of words is different **(i.e. run, running).**<br> Simplifying words down to the basic core word (or word *stem*) is referred to as **\"stemming\"**. <br><br> A more advanced form of this also understands things like words that are just in a **different tense** such as  i.e.  **\"ran\", \"run\", \"running\"**. This process is called  **\"lemmatization**, where the words are reduced to their simplest form, called \"**lemmas**\"<br>  \n",
    "\n",
    "> Version 4 of the tweets are all reduced down to their word lemmas, futher aiding the algorithm in learning the meaning of the texts.\n",
    "\n",
    "\n",
    "#### EXAMPLE TWEETS AND PROCESSING STEPS:\n",
    "\n",
    "**TWEET FROM 08-25-2017 12:25:10:**\n",
    "* **[\"content\"] column:**<p><blockquote>***\"Strange statement by Bob Corker considering that he is constantly asking me whether or not he should run again in '18. Tennessee not happy!\"***\n",
    "    \n",
    "    \n",
    "* **[\"content_min_clean\"] column:**<p><blockquote>***\"strange statement by bob corker considering that he is constantly asking me whether or not he should run again in  18  tennessee not happy \"***\n",
    "    \n",
    "    \n",
    "* **[\"cleaned_stopped_content\"] column:**<p><blockquote>***\"strange statement bob corker considering constantly asking whether run tennessee happy\"***\n",
    "    \n",
    "    \n",
    "* **[\"cleaned_stopped_tokens\"] column:**<p><blockquote>***\"['strange', 'statement', 'bob', 'corker', 'considering', 'constantly', 'asking', 'whether', 'run', 'tennessee', 'happy']\"***\n",
    "    \n",
    "    \n",
    "* **[\"cleaned_stopped_lemmas\"] column:**<p><blockquote>***\"strange statement bob corker considering constantly asking whether run tennessee happy\"***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practicing Text Preprocessing with Trump's Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Bag-of-Words Frequency Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:21:02.851808Z",
     "start_time": "2020-02-05T17:21:02.842434Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://t.co/EVAEYD1AgV'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:21:47.037868Z",
     "start_time": "2020-02-05T17:21:45.454066Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 355279),\n",
       " ('e', 193636),\n",
       " ('t', 151004),\n",
       " ('a', 133814),\n",
       " ('o', 132306),\n",
       " ('n', 115782),\n",
       " ('i', 111789),\n",
       " ('r', 107277),\n",
       " ('s', 97228),\n",
       " ('h', 76548),\n",
       " ('l', 67415),\n",
       " ('d', 57543),\n",
       " ('u', 46550),\n",
       " ('c', 45746),\n",
       " ('m', 43554),\n",
       " ('p', 37247),\n",
       " ('g', 35477),\n",
       " ('y', 33375),\n",
       " ('.', 32244),\n",
       " ('w', 30005),\n",
       " ('f', 29211),\n",
       " ('b', 22246),\n",
       " ('T', 19911),\n",
       " ('v', 17186),\n",
       " (',', 14545),\n",
       " ('k', 13783),\n",
       " ('/', 13588),\n",
       " ('S', 12999),\n",
       " ('A', 12922),\n",
       " ('R', 12294),\n",
       " ('C', 10866),\n",
       " ('I', 10743),\n",
       " (':', 9821),\n",
       " ('N', 9009),\n",
       " ('!', 8861),\n",
       " ('D', 8787),\n",
       " ('@', 8646),\n",
       " ('M', 8063),\n",
       " ('E', 8008),\n",
       " ('P', 7726),\n",
       " ('W', 7249),\n",
       " ('O', 6828),\n",
       " ('H', 6614),\n",
       " ('B', 6115),\n",
       " ('G', 5937),\n",
       " ('F', 5701),\n",
       " ('L', 4774),\n",
       " ('U', 4765),\n",
       " ('0', 4659),\n",
       " ('x', 4068),\n",
       " ('J', 4060),\n",
       " ('‚Äô', 3445),\n",
       " ('1', 3191),\n",
       " ('j', 3084),\n",
       " ('‚Ä¶', 2956),\n",
       " ('2', 2812),\n",
       " ('-', 2672),\n",
       " ('K', 2622),\n",
       " ('V', 2460),\n",
       " ('Y', 2194),\n",
       " (';', 2105),\n",
       " ('z', 2082),\n",
       " ('&', 2079),\n",
       " ('‚Äú', 1993),\n",
       " ('‚Äù', 1890),\n",
       " ('5', 1848),\n",
       " ('#', 1785),\n",
       " ('3', 1730),\n",
       " ('4', 1620),\n",
       " ('7', 1534),\n",
       " ('6', 1491),\n",
       " ('8', 1468),\n",
       " ('9', 1437),\n",
       " ('q', 1346),\n",
       " ('(', 1181),\n",
       " (')', 1174),\n",
       " (\"'\", 1145),\n",
       " ('?', 1007),\n",
       " ('Z', 922),\n",
       " ('X', 862),\n",
       " ('Q', 828),\n",
       " ('\"', 797),\n",
       " ('%', 487),\n",
       " ('_', 469),\n",
       " ('üá∫', 308),\n",
       " ('üá∏', 305),\n",
       " ('$', 257),\n",
       " ('‚Äì', 166),\n",
       " ('‚Äî', 148),\n",
       " ('Ô∏è', 131),\n",
       " ('\\n', 80),\n",
       " ('‚Äò', 60),\n",
       " ('‚û°', 59),\n",
       " ('\\u2066', 52),\n",
       " ('\\u2069', 47),\n",
       " ('üö®', 45),\n",
       " ('*', 43),\n",
       " ('‚úÖ', 36),\n",
       " ('+', 33),\n",
       " ('„ÄÅ', 30)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq= FreqDist(','.join(corpus))\n",
    "freq.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:23:05.205438Z",
     "start_time": "2020-02-05T17:23:01.594027Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 15560),\n",
       " (',', 14160),\n",
       " ('.', 13708),\n",
       " (':', 9462),\n",
       " ('to', 9308),\n",
       " ('!', 8861),\n",
       " ('@', 8646),\n",
       " ('and', 8497),\n",
       " ('of', 7176),\n",
       " ('a', 5658),\n",
       " ('is', 5096),\n",
       " ('in', 4996),\n",
       " ('https', 4265),\n",
       " ('for', 4081),\n",
       " ('RT', 3819),\n",
       " ('‚Äô', 3445),\n",
       " ('on', 3150),\n",
       " ('I', 3132),\n",
       " ('that', 3031),\n",
       " ('are', 2825),\n",
       " ('with', 2650),\n",
       " ('...', 2620),\n",
       " ('be', 2501),\n",
       " ('will', 2486),\n",
       " ('our', 2418),\n",
       " ('The', 2367),\n",
       " ('have', 2116),\n",
       " (';', 2105),\n",
       " ('&', 2079),\n",
       " ('amp', 2070),\n",
       " ('‚Äú', 1993),\n",
       " ('it', 1932),\n",
       " ('‚Äù', 1890),\n",
       " ('you', 1834),\n",
       " ('was', 1789),\n",
       " ('#', 1785),\n",
       " ('at', 1621),\n",
       " ('has', 1600),\n",
       " ('they', 1553),\n",
       " ('s', 1517),\n",
       " ('great', 1501),\n",
       " ('President', 1492),\n",
       " ('not', 1415),\n",
       " ('we', 1389),\n",
       " ('by', 1375),\n",
       " ('this', 1336),\n",
       " ('all', 1304),\n",
       " ('t', 1289),\n",
       " ('(', 1181),\n",
       " (')', 1174),\n",
       " ('Trump', 1154),\n",
       " ('Democrats', 1145),\n",
       " ('people', 1124),\n",
       " ('very', 1084),\n",
       " ('-', 1079),\n",
       " ('We', 1072),\n",
       " ('who', 1040),\n",
       " ('?', 1007),\n",
       " ('realDonaldTrump', 996),\n",
       " ('from', 979),\n",
       " ('my', 966),\n",
       " ('as', 956),\n",
       " ('he', 943),\n",
       " ('their', 918),\n",
       " ('been', 838),\n",
       " ('so', 833),\n",
       " ('more', 824),\n",
       " ('about', 801),\n",
       " ('his', 796),\n",
       " ('but', 787),\n",
       " ('now', 784),\n",
       " ('do', 784),\n",
       " ('or', 776),\n",
       " ('out', 769),\n",
       " ('just', 762),\n",
       " ('It', 737),\n",
       " ('an', 736),\n",
       " ('no', 733),\n",
       " ('Thank', 715),\n",
       " ('They', 692),\n",
       " ('up', 686),\n",
       " ('than', 681),\n",
       " ('me', 680),\n",
       " ('would', 678),\n",
       " ('Fake', 653),\n",
       " ('many', 645),\n",
       " ('get', 637),\n",
       " ('Great', 636),\n",
       " ('News', 628),\n",
       " ('U.S.', 625),\n",
       " ('can', 619),\n",
       " ('He', 613),\n",
       " ('time', 612),\n",
       " ('This', 603),\n",
       " ('should', 589),\n",
       " ('what', 583),\n",
       " ('Border', 578),\n",
       " ('there', 576),\n",
       " ('American', 571),\n",
       " ('want', 570)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "tokens = word_tokenize(','.join(corpus))\n",
    "freq=FreqDist(tokens)\n",
    "freq.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T16:45:26.260858Z",
     "start_time": "2020-02-05T16:45:23.355969Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T16:45:26.273582Z",
     "start_time": "2020-02-05T16:45:26.262871Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:24:05.342198Z",
     "start_time": "2020-02-05T17:24:05.339616Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make a list of stopwords to remove\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:30:28.386315Z",
     "start_time": "2020-02-05T17:30:28.381096Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "# Get all the stop words in the English language\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list+=string.punctuation\n",
    "print(stopwords_list)\n",
    "stopwords_list.remove('until')\n",
    "stopwords_list.extend(['‚Äú','...','‚Äù'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:30:28.753711Z",
     "start_time": "2020-02-05T17:30:28.750474Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Commentary on not always accepting what is or isn't in stopwords\n",
    "'until' in stopwords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:30:30.340320Z",
     "start_time": "2020-02-05T17:30:29.205638Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('https', 4265),\n",
       " ('rt', 3819),\n",
       " ('‚Äô', 3445),\n",
       " ('great', 2552),\n",
       " ('amp', 2070),\n",
       " ('president', 1605),\n",
       " ('people', 1309),\n",
       " ('trump', 1193),\n",
       " ('democrats', 1166),\n",
       " ('realdonaldtrump', 1045),\n",
       " ('country', 947),\n",
       " ('news', 931),\n",
       " ('thank', 929),\n",
       " ('big', 832),\n",
       " ('fake', 802),\n",
       " ('new', 791),\n",
       " ('many', 749),\n",
       " ('today', 747),\n",
       " ('get', 741),\n",
       " ('would', 714),\n",
       " ('border', 711),\n",
       " ('america', 699),\n",
       " ('never', 676),\n",
       " ('time', 666),\n",
       " ('u.s.', 625),\n",
       " ('american', 611),\n",
       " ('much', 594),\n",
       " ('want', 591),\n",
       " ('one', 588),\n",
       " ('years', 587),\n",
       " ('media', 582),\n",
       " ('good', 567),\n",
       " ('united', 543),\n",
       " ('even', 525),\n",
       " ('house', 523),\n",
       " ('states', 509),\n",
       " ('back', 492),\n",
       " ('``', 491),\n",
       " ('done', 485),\n",
       " (\"'s\", 479),\n",
       " ('must', 478),\n",
       " ('make', 478),\n",
       " ('china', 474),\n",
       " ('like', 465),\n",
       " ('going', 460),\n",
       " ('vote', 458),\n",
       " ('nothing', 455),\n",
       " ('dems', 452),\n",
       " ('job', 440),\n",
       " ('impeachment', 435),\n",
       " ('jobs', 431),\n",
       " ('state', 413),\n",
       " ('day', 407),\n",
       " ('first', 406),\n",
       " ('us', 404),\n",
       " ('bad', 402),\n",
       " ('whitehouse', 398),\n",
       " ('made', 391),\n",
       " ('military', 386),\n",
       " ('deal', 386),\n",
       " ('election', 382),\n",
       " ('ever', 377),\n",
       " ('trade', 375),\n",
       " ('said', 373),\n",
       " ('way', 366),\n",
       " ('see', 365),\n",
       " ('republican', 362),\n",
       " ('crime', 359),\n",
       " ('fbi', 352),\n",
       " ('win', 351),\n",
       " ('wall', 348),\n",
       " ('working', 344),\n",
       " ('republicans', 342),\n",
       " ('world', 337),\n",
       " ('witch', 336),\n",
       " ('history', 333),\n",
       " ('hunt', 332),\n",
       " ('collusion', 331),\n",
       " ('know', 330),\n",
       " ('russia', 329),\n",
       " ('security', 327),\n",
       " ('also', 323),\n",
       " ('strong', 322),\n",
       " ('work', 320),\n",
       " ('last', 320),\n",
       " ('congress', 319),\n",
       " ('go', 318),\n",
       " ('obama', 314),\n",
       " ('hard', 312),\n",
       " ('year', 311),\n",
       " ('economy', 311),\n",
       " ('north', 310),\n",
       " (\"''\", 307),\n",
       " ('foxnews', 304),\n",
       " ('really', 301),\n",
       " ('two', 300),\n",
       " ('tax', 299),\n",
       " ('look', 295),\n",
       " ('democrat', 291),\n",
       " ('far', 289)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopped_tokens = [w.lower() for w in tokens if w.lower() not in stopwords_list]\n",
    "freq = FreqDist(stopped_tokens)\n",
    "freq.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:32:14.003485Z",
     "start_time": "2020-02-05T17:32:13.971115Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99a33ad0b7ab4b90a9ecd2d16d81186e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=7032, description='i', max=14065), Output()), _dom_classes=('widget-inte‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from ipywidgets import interact\n",
    "\n",
    "@interact\n",
    "def tokenize_tweet(i=(0,len(corpus)-1)):\n",
    "    from nltk.corpus import stopwords\n",
    "    import string\n",
    "    from nltk import word_tokenize,regexp_tokenize\n",
    "    \n",
    "    print(f\"- Tweet #{i}:\\n\")\n",
    "    print(corpus[i],'\\n')\n",
    "    tokens = word_tokenize(corpus[i])\n",
    "\n",
    "    # Get all the stop words in the English language\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    stopwords_list += string.punctuation\n",
    "    stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "    \n",
    "    print(tokens,end='\\n\\n')\n",
    "    print(stopped_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:33:40.140608Z",
     "start_time": "2020-02-05T17:33:40.136488Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://t.co/EVAEYD1AgV',\n",
       " 'HAPPY NEW YEAR!',\n",
       " 'Our fantastic First Lady! https://t.co/6iswto4WDI',\n",
       " 'RT @DanScavino: https://t.co/CJRPySkF1Z',\n",
       " 'RT @SenJohnKennedy: I think Speaker Pelosi is having 2nd thoughts about impeaching the President. The Senate should get back to work on USM‚Ä¶',\n",
       " 'Thank you Steve. The greatest Witch Hunt in U.S. history! https://t.co/I3bSNVp6gC',\n",
       " 'RT @ThisWeekABC: Sen. Ron Johnson says charges against Pres. Trump are \"pretty thin gruel\" and Speaker Nancy Pelosi\\'s decision to withhold‚Ä¶',\n",
       " \"RT @SenJohnKennedy: The Senate needs to reauthorize the Violence Against Women Act and I am proud to cosponsor @SenJoniErnst's bill that g‚Ä¶\",\n",
       " 'RT @LindseyGrahamSC: To our Iraqi allies:This is your moment to convince the American people the US-Iraq relationship is meaningful to yo‚Ä¶',\n",
       " 'RT @LindseyGrahamSC: President Trump unlike President Obama will hold you accountable for threats against Americans and hit you where it‚Ä¶',\n",
       " 'RT @LindseyGrahamSC: Very proud of President @realDonaldTrump acting decisively in the face of threats to our embassy in Baghdad. He has‚Ä¶',\n",
       " 'RT @dcexaminer: \"He has put the world on notice - there will be no Benghazis on his watch.\" @LindseyGrahamSC praises @realDonaldTrump for‚Ä¶',\n",
       " 'RT @LindseyGrahamSC: Just had a very good meeting with President @realDonaldTrump and his team regarding the situation in Iraq.President‚Ä¶',\n",
       " 'Thank you to the @dcexaminer Washington Examiner. The list is growing every day! https://t.co/jAOZ0CPYO4',\n",
       " 'One of my greatest honors was to have gotten CHOICE approved for our great Veterans. Others have tried for decades and failed! https://t.co/NcVbbnrT5O',\n",
       " 'RT @heatherjones333: MAGNIFICENT TRUMP- KEEPING HIS PROMISES  üá∫üá∏ üá∫üá∏ üá∫üá∏BREAKING: President Trump Announces He Will Sign Phase One in China‚Ä¶',\n",
       " 'RT @heatherjones333: üî•üî•üî•üî•üî•Lindsey Graham: Trump Has World on Notice ‚ÄòThere Will Be No Benghazis on His Watch‚Äô https://t.co/4yfHKlZlV9',\n",
       " 'RT @heatherjones333: Congratulations Sleepy Joe! You just won Stupid Person of 2019 Award! üèÜJoe Biden: ‚ÄòWe‚Äôre All Dead‚Äô if We Don‚Äôt Stop‚Ä¶',\n",
       " 'RT @heatherjones333: DEAR STUPID PEOPLE- PEACE IS A GOOD THINGPutin Calls Trump To Thank Him For Helping Russia Thwart Terrorist Attack h‚Ä¶',\n",
       " \"RT @heatherjones333: 'We Have Taken Appropriate Force Protection Actions to Ensure Safety of Americans' - AH-64 Apaches Protect US Embassy‚Ä¶\"]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get FreqDist for Cleaned Text Data\n",
    "corpus[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Phases of Proprocessing/Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T16:45:27.348954Z",
     "start_time": "2020-02-05T16:45:27.346940Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def clean_text(text,exclude_words=['until']):\n",
    "#     from nltk.corpus import stopwords\n",
    "#     import string\n",
    "#     from nltk import word_tokenize,regexp_tokenize\n",
    "#     ## tokenize text\n",
    "#     tokens = word_tokenize(text)\n",
    "#     # Get all the stop words in the English language\n",
    "#     stopwords_list = stopwords.words('english')\n",
    "#     stopwords_list += string.punctuation\n",
    "#     stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "#     return stopped_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T16:45:27.382152Z",
     "start_time": "2020-02-05T16:45:27.350720Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from ipywidgets import interact\n",
    "\n",
    "@interact\n",
    "def tokenize_tweet(i=(0,len(corpus)-1)):\n",
    "    from nltk.corpus import stopwords\n",
    "    import string\n",
    "    from nltk import word_tokenize,regexp_tokenize\n",
    "    \n",
    "    print(f\"- Tweet #{i}:\\n\")\n",
    "    print(corpus[i],'\\n')\n",
    "    tokens = word_tokenize(corpus[i])\n",
    "\n",
    "    # Get all the stop words in the English language\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    stopwords_list += string.punctuation\n",
    "    stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "    \n",
    "    print(tokens,end='\\n\\n')\n",
    "    print(stopped_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best regexp resource and tester: https://regex101.com/\n",
    "\n",
    "    - Make sure to check \"Python\" under Flavor menu on left side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:33:00.427588Z",
     "start_time": "2020-02-05T17:33:00.423967Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I will be in Green Bay Wisconsin on Saturday April 27th at the Resch Center ‚Äî 7:00pm (CDT). Big crowd expected! #MAGA https://t.co/BPYK8PF0O8'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text =  corpus[6615]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:33:00.879037Z",
     "start_time": "2020-02-05T17:33:00.873783Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @real_defender: @realDonaldTrump Protecting America and putting Americans first. Thank you Mr. President!'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2=corpus[7347]\n",
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:39:37.243670Z",
     "start_time": "2020-02-05T17:39:37.238793Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'will',\n",
       " 'be',\n",
       " 'in',\n",
       " 'Green',\n",
       " 'Bay',\n",
       " 'Wisconsin',\n",
       " 'on',\n",
       " 'Saturday',\n",
       " 'April',\n",
       " 'th',\n",
       " 'at',\n",
       " 'the',\n",
       " 'Resch',\n",
       " 'Center',\n",
       " 'pm',\n",
       " 'CDT',\n",
       " 'Big',\n",
       " 'crowd',\n",
       " 'expected',\n",
       " 'MAGA',\n",
       " 'https',\n",
       " 't',\n",
       " 'co',\n",
       " 'BPYK',\n",
       " 'PF',\n",
       " 'O']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import regexp_tokenize\n",
    "pattern = r\"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "regexp_tokenize(text,pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:39:47.967157Z",
     "start_time": "2020-02-05T17:39:47.962699Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Word Tokenize:\n",
      "------------------------------------------------------------\n",
      "['I', 'will', 'be', 'in', 'Green', 'Bay', 'Wisconsin', 'on', 'Saturday', 'April', '27th', 'at', 'the', 'Resch', 'Center', '‚Äî', '7:00pm', '(', 'CDT', ')', '.', 'Big', 'crowd', 'expected', '!', '#', 'MAGA', 'https', ':', '//t.co/BPYK8PF0O8']\n",
      "\n",
      "[i] Regexp Tokenize:\n",
      "------------------------------------------------------------\n",
      "['I', 'will', 'be', 'in', 'Green', 'Bay', 'Wisconsin', 'on', 'Saturday', 'April', 'th', 'at', 'the', 'Resch', 'Center', 'pm', 'CDT', 'Big', 'crowd', 'expected', 'MAGA', 'https', 't', 'co', 'BPYK', 'PF', 'O']\n"
     ]
    }
   ],
   "source": [
    "print('[i] Word Tokenize:',end='\\n'+'---'*20+'\\n')\n",
    "print(word_tokenize(text))\n",
    "\n",
    "print('\\n[i] Regexp Tokenize:',end='\\n'+'---'*20+'\\n')\n",
    "print(regexp_tokenize(text,pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:40:27.416410Z",
     "start_time": "2020-02-05T17:40:27.411167Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text,regex=True):\n",
    "    from nltk.corpus import stopwords\n",
    "    import string\n",
    "    from nltk import word_tokenize,regexp_tokenize\n",
    "\n",
    "    ## tokenize text\n",
    "    if regex:\n",
    "        pattern = r\"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "        tokens= regexp_tokenize(text,pattern)\n",
    "    else:\n",
    "        tokens = word_tokenize(text)\n",
    "    # Get all the stop words in the English language\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    stopwords_list += string.punctuation\n",
    "    stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "    return stopped_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T16:45:27.419890Z",
     "start_time": "2020-02-05T16:45:27.417783Z"
    }
   },
   "outputs": [],
   "source": [
    "# @interact\n",
    "# def regexp_tokenize_tweet(i=(0,len(corpus)-1)):\n",
    "#     print(f\"- Tweet #{i}:\\n\")\n",
    "#     print(corpus[i],'\\n')\n",
    "#     from nltk import regexp_tokenize\n",
    "#     pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "#     tokens= regexp_tokenize(corpus[i],pattern)\n",
    "\n",
    "#     # It is usually a good idea to lowercase all tokens during this step, as well\n",
    "#     stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "#     print(tokens,end='\\n\\n')\n",
    "#     return print(stopped_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:41:40.216542Z",
     "start_time": "2020-02-05T17:41:40.211310Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def find_urls(string): \n",
    "    return re.findall(r\"(http[s]?://\\w*\\.\\w*/+\\w+)\",string)\n",
    "\n",
    "def find_hashtags(string):\n",
    "    return re.findall(r'\\#\\w*',string)\n",
    "\n",
    "def find_retweets(string):\n",
    "    return re.findall(r'RT [@]?\\w*:',string)\n",
    "\n",
    "def find_mentions(string):\n",
    "    return re.findall(r'\\@\\w*',string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:41:40.414919Z",
     "start_time": "2020-02-05T17:41:40.410981Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://t.co/BPYK8PF0O8']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_urls(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T16:45:27.430054Z",
     "start_time": "2020-02-05T16:45:27.423484Z"
    }
   },
   "outputs": [],
   "source": [
    "find_mentions(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming/Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:43:48.150618Z",
     "start_time": "2020-02-05T17:43:46.167428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foot\n",
      "running\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize('feet')) # foot\n",
    "print(lemmatizer.lemmatize('running')) # run [?!] Does not match expected output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:54:38.969730Z",
     "start_time": "2020-02-05T17:54:38.963457Z"
    }
   },
   "outputs": [],
   "source": [
    "text_in =  corpus[6615]\n",
    "\n",
    "# # urls = find_urls(text)\n",
    "# def clean_text(text,regex=True):\n",
    "#     from nltk.corpus import stopwords\n",
    "#     import string\n",
    "#     from nltk import word_tokenize,regexp_tokenize\n",
    "\n",
    "#     ## tokenize text\n",
    "#     if regex:\n",
    "#         pattern = r\"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "#         tokens= regexp_tokenize(text,pattern)\n",
    "#     else:\n",
    "#         tokens = word_tokenize(text)\n",
    "#     # Get all the stop words in the English language\n",
    "#     stopwords_list = stopwords.words('english')\n",
    "#     stopwords_list += string.punctuation\n",
    "#     stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "#     return stopped_tokens\n",
    "\n",
    "def process_tweet(text,as_lemmas=False,as_tokens=True):\n",
    "#     text=text.copy()\n",
    "    for x in find_urls(text):\n",
    "        text = text.replace(x,'')\n",
    "        \n",
    "    for x in find_retweets(text):\n",
    "        text = text.replace(x,'')    \n",
    "        \n",
    "    for x in find_hashtags(text):\n",
    "        text = text.replace(x,'')    \n",
    "\n",
    "    if as_lemmas:\n",
    "        from nltk.stem.wordnet import WordNetLemmatizer\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        text = lemmatizer.lemmatize(text)\n",
    "    \n",
    "    if as_tokens:\n",
    "        text = clean_text(text)\n",
    "    \n",
    "    if len(text)==0:\n",
    "        text=''\n",
    "            \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:54:39.472657Z",
     "start_time": "2020-02-05T17:54:39.440971Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d6a7951ed1747d2aa371c762820ec41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=7032, description='i', max=14065), Output()), _dom_classes=('widget-inte‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact\n",
    "def show_processed_text(i=(0,len(corpus)-1)):\n",
    "    text_in = corpus[i]#.copy()\n",
    "    print(text_in)\n",
    "    text_out = process_tweet(text_in)\n",
    "    print(text_out)\n",
    "    text_out2 = process_tweet(text_in,as_lemmas=True)\n",
    "    print(text_out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:54:40.266948Z",
     "start_time": "2020-02-05T17:54:40.262936Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://t.co/EVAEYD1AgV',\n",
       " 'HAPPY NEW YEAR!',\n",
       " 'Our fantastic First Lady! https://t.co/6iswto4WDI',\n",
       " 'RT @DanScavino: https://t.co/CJRPySkF1Z',\n",
       " 'RT @SenJohnKennedy: I think Speaker Pelosi is having 2nd thoughts about impeaching the President. The Senate should get back to work on USM‚Ä¶',\n",
       " 'Thank you Steve. The greatest Witch Hunt in U.S. history! https://t.co/I3bSNVp6gC']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Potential Tasks: Classify Android vs iPhone tweets (from period where Android tweets still exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:55:23.351071Z",
     "start_time": "2020-02-05T17:55:21.827219Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>id_str</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Twitter Media Studio</td>\n",
       "      <td>https://t.co/EVAEYD1AgV</td>\n",
       "      <td>01-01-2020 03:12:07</td>\n",
       "      <td>25016</td>\n",
       "      <td>108830</td>\n",
       "      <td>False</td>\n",
       "      <td>1212209862094012416</td>\n",
       "      <td>2020-01-01 03:12:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>HAPPY NEW YEAR!</td>\n",
       "      <td>01-01-2020 01:30:35</td>\n",
       "      <td>85409</td>\n",
       "      <td>576045</td>\n",
       "      <td>False</td>\n",
       "      <td>1212184310389850119</td>\n",
       "      <td>2020-01-01 01:30:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Our fantastic First Lady! https://t.co/6iswto4WDI</td>\n",
       "      <td>01-01-2020 01:22:28</td>\n",
       "      <td>27567</td>\n",
       "      <td>132633</td>\n",
       "      <td>False</td>\n",
       "      <td>1212182267113680896</td>\n",
       "      <td>2020-01-01 01:22:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @DanScavino: https://t.co/CJRPySkF1Z</td>\n",
       "      <td>01-01-2020 01:18:47</td>\n",
       "      <td>10796</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1212181341078458369</td>\n",
       "      <td>2020-01-01 01:18:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @SenJohnKennedy: I think Speaker Pelosi is ...</td>\n",
       "      <td>01-01-2020 01:17:43</td>\n",
       "      <td>8893</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1212181071988703232</td>\n",
       "      <td>2020-01-01 01:17:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14061</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>The President of Taiwan CALLED ME today to wis...</td>\n",
       "      <td>12-03-2016 00:44:20</td>\n",
       "      <td>24700</td>\n",
       "      <td>111106</td>\n",
       "      <td>False</td>\n",
       "      <td>804848711599882240</td>\n",
       "      <td>2016-12-03 00:44:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14062</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Thank you Ohio! Together we made history ‚Äì and...</td>\n",
       "      <td>12-02-2016 02:45:18</td>\n",
       "      <td>17283</td>\n",
       "      <td>72196</td>\n",
       "      <td>False</td>\n",
       "      <td>804516764562374656</td>\n",
       "      <td>2016-12-02 02:45:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14063</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Heading to U.S. Bank Arena in Cincinnati Ohio ...</td>\n",
       "      <td>12-01-2016 22:52:10</td>\n",
       "      <td>5564</td>\n",
       "      <td>31256</td>\n",
       "      <td>False</td>\n",
       "      <td>804458095569158144</td>\n",
       "      <td>2016-12-01 22:52:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14064</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>Getting ready to leave for the Great State of ...</td>\n",
       "      <td>12-01-2016 14:38:09</td>\n",
       "      <td>9834</td>\n",
       "      <td>57249</td>\n",
       "      <td>False</td>\n",
       "      <td>804333771021570048</td>\n",
       "      <td>2016-12-01 14:38:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14065</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>My thoughts and prayers are with those affecte...</td>\n",
       "      <td>12-01-2016 14:37:57</td>\n",
       "      <td>12077</td>\n",
       "      <td>65724</td>\n",
       "      <td>False</td>\n",
       "      <td>804333718999539712</td>\n",
       "      <td>2016-12-01 14:37:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14066 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     source  \\\n",
       "0      Twitter Media Studio   \n",
       "1        Twitter for iPhone   \n",
       "2        Twitter for iPhone   \n",
       "3        Twitter for iPhone   \n",
       "4        Twitter for iPhone   \n",
       "...                     ...   \n",
       "14061   Twitter for Android   \n",
       "14062    Twitter for iPhone   \n",
       "14063    Twitter for iPhone   \n",
       "14064   Twitter for Android   \n",
       "14065    Twitter for iPhone   \n",
       "\n",
       "                                                    text           created_at  \\\n",
       "0                                https://t.co/EVAEYD1AgV  01-01-2020 03:12:07   \n",
       "1                                        HAPPY NEW YEAR!  01-01-2020 01:30:35   \n",
       "2      Our fantastic First Lady! https://t.co/6iswto4WDI  01-01-2020 01:22:28   \n",
       "3                RT @DanScavino: https://t.co/CJRPySkF1Z  01-01-2020 01:18:47   \n",
       "4      RT @SenJohnKennedy: I think Speaker Pelosi is ...  01-01-2020 01:17:43   \n",
       "...                                                  ...                  ...   \n",
       "14061  The President of Taiwan CALLED ME today to wis...  12-03-2016 00:44:20   \n",
       "14062  Thank you Ohio! Together we made history ‚Äì and...  12-02-2016 02:45:18   \n",
       "14063  Heading to U.S. Bank Arena in Cincinnati Ohio ...  12-01-2016 22:52:10   \n",
       "14064  Getting ready to leave for the Great State of ...  12-01-2016 14:38:09   \n",
       "14065  My thoughts and prayers are with those affecte...  12-01-2016 14:37:57   \n",
       "\n",
       "       retweet_count  favorite_count is_retweet               id_str  \\\n",
       "0              25016          108830      False  1212209862094012416   \n",
       "1              85409          576045      False  1212184310389850119   \n",
       "2              27567          132633      False  1212182267113680896   \n",
       "3              10796               0       True  1212181341078458369   \n",
       "4               8893               0       True  1212181071988703232   \n",
       "...              ...             ...        ...                  ...   \n",
       "14061          24700          111106      False   804848711599882240   \n",
       "14062          17283           72196      False   804516764562374656   \n",
       "14063           5564           31256      False   804458095569158144   \n",
       "14064           9834           57249      False   804333771021570048   \n",
       "14065          12077           65724      False   804333718999539712   \n",
       "\n",
       "                 datetime  \n",
       "0     2020-01-01 03:12:07  \n",
       "1     2020-01-01 01:30:35  \n",
       "2     2020-01-01 01:22:28  \n",
       "3     2020-01-01 01:18:47  \n",
       "4     2020-01-01 01:17:43  \n",
       "...                   ...  \n",
       "14061 2016-12-03 00:44:20  \n",
       "14062 2016-12-02 02:45:18  \n",
       "14063 2016-12-01 22:52:10  \n",
       "14064 2016-12-01 14:38:09  \n",
       "14065 2016-12-01 14:37:57  \n",
       "\n",
       "[14066 rows x 8 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['datetime'] = pd.to_datetime(df['created_at'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:55:28.760867Z",
     "start_time": "2020-02-05T17:55:28.730119Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>id_str</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2016-12-01 14:37:57</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>My thoughts and prayers are with those affecte...</td>\n",
       "      <td>12-01-2016 14:37:57</td>\n",
       "      <td>12077</td>\n",
       "      <td>65724</td>\n",
       "      <td>False</td>\n",
       "      <td>804333718999539712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-12-01 14:38:09</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>Getting ready to leave for the Great State of ...</td>\n",
       "      <td>12-01-2016 14:38:09</td>\n",
       "      <td>9834</td>\n",
       "      <td>57249</td>\n",
       "      <td>False</td>\n",
       "      <td>804333771021570048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-12-01 22:52:10</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Heading to U.S. Bank Arena in Cincinnati Ohio ...</td>\n",
       "      <td>12-01-2016 22:52:10</td>\n",
       "      <td>5564</td>\n",
       "      <td>31256</td>\n",
       "      <td>False</td>\n",
       "      <td>804458095569158144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-12-02 02:45:18</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Thank you Ohio! Together we made history ‚Äì and...</td>\n",
       "      <td>12-02-2016 02:45:18</td>\n",
       "      <td>17283</td>\n",
       "      <td>72196</td>\n",
       "      <td>False</td>\n",
       "      <td>804516764562374656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-12-03 00:44:20</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>The President of Taiwan CALLED ME today to wis...</td>\n",
       "      <td>12-03-2016 00:44:20</td>\n",
       "      <td>24700</td>\n",
       "      <td>111106</td>\n",
       "      <td>False</td>\n",
       "      <td>804848711599882240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-01 01:17:43</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @SenJohnKennedy: I think Speaker Pelosi is ...</td>\n",
       "      <td>01-01-2020 01:17:43</td>\n",
       "      <td>8893</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1212181071988703232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-01 01:18:47</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @DanScavino: https://t.co/CJRPySkF1Z</td>\n",
       "      <td>01-01-2020 01:18:47</td>\n",
       "      <td>10796</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1212181341078458369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-01 01:22:28</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Our fantastic First Lady! https://t.co/6iswto4WDI</td>\n",
       "      <td>01-01-2020 01:22:28</td>\n",
       "      <td>27567</td>\n",
       "      <td>132633</td>\n",
       "      <td>False</td>\n",
       "      <td>1212182267113680896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-01 01:30:35</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>HAPPY NEW YEAR!</td>\n",
       "      <td>01-01-2020 01:30:35</td>\n",
       "      <td>85409</td>\n",
       "      <td>576045</td>\n",
       "      <td>False</td>\n",
       "      <td>1212184310389850119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-01 03:12:07</td>\n",
       "      <td>Twitter Media Studio</td>\n",
       "      <td>https://t.co/EVAEYD1AgV</td>\n",
       "      <td>01-01-2020 03:12:07</td>\n",
       "      <td>25016</td>\n",
       "      <td>108830</td>\n",
       "      <td>False</td>\n",
       "      <td>1212209862094012416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14066 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   source  \\\n",
       "datetime                                    \n",
       "2016-12-01 14:37:57    Twitter for iPhone   \n",
       "2016-12-01 14:38:09   Twitter for Android   \n",
       "2016-12-01 22:52:10    Twitter for iPhone   \n",
       "2016-12-02 02:45:18    Twitter for iPhone   \n",
       "2016-12-03 00:44:20   Twitter for Android   \n",
       "...                                   ...   \n",
       "2020-01-01 01:17:43    Twitter for iPhone   \n",
       "2020-01-01 01:18:47    Twitter for iPhone   \n",
       "2020-01-01 01:22:28    Twitter for iPhone   \n",
       "2020-01-01 01:30:35    Twitter for iPhone   \n",
       "2020-01-01 03:12:07  Twitter Media Studio   \n",
       "\n",
       "                                                                  text  \\\n",
       "datetime                                                                 \n",
       "2016-12-01 14:37:57  My thoughts and prayers are with those affecte...   \n",
       "2016-12-01 14:38:09  Getting ready to leave for the Great State of ...   \n",
       "2016-12-01 22:52:10  Heading to U.S. Bank Arena in Cincinnati Ohio ...   \n",
       "2016-12-02 02:45:18  Thank you Ohio! Together we made history ‚Äì and...   \n",
       "2016-12-03 00:44:20  The President of Taiwan CALLED ME today to wis...   \n",
       "...                                                                ...   \n",
       "2020-01-01 01:17:43  RT @SenJohnKennedy: I think Speaker Pelosi is ...   \n",
       "2020-01-01 01:18:47            RT @DanScavino: https://t.co/CJRPySkF1Z   \n",
       "2020-01-01 01:22:28  Our fantastic First Lady! https://t.co/6iswto4WDI   \n",
       "2020-01-01 01:30:35                                    HAPPY NEW YEAR!   \n",
       "2020-01-01 03:12:07                            https://t.co/EVAEYD1AgV   \n",
       "\n",
       "                              created_at  retweet_count  favorite_count  \\\n",
       "datetime                                                                  \n",
       "2016-12-01 14:37:57  12-01-2016 14:37:57          12077           65724   \n",
       "2016-12-01 14:38:09  12-01-2016 14:38:09           9834           57249   \n",
       "2016-12-01 22:52:10  12-01-2016 22:52:10           5564           31256   \n",
       "2016-12-02 02:45:18  12-02-2016 02:45:18          17283           72196   \n",
       "2016-12-03 00:44:20  12-03-2016 00:44:20          24700          111106   \n",
       "...                                  ...            ...             ...   \n",
       "2020-01-01 01:17:43  01-01-2020 01:17:43           8893               0   \n",
       "2020-01-01 01:18:47  01-01-2020 01:18:47          10796               0   \n",
       "2020-01-01 01:22:28  01-01-2020 01:22:28          27567          132633   \n",
       "2020-01-01 01:30:35  01-01-2020 01:30:35          85409          576045   \n",
       "2020-01-01 03:12:07  01-01-2020 03:12:07          25016          108830   \n",
       "\n",
       "                    is_retweet               id_str  \n",
       "datetime                                             \n",
       "2016-12-01 14:37:57      False   804333718999539712  \n",
       "2016-12-01 14:38:09      False   804333771021570048  \n",
       "2016-12-01 22:52:10      False   804458095569158144  \n",
       "2016-12-02 02:45:18      False   804516764562374656  \n",
       "2016-12-03 00:44:20      False   804848711599882240  \n",
       "...                        ...                  ...  \n",
       "2020-01-01 01:17:43       True  1212181071988703232  \n",
       "2020-01-01 01:18:47       True  1212181341078458369  \n",
       "2020-01-01 01:22:28      False  1212182267113680896  \n",
       "2020-01-01 01:30:35      False  1212184310389850119  \n",
       "2020-01-01 03:12:07      False  1212209862094012416  \n",
       "\n",
       "[14066 rows x 7 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.set_index('datetime').sort_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:55:35.507169Z",
     "start_time": "2020-02-05T17:55:31.878608Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>id_str</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2016-12-01 14:37:57</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>My thoughts and prayers are with those affecte...</td>\n",
       "      <td>12-01-2016 14:37:57</td>\n",
       "      <td>12077</td>\n",
       "      <td>65724</td>\n",
       "      <td>False</td>\n",
       "      <td>804333718999539712</td>\n",
       "      <td>[my, thoughts, prayers, affected, tragic, stor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-12-01 14:38:09</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>Getting ready to leave for the Great State of ...</td>\n",
       "      <td>12-01-2016 14:38:09</td>\n",
       "      <td>9834</td>\n",
       "      <td>57249</td>\n",
       "      <td>False</td>\n",
       "      <td>804333771021570048</td>\n",
       "      <td>[getting, ready, leave, great, state, indiana,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-12-01 22:52:10</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Heading to U.S. Bank Arena in Cincinnati Ohio ...</td>\n",
       "      <td>12-01-2016 22:52:10</td>\n",
       "      <td>5564</td>\n",
       "      <td>31256</td>\n",
       "      <td>False</td>\n",
       "      <td>804458095569158144</td>\n",
       "      <td>[heading, u, s, bank, arena, cincinnati, ohio,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-12-02 02:45:18</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Thank you Ohio! Together we made history ‚Äì and...</td>\n",
       "      <td>12-02-2016 02:45:18</td>\n",
       "      <td>17283</td>\n",
       "      <td>72196</td>\n",
       "      <td>False</td>\n",
       "      <td>804516764562374656</td>\n",
       "      <td>[thank, ohio, together, made, history, real, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-12-03 00:44:20</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>The President of Taiwan CALLED ME today to wis...</td>\n",
       "      <td>12-03-2016 00:44:20</td>\n",
       "      <td>24700</td>\n",
       "      <td>111106</td>\n",
       "      <td>False</td>\n",
       "      <td>804848711599882240</td>\n",
       "      <td>[the, president, taiwan, called, me, today, wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-01 01:17:43</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @SenJohnKennedy: I think Speaker Pelosi is ...</td>\n",
       "      <td>01-01-2020 01:17:43</td>\n",
       "      <td>8893</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1212181071988703232</td>\n",
       "      <td>[i, think, speaker, pelosi, nd, thoughts, impe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-01 01:18:47</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @DanScavino: https://t.co/CJRPySkF1Z</td>\n",
       "      <td>01-01-2020 01:18:47</td>\n",
       "      <td>10796</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1212181341078458369</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-01 01:22:28</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Our fantastic First Lady! https://t.co/6iswto4WDI</td>\n",
       "      <td>01-01-2020 01:22:28</td>\n",
       "      <td>27567</td>\n",
       "      <td>132633</td>\n",
       "      <td>False</td>\n",
       "      <td>1212182267113680896</td>\n",
       "      <td>[our, fantastic, first, lady]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-01 01:30:35</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>HAPPY NEW YEAR!</td>\n",
       "      <td>01-01-2020 01:30:35</td>\n",
       "      <td>85409</td>\n",
       "      <td>576045</td>\n",
       "      <td>False</td>\n",
       "      <td>1212184310389850119</td>\n",
       "      <td>[happy, new, year]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-01-01 03:12:07</td>\n",
       "      <td>Twitter Media Studio</td>\n",
       "      <td>https://t.co/EVAEYD1AgV</td>\n",
       "      <td>01-01-2020 03:12:07</td>\n",
       "      <td>25016</td>\n",
       "      <td>108830</td>\n",
       "      <td>False</td>\n",
       "      <td>1212209862094012416</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14066 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   source  \\\n",
       "datetime                                    \n",
       "2016-12-01 14:37:57    Twitter for iPhone   \n",
       "2016-12-01 14:38:09   Twitter for Android   \n",
       "2016-12-01 22:52:10    Twitter for iPhone   \n",
       "2016-12-02 02:45:18    Twitter for iPhone   \n",
       "2016-12-03 00:44:20   Twitter for Android   \n",
       "...                                   ...   \n",
       "2020-01-01 01:17:43    Twitter for iPhone   \n",
       "2020-01-01 01:18:47    Twitter for iPhone   \n",
       "2020-01-01 01:22:28    Twitter for iPhone   \n",
       "2020-01-01 01:30:35    Twitter for iPhone   \n",
       "2020-01-01 03:12:07  Twitter Media Studio   \n",
       "\n",
       "                                                                  text  \\\n",
       "datetime                                                                 \n",
       "2016-12-01 14:37:57  My thoughts and prayers are with those affecte...   \n",
       "2016-12-01 14:38:09  Getting ready to leave for the Great State of ...   \n",
       "2016-12-01 22:52:10  Heading to U.S. Bank Arena in Cincinnati Ohio ...   \n",
       "2016-12-02 02:45:18  Thank you Ohio! Together we made history ‚Äì and...   \n",
       "2016-12-03 00:44:20  The President of Taiwan CALLED ME today to wis...   \n",
       "...                                                                ...   \n",
       "2020-01-01 01:17:43  RT @SenJohnKennedy: I think Speaker Pelosi is ...   \n",
       "2020-01-01 01:18:47            RT @DanScavino: https://t.co/CJRPySkF1Z   \n",
       "2020-01-01 01:22:28  Our fantastic First Lady! https://t.co/6iswto4WDI   \n",
       "2020-01-01 01:30:35                                    HAPPY NEW YEAR!   \n",
       "2020-01-01 03:12:07                            https://t.co/EVAEYD1AgV   \n",
       "\n",
       "                              created_at  retweet_count  favorite_count  \\\n",
       "datetime                                                                  \n",
       "2016-12-01 14:37:57  12-01-2016 14:37:57          12077           65724   \n",
       "2016-12-01 14:38:09  12-01-2016 14:38:09           9834           57249   \n",
       "2016-12-01 22:52:10  12-01-2016 22:52:10           5564           31256   \n",
       "2016-12-02 02:45:18  12-02-2016 02:45:18          17283           72196   \n",
       "2016-12-03 00:44:20  12-03-2016 00:44:20          24700          111106   \n",
       "...                                  ...            ...             ...   \n",
       "2020-01-01 01:17:43  01-01-2020 01:17:43           8893               0   \n",
       "2020-01-01 01:18:47  01-01-2020 01:18:47          10796               0   \n",
       "2020-01-01 01:22:28  01-01-2020 01:22:28          27567          132633   \n",
       "2020-01-01 01:30:35  01-01-2020 01:30:35          85409          576045   \n",
       "2020-01-01 03:12:07  01-01-2020 03:12:07          25016          108830   \n",
       "\n",
       "                    is_retweet               id_str  \\\n",
       "datetime                                              \n",
       "2016-12-01 14:37:57      False   804333718999539712   \n",
       "2016-12-01 14:38:09      False   804333771021570048   \n",
       "2016-12-01 22:52:10      False   804458095569158144   \n",
       "2016-12-02 02:45:18      False   804516764562374656   \n",
       "2016-12-03 00:44:20      False   804848711599882240   \n",
       "...                        ...                  ...   \n",
       "2020-01-01 01:17:43       True  1212181071988703232   \n",
       "2020-01-01 01:18:47       True  1212181341078458369   \n",
       "2020-01-01 01:22:28      False  1212182267113680896   \n",
       "2020-01-01 01:30:35      False  1212184310389850119   \n",
       "2020-01-01 03:12:07      False  1212209862094012416   \n",
       "\n",
       "                                                            clean_text  \n",
       "datetime                                                                \n",
       "2016-12-01 14:37:57  [my, thoughts, prayers, affected, tragic, stor...  \n",
       "2016-12-01 14:38:09  [getting, ready, leave, great, state, indiana,...  \n",
       "2016-12-01 22:52:10  [heading, u, s, bank, arena, cincinnati, ohio,...  \n",
       "2016-12-02 02:45:18  [thank, ohio, together, made, history, real, w...  \n",
       "2016-12-03 00:44:20  [the, president, taiwan, called, me, today, wi...  \n",
       "...                                                                ...  \n",
       "2020-01-01 01:17:43  [i, think, speaker, pelosi, nd, thoughts, impe...  \n",
       "2020-01-01 01:18:47                                                     \n",
       "2020-01-01 01:22:28                      [our, fantastic, first, lady]  \n",
       "2020-01-01 01:30:35                                 [happy, new, year]  \n",
       "2020-01-01 03:12:07                                                     \n",
       "\n",
       "[14066 rows x 8 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text'] = df['text'].apply(process_tweet)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:56:12.423626Z",
     "start_time": "2020-02-05T17:56:12.408720Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2016-12-01 14:38:09', '2016-12-03 00:44:20',\n",
       "               '2016-12-03 01:41:30', '2016-12-03 03:06:41',\n",
       "               '2016-12-03 16:37:27', '2016-12-04 05:13:58',\n",
       "               '2016-12-04 11:41:47', '2016-12-04 11:49:06',\n",
       "               '2016-12-04 11:57:41', '2016-12-04 12:05:35',\n",
       "               ...\n",
       "               '2017-03-05 11:40:20', '2017-03-07 12:04:13',\n",
       "               '2017-03-07 12:13:59', '2017-03-07 13:13:20',\n",
       "               '2017-03-07 13:41:58', '2017-03-07 13:46:28',\n",
       "               '2017-03-07 14:14:03', '2017-03-08 12:11:25',\n",
       "               '2017-03-25 14:37:52', '2017-03-25 14:41:14'],\n",
       "              dtype='datetime64[ns]', name='datetime', length=364, freq=None)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "android = df.groupby('source').get_group('Twitter for Android')\n",
    "android.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:56:15.002976Z",
     "start_time": "2020-02-05T17:56:14.977599Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>id_str</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2016-12-01 14:37:57</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>My thoughts and prayers are with those affecte...</td>\n",
       "      <td>12-01-2016 14:37:57</td>\n",
       "      <td>12077</td>\n",
       "      <td>65724</td>\n",
       "      <td>False</td>\n",
       "      <td>804333718999539712</td>\n",
       "      <td>[my, thoughts, prayers, affected, tragic, stor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-12-01 22:52:10</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Heading to U.S. Bank Arena in Cincinnati Ohio ...</td>\n",
       "      <td>12-01-2016 22:52:10</td>\n",
       "      <td>5564</td>\n",
       "      <td>31256</td>\n",
       "      <td>False</td>\n",
       "      <td>804458095569158144</td>\n",
       "      <td>[heading, u, s, bank, arena, cincinnati, ohio,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-12-02 02:45:18</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Thank you Ohio! Together we made history ‚Äì and...</td>\n",
       "      <td>12-02-2016 02:45:18</td>\n",
       "      <td>17283</td>\n",
       "      <td>72196</td>\n",
       "      <td>False</td>\n",
       "      <td>804516764562374656</td>\n",
       "      <td>[thank, ohio, together, made, history, real, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-12-03 19:09:40</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>State Treasurer John Kennedy is my choice for ...</td>\n",
       "      <td>12-03-2016 19:09:40</td>\n",
       "      <td>9800</td>\n",
       "      <td>39057</td>\n",
       "      <td>False</td>\n",
       "      <td>805126876779913216</td>\n",
       "      <td>[state, treasurer, john, kennedy, choice, us, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-12-03 19:13:01</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Our great VPE @mike_pence is in Louisiana camp...</td>\n",
       "      <td>12-03-2016 19:13:01</td>\n",
       "      <td>9224</td>\n",
       "      <td>39351</td>\n",
       "      <td>False</td>\n",
       "      <td>805127720749383680</td>\n",
       "      <td>[our, great, vpe, mike, pence, louisiana, camp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-03-24 12:14:32</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>After seven horrible years of ObamaCare (skyro...</td>\n",
       "      <td>03-24-2017 12:14:32</td>\n",
       "      <td>12566</td>\n",
       "      <td>68241</td>\n",
       "      <td>False</td>\n",
       "      <td>845247455868391425</td>\n",
       "      <td>[after, seven, horrible, years, obamacare, sky...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-03-24 12:23:00</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>The irony is that the Freedom Caucus which is ...</td>\n",
       "      <td>03-24-2017 12:23:00</td>\n",
       "      <td>13364</td>\n",
       "      <td>61991</td>\n",
       "      <td>False</td>\n",
       "      <td>845249587178819584</td>\n",
       "      <td>[the, irony, freedom, caucus, pro, life, plann...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-03-24 17:03:46</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Today I was pleased to announce the official a...</td>\n",
       "      <td>03-24-2017 17:03:46</td>\n",
       "      <td>12933</td>\n",
       "      <td>66692</td>\n",
       "      <td>False</td>\n",
       "      <td>845320243614547968</td>\n",
       "      <td>[today, i, pleased, announce, official, approv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-03-24 17:59:42</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Today I was thrilled to announce a commitment ...</td>\n",
       "      <td>03-24-2017 17:59:42</td>\n",
       "      <td>20212</td>\n",
       "      <td>89339</td>\n",
       "      <td>False</td>\n",
       "      <td>845334323045765121</td>\n",
       "      <td>[today, i, thrilled, announce, commitment, bil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2017-03-25 13:29:17</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Happy #MedalOfHonorDay to our heroes! ‚û°Ô∏èhttps:...</td>\n",
       "      <td>03-25-2017 13:29:17</td>\n",
       "      <td>14139</td>\n",
       "      <td>68302</td>\n",
       "      <td>False</td>\n",
       "      <td>845628655493677056</td>\n",
       "      <td>[happy, heroes]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 source  \\\n",
       "datetime                                  \n",
       "2016-12-01 14:37:57  Twitter for iPhone   \n",
       "2016-12-01 22:52:10  Twitter for iPhone   \n",
       "2016-12-02 02:45:18  Twitter for iPhone   \n",
       "2016-12-03 19:09:40  Twitter for iPhone   \n",
       "2016-12-03 19:13:01  Twitter for iPhone   \n",
       "...                                 ...   \n",
       "2017-03-24 12:14:32  Twitter for iPhone   \n",
       "2017-03-24 12:23:00  Twitter for iPhone   \n",
       "2017-03-24 17:03:46  Twitter for iPhone   \n",
       "2017-03-24 17:59:42  Twitter for iPhone   \n",
       "2017-03-25 13:29:17  Twitter for iPhone   \n",
       "\n",
       "                                                                  text  \\\n",
       "datetime                                                                 \n",
       "2016-12-01 14:37:57  My thoughts and prayers are with those affecte...   \n",
       "2016-12-01 22:52:10  Heading to U.S. Bank Arena in Cincinnati Ohio ...   \n",
       "2016-12-02 02:45:18  Thank you Ohio! Together we made history ‚Äì and...   \n",
       "2016-12-03 19:09:40  State Treasurer John Kennedy is my choice for ...   \n",
       "2016-12-03 19:13:01  Our great VPE @mike_pence is in Louisiana camp...   \n",
       "...                                                                ...   \n",
       "2017-03-24 12:14:32  After seven horrible years of ObamaCare (skyro...   \n",
       "2017-03-24 12:23:00  The irony is that the Freedom Caucus which is ...   \n",
       "2017-03-24 17:03:46  Today I was pleased to announce the official a...   \n",
       "2017-03-24 17:59:42  Today I was thrilled to announce a commitment ...   \n",
       "2017-03-25 13:29:17  Happy #MedalOfHonorDay to our heroes! ‚û°Ô∏èhttps:...   \n",
       "\n",
       "                              created_at  retweet_count  favorite_count  \\\n",
       "datetime                                                                  \n",
       "2016-12-01 14:37:57  12-01-2016 14:37:57          12077           65724   \n",
       "2016-12-01 22:52:10  12-01-2016 22:52:10           5564           31256   \n",
       "2016-12-02 02:45:18  12-02-2016 02:45:18          17283           72196   \n",
       "2016-12-03 19:09:40  12-03-2016 19:09:40           9800           39057   \n",
       "2016-12-03 19:13:01  12-03-2016 19:13:01           9224           39351   \n",
       "...                                  ...            ...             ...   \n",
       "2017-03-24 12:14:32  03-24-2017 12:14:32          12566           68241   \n",
       "2017-03-24 12:23:00  03-24-2017 12:23:00          13364           61991   \n",
       "2017-03-24 17:03:46  03-24-2017 17:03:46          12933           66692   \n",
       "2017-03-24 17:59:42  03-24-2017 17:59:42          20212           89339   \n",
       "2017-03-25 13:29:17  03-25-2017 13:29:17          14139           68302   \n",
       "\n",
       "                    is_retweet              id_str  \\\n",
       "datetime                                             \n",
       "2016-12-01 14:37:57      False  804333718999539712   \n",
       "2016-12-01 22:52:10      False  804458095569158144   \n",
       "2016-12-02 02:45:18      False  804516764562374656   \n",
       "2016-12-03 19:09:40      False  805126876779913216   \n",
       "2016-12-03 19:13:01      False  805127720749383680   \n",
       "...                        ...                 ...   \n",
       "2017-03-24 12:14:32      False  845247455868391425   \n",
       "2017-03-24 12:23:00      False  845249587178819584   \n",
       "2017-03-24 17:03:46      False  845320243614547968   \n",
       "2017-03-24 17:59:42      False  845334323045765121   \n",
       "2017-03-25 13:29:17      False  845628655493677056   \n",
       "\n",
       "                                                            clean_text  \n",
       "datetime                                                                \n",
       "2016-12-01 14:37:57  [my, thoughts, prayers, affected, tragic, stor...  \n",
       "2016-12-01 22:52:10  [heading, u, s, bank, arena, cincinnati, ohio,...  \n",
       "2016-12-02 02:45:18  [thank, ohio, together, made, history, real, w...  \n",
       "2016-12-03 19:09:40  [state, treasurer, john, kennedy, choice, us, ...  \n",
       "2016-12-03 19:13:01  [our, great, vpe, mike, pence, louisiana, camp...  \n",
       "...                                                                ...  \n",
       "2017-03-24 12:14:32  [after, seven, horrible, years, obamacare, sky...  \n",
       "2017-03-24 12:23:00  [the, irony, freedom, caucus, pro, life, plann...  \n",
       "2017-03-24 17:03:46  [today, i, pleased, announce, official, approv...  \n",
       "2017-03-24 17:59:42  [today, i, thrilled, announce, commitment, bil...  \n",
       "2017-03-25 13:29:17                                    [happy, heroes]  \n",
       "\n",
       "[240 rows x 8 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iphone = df.groupby('source').get_group('Twitter for iPhone').loc[:android.index[-1]]\n",
    "iphone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:56:48.102617Z",
     "start_time": "2020-02-05T17:56:48.097448Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364, 240)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(android), len(iphone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:56:48.659622Z",
     "start_time": "2020-02-05T17:56:48.650431Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Twitter for Android    364\n",
       "Twitter for iPhone     240\n",
       "Name: source, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corpus = pd.concat([iphone,android],axis=0)\n",
    "df_corpus['source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Count vectorization\n",
    "- Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "    -  Used for multiple texts\n",
    "    \n",
    "    \n",
    "**_Term Frequency_** is calculated with the following formula:\n",
    "\n",
    "$$ \\text{Term Frequency}(t) = \\frac{\\text{number of times it appears in a document}} {\\text{total number of terms in the document}} $$ \n",
    "\n",
    "**_Inverse Document Frequency_** is calculated with the following formula:\n",
    "\n",
    "$$ IDF(t) = log_e(\\frac{\\text{Total Number of Documents}}{\\text{Number of Documents with it in it}})$$\n",
    "\n",
    "The **_TF-IDF_** value for a given word in a given document is just found by multiplying the two!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions/Topics \n",
    "- Next time: vectorization\n",
    "- Vs Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:56:51.217196Z",
     "start_time": "2020-02-05T17:56:51.214208Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-05T17:56:54.768705Z",
     "start_time": "2020-02-05T17:56:54.750115Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-37ea0caf7368>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_corpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         \"\"\"\n\u001b[1;32m   1858\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1859\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1860\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1220\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \"\"\"\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "vectorizer.fit_transform(df_corpus['clean_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
