{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ByfUTZodQsND"
   },
   "source": [
    "# Sect 40: Neural Networks - Intro to Deep Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions/ Comments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Start By Discussing Biological Neural Networks (powerpoint)\n",
    "- Connect back to introduction from Learn\n",
    "- Demonstrate / play with Neural Network with Tensorflow Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uceu7mnytDno"
   },
   "source": [
    "# Artificial Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlvqq-TbP5zO"
   },
   "source": [
    "    \n",
    "- **The purpose of a neural network is to model $\\hat y \\approx y$ by minimizing loss/cost functions using gradient descent.**\n",
    "\n",
    "- Neural networks are very good with unstructured data. (images, audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-introduction-to-neural-networks-online-ds-ft-100719/master/images/new_first_network_num.png\" width=50%%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlvqq-TbP5zO"
   },
   "source": [
    "- **Networks are comprised of sequential layers of neurons/nodes.**\n",
    "    - Each neuron applies a **linear transformation** and an **activation function** and outputs its results to all neurons in the next layer.\n",
    "    - Minimizing Loss functions by adjusting parameters (weights and bias) of each connection using gradient descent (forward and back propagation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlvqq-TbP5zO"
   },
   "source": [
    "- **Activation functions** control the output of a neuron.($\\hat y =f_{activation}(x)$ )\n",
    "    - Most basic activation function is sigmoid functin ($\\hat y =\\sigma(x)$)\n",
    "    - Choice of activation function controls the size/range of the output.\n",
    "- **Linear transformations** ( $z = w^T x + b$ ) are used control the output of the activation function .\n",
    "    - where $w^T $ is the weight(/coefficient), $x$ is the input, and  $b$ is a bias. \n",
    "        - weights: \n",
    "        - bias:\n",
    "        \n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-40-02-introduction-to-neural-networks-online-ds-ft-021119/master/figures/log_reg.png\">\n",
    "\n",
    "\n",
    "\n",
    "- **Loss functions** ($\\mathcal{L}(\\hat y, y) $)  measure inconsistency between predicted ($\\hat y$) and actual $y$\n",
    "    - will be optimized using gradient descent\n",
    "    - defined over 1 traning sample\n",
    "- **Cost functions** takes the average loss over all of the samples.\n",
    "    - $J(w,b) = \\displaystyle\\frac{1}{l}\\displaystyle\\sum^l_{i=1}\\mathcal{L}(\\hat y^{(i)}, y^{(i)})$\n",
    "    - where $l$ is the number of samples\n",
    "\n",
    "\n",
    "- **Forward propagation** is the calculating  loss and cost functions.\n",
    "- **Back propagation** involves using gradient descent to update the values for  $w$ and $b$.\n",
    "    - $w := w- \\alpha\\displaystyle \\frac{dJ(w)}{dw}$ <br><br>\n",
    "    - $b := b- \\alpha\\displaystyle \\frac{dJ(b)}{db}$\n",
    "\n",
    "        - where $ \\displaystyle \\frac{dJ(w)}{dw}$ and $\\displaystyle \\frac{dJ(b)}{db}$ represent the *slope* of the function $J$ with respect to $w$ and $b$ respectively\n",
    "        - $\\alpha$ denote the *learning rate*. \n",
    "        \n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/fsds_100719_cohort_notes/master/images/neural_network_steps.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Note On Shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inputs:\n",
    "    - $n$: Number of inputs (columns) in the feature vector \n",
    "    - $l$: Number of items (rows) in the training set \n",
    "    - $m$: Number of items (rows) in the test set\n",
    "    \n",
    "- Input X:\n",
    "    - Will have shape $n$ x $l$ (number of features x number of training data points/rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlvqq-TbP5zO"
   },
   "source": [
    "### Using the chain rule for updating parameters with sigmoid activation function example:\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-40-02-introduction-to-neural-networks-online-ds-ft-021119/master/figures/log_reg_deriv.png\" >\n",
    "- $\\displaystyle \\frac{dJ(w,b)}{dw_i} = \\displaystyle\\frac{1}{l}\\displaystyle\\sum^l_{i=1} \\frac{d\\mathcal{L}(\\hat y^{(i)}, y^{(i)})}{dw_i}$\n",
    " \n",
    " \n",
    "- For each training sample $1,...,l$ you'll need to compute:\n",
    "\n",
    "    - $ z^{(i)} = w^T x^ {(i)} +b $\n",
    "\n",
    "    - $\\hat y^{(i)} = \\sigma (z^{(i)})$\n",
    "\n",
    "    - $dz^{(i)} = \\hat y^{(i)}- y^{(i)}$\n",
    "\n",
    "- Then, you'll need to make update:\n",
    "\n",
    "    - $J_{+1} = - [y^{(i)} \\log (\\hat y^{(i)}) + (1-y^{(i)}) \\log(1-\\hat y^{(i)})$ (for the sigmoid function)\n",
    "\n",
    "    - $dw_{1, +1}^{(i)} = x_1^{(i)} * dz^{(i)}$\n",
    "\n",
    "    - $dw_{2, +1}^{(i)} = x_2^{(i)} * dz^{(i)}$\n",
    "\n",
    "    - $db_{+1}^{(i)} =  dz^{(i)}$\n",
    "\n",
    "    - $\\dfrac{J}{m}$, $\\dfrac{dw_1}{m}$, $\\dfrac{dw_1}{m}$, $\\dfrac{db}{m}$\n",
    "\n",
    "- After that, update: \n",
    "\n",
    "    $w_1 := w_1 - \\alpha dw_1$\n",
    "\n",
    "    $w_2 := w_2 - \\alpha dw_2$\n",
    "\n",
    "    $b := b - \\alpha db$\n",
    "\n",
    "    repeat until convergence!\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlvqq-TbP5zO"
   },
   "source": [
    "## Activation Functions (will call $f_a$ here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlvqq-TbP5zO"
   },
   "source": [
    "- **sigmoid:**<br><img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-40-04-deeper-neural-networks-online-ds-ft-021119/master/index_files/index_33_1.png\" width=200>\n",
    "    - $ f_a=\\dfrac{1}{1+ \\exp(-z)}$\n",
    "    - outputs 0 to +1\n",
    "    \n",
    "- **tanh (hyperbolic tan):**<br><img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-40-04-deeper-neural-networks-online-ds-ft-021119/master/index_files/index_36_1.png\" width=200> <br>(Note:title is incorrect)\n",
    "    - $f_a = =\\dfrac{\\exp(z)- \\exp(-z)}{\\exp(z)+ \\exp(-z)}$\n",
    "    - outputs -1 to +1\n",
    "    - Generally works well in intermediate layers\n",
    "    - one of most popular functions\n",
    "    \n",
    "- **arctan**<br><img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-40-04-deeper-neural-networks-online-ds-ft-021119/master/index_files/index_40_1.png\" width=200>\n",
    "    -  similar qualities as tanh, but slope is more gentle than tanh\n",
    "    - outputs ~ 1.6 to 1.6\n",
    "    \n",
    "-  **Rectified Linear Unit (relu):**<br><img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-40-04-deeper-neural-networks-online-ds-ft-021119/master/index_files/index_43_1.png\" width=200>\n",
    "    - most popular activation function\n",
    "    - Activation is exactly 0 when Z <0\n",
    "    - Makes taking directives slightly cumbersome\n",
    "    - $f_a=\\max(0,z)$\n",
    "- **leaky_relu:**<br><img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-40-04-deeper-neural-networks-online-ds-ft-021119/master/index_files/index_46_1.png\" width=200>\n",
    "    -  altered version of relu where the activatiom is slightly negative when $z<0$\n",
    "    - $f_a=\\max(0.001*z,z)$\n",
    "\n",
    "### Additional Resources\n",
    " - Good summary article with helpful visuals:\n",
    "    - https://towardsdatascience.com/multi-layer-neural-networks-with-sigmoid-function-deep-learning-for-rookies-2-bf464f09eb7f \n",
    "- Interactive Visualizer for Activation Functions\n",
    "    - https://dashee87.github.io/deep%20learning/visualising-activation-functions-in-neural-networks/\n",
    "    \n",
    " ### Section Summary:\n",
    " - Compared to more tradistional statistics and ML techniques, neural networks perform particulary well when using unstructured data\n",
    " - Apart from densely connected networks, otyher types of neural networks include convolutional neural networks, recurrent neural networks, and generative adversarial neural networks.\n",
    " - When working with image data, it's important to understand how image data is stored when working with them in Python\n",
    "- Single Layer Neural Network with a sigmoid activation function very similar to logistic regression.\n",
    "- Backward and forward propagation are used to estimate the so-called \"model weights\"\n",
    "- Adding more layers to neural networks can substantially increase model performance\n",
    "- Several activations can be used in model nodes, you can explore with different types and evaluate how it affects performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T22:40:27.163071Z",
     "start_time": "2020-05-21T22:40:27.159937Z"
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# # q\n",
    "# def sigmoid(x, derivative=False):\n",
    "#     f = 1 / (1 + np.exp(-x))\n",
    "#     if (derivative == True):\n",
    "#         return f * (1 - f)\n",
    "#     return f\n",
    "\n",
    "# def tanh(x, derivative=False):\n",
    "#     f = np.tanh(x)\n",
    "#     if (derivative == True):\n",
    "#         return (1 - (f ** 2))\n",
    "#     return np.tanh(x)\n",
    "\n",
    "# def relu(x, derivative=False):\n",
    "#     f = np.zeros(len(x))\n",
    "#     if (derivative == True):\n",
    "#         for i in range(0, len(x)):\n",
    "#             if x[i] > 0:\n",
    "#                 f[i] = 1  \n",
    "#             else:\n",
    "#                 f[i] = 0\n",
    "#         return f\n",
    "#     for i in range(0, len(x)):\n",
    "#         if x[i] > 0:\n",
    "#             f[i] = x[i]  \n",
    "#         else:\n",
    "#             f[i] = 0\n",
    "#     return f\n",
    "\n",
    "# def leaky_relu(x, leakage = 0.05, derivative=False):\n",
    "#     f = np.zeros(len(x))\n",
    "#     if (derivative == True):\n",
    "#         for i in range(0, len(x)):\n",
    "#             if x[i] > 0:\n",
    "#                 f[i] = 1  \n",
    "#             else:\n",
    "#                 f[i] = leakage\n",
    "#         return f\n",
    "#     for i in range(0, len(x)):\n",
    "#         if x[i] > 0:\n",
    "#             f[i] = x[i]  \n",
    "#         else:\n",
    "#             f[i] = x[i]* leakage\n",
    "#     return f\n",
    "\n",
    "# def arctan(x, derivative=False):\n",
    "#     if (derivative == True):\n",
    "#         return 1/(1+np.square(x))\n",
    "#     return np.arctan(x)\n",
    "\n",
    "\n",
    "\n",
    "# def plot_activation(fn):\n",
    "#     z = np.arange(-10, 10, 0.2)\n",
    "#     y = fn(z)\n",
    "#     dy = fn(z, derivative=True)\n",
    "#     fig,ax=plt.subplots(figsize=(6,4))\n",
    "#     ax.set_title(f'{fn.__name__}')\n",
    "#     ax.set(xlabel='Input',ylabel='Output')\n",
    "#     ax.axhline(color='gray', linewidth=1,)\n",
    "#     ax.axvline(color='gray', linewidth=1,)\n",
    "#     ax.plot(z, y, 'r', label='original (y)')\n",
    "#     ax.plot(z, dy, 'b', label='derivative (dy)')\n",
    "#     ax.legend();\n",
    "#     plt.show()\n",
    "# ## Plot activation functions\n",
    "# act_funcs = [sigmoid,tanh,arctan,relu,leaky_relu]\n",
    "# [plot_activation(fn) for fn in act_funcs]\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IA6ZLbuoQv8D"
   },
   "source": [
    "# Deeper Neural Networks:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hHZ0q-QYomgR"
   },
   "source": [
    "       \n",
    "- **Advantages:**\n",
    "    - largely eliminates need for feature engineering\n",
    "    - multiple levels of information processing in one networking.\n",
    "        - Ex: for images:\n",
    "            - First layer detects edges\n",
    "            - second layer gorups edges and detects patterns\n",
    "            - more layers group even bigger parts together\n",
    "        - Ex: for audio:\n",
    "            - first layer: low level wave features\n",
    "            - second: basic units of sounds (\"phonemes\")\n",
    "            - third: word recognition\n",
    "            - fourth: sentence recognition\n",
    "    \n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-40-04-deeper-neural-networks-online-ds-ft-021119/master/figures/small_deeper.png\">\n",
    "\n",
    "- Networks are comprised of sequential layers of neurons/nodes.\n",
    "    - \\# of layers = hidden+output layer\n",
    "        - The input layer is not counted as formal layer.\n",
    "    - All layers except the final are _hidden layers_.\n",
    "\n",
    "\n",
    "## NOTATION:\n",
    "Generally, the output of layer $j$ is denoted as $a^{[j]}$.\n",
    "\n",
    "**For our 2-layer neural network above, this means that:**\n",
    "\n",
    "- $x = a^{[0]}$  as x is what comes out of the input layer\n",
    "- $a^{[1]} = \\begin{bmatrix} a^{[1]}_1  \\\\ a^{[1]}_2 \\\\ a^{[1]}_3  \\\\\\end{bmatrix}$ is the value generated by the hidden layer\n",
    "- $\\hat y =  a^{[2]}$, the output layer will generate a value $a^{[2]}$, which is equal to $\\hat y$.\n",
    "\n",
    "\n",
    "<br>For the **first node** in the hidden layer:\n",
    "- The linear transformation that occurs is:  $ z^{[1]}_1 = w^{[1]}_1 x +b^{[1]}_1$,\n",
    "    - Where $w$ = the weight, and $b$ = bias\n",
    "\n",
    "- For **all nodes** in the hidden layer:\n",
    "    - $ z^{[1]}_1 = w^{[1]}_1 x +b^{[1]}_1$ and  $a^{[1]}_1= f(z^{[1]}_1)$\n",
    "\n",
    "    - $ z^{[1]}_2 = w^{[1]}_2 x +b^{[1]}_2$ and $a^{[1]}_2= f(z^{[1]}_2)$\n",
    "\n",
    "    - $ z^{[1]}_3 = w^{[1]}_3 x +b^{[1]}_3$ and $a^{[1]}_3= f(z^{[1]}_3)$\n",
    "\n",
    "The **dimensions** of the elements:\n",
    "\n",
    "- $w^{[1]} = \\begin{bmatrix} w^{[1]}_{1,1}  & w^{[1]}_{2,1} & w^{[1]}_{3,1}  \\\\ w^{[1]}_{1,2}  & w^{[1]}_{2,2} & w^{[1]}_{3,2}\\end{bmatrix}$\n",
    "    - where, eg. $w^{[1]}_{1,2}$ denotes the weight of the arrow going **from $x_2$ into the first node** of the hidden layer. \n",
    "\n",
    "\n",
    "- When multiplying the transpose of this matrix (making it a 2 x 3 matrix) \n",
    "    - $w^{[1]T}_1$ with $x = \\begin{bmatrix} x_1  \\\\x_2\\end{bmatrix}$ and add $b^{[1]} = \\begin{bmatrix} b^{[1]}_1  \\\\b^{[1]}_2 \\\\ b^{[1]}_3 \\end{bmatrix}$,\n",
    "    - we obtain $z^{[1]} = \\begin{bmatrix} z^{[1]}_1  \\\\z^{[1]}_2 \\\\ z^{[1]}_3 \\end{bmatrix}$.\n",
    "\n",
    "----\n",
    "\n",
    "- The activation function is   $a^{[1]}_1= f(z^{[1]}_1)$.\n",
    "$w^{[1]}_{1,2}$\n",
    "\n",
    "$w^{[1]} = \\begin{bmatrix} w^{[1]}_{1,1}  & w^{[1]}_{2,1} & w^{[1]}_{3,1}  \\\\ w^{[1]}_{1,2}  & w^{[1]}_{2,2} & w^{[1]}_{3,2}\\end{bmatrix}$ \n",
    "\n",
    "[Reminder: $x = \\begin{bmatrix} x_1  \\\\x_2\\end{bmatrix} \\equiv a^{[0]}$ and that $a^{[2]} = \\hat y$ ]\n",
    "\n",
    "- Then, given input $x$:\n",
    "\n",
    "    - $z^{[1]} = w^{[1]T} a^{[0]} + b^{[1]}$\n",
    "\n",
    "    - $a^{[1]} = f(z^{[1]})$\n",
    "\n",
    "    - $z^{[2]} = w^{[2]T} a^{[1]} + b^{[2]}$\n",
    "\n",
    "    - $a^{[2]} = f(z^{[1]})$\n",
    "    \n",
    "    \n",
    "- When adding in several training samples ($i$), these become:\n",
    "    - $z^{[1](i)} = w^{[1]T} a^{[0](i)} + b^{[0]}$\n",
    "\n",
    "    - $a^{[1](i)} = f(z^{[1](i)})$\n",
    "\n",
    "    - $z^{[2](i)} = w^{[2]T} a^{[1](i)} + b^{[2]}$\n",
    "\n",
    "    - $a^{[2](i)} = f(z^{[1](i)})$\n",
    "    \n",
    "    \n",
    "### Process Summary\n",
    "- We begin by defining a model architecture which includes the number of hidden layers, activation functions (sigmoid or relu) and the number of units in each of these.  \n",
    "- We then initialize parameters for each of these layers (typically randomly). After the initial parameters are set, forward propagation evaluates the model giving a prediction, which is then used to evaluate a cost function. Forward propogation involves evaluating each layer and then piping this output into the next layer. \n",
    "- Each layer consists of a linear transformation and an activation function.  The parameters for the linear transformation in **each** layer include $W^l$ and $b^l$. The output of this linear transformation is represented by $Z^l$. This is then fed through the activation function (again, for each layer) giving us an output $A^l$ which is the input for the next layer of the model.  \n",
    "- After forward propogation is completed and the cost function is evaluated, backpropogation is used to calculate gradients of the initial parameters with respect to this cost function. Finally, these gradients are then used in an optimization algorithm, such as gradient descent, to make small adjustments to the parameters and the entire process of forward propogation, back propogation and parameter adjustments is repeated until the modeller is satisfied with the results.\n",
    "\n",
    "\n",
    "### Parameter Summary (Deep Networks Lesson):\n",
    "Notation for when there are $L$ layers present (and $l$ is current layer)\n",
    "\n",
    "**Parameters for the linear transformation: **  \n",
    "\n",
    "$W^{[l]}: (n^{[l]}, n^{[l-1]})$\n",
    "\n",
    "$b^{[l]}: (n^{[l]}, 1)$\n",
    "\n",
    "$dW^{[l]}: (n^{[l]}, n^{[l-1]})$\n",
    "\n",
    "$db^{[l]}: (n^{[l]}, 1)$\n",
    "\n",
    "**Parameters for the activation function**  \n",
    "\n",
    "$ a^{[l]}, z^{[l]}: (n^{[l]}, 1)$\n",
    "\n",
    "$ Z^{[l]}, A^{[l]}: (n^{[l]}, m)$\n",
    "\n",
    "$ dZ^{[l]}, dA^{[l]}: (n^{[l]}, m)$\n",
    "\n",
    "\n",
    "\n",
    "### Forward propagation:\n",
    "- Process: \n",
    "    - evaluating a cost function associated with the output of the neural network by successively calculating the output of each layer given initial parameter values, and passing this output on to the next layer until a finalized output has been calculated and the cost function can then be evaluated..\n",
    "        - Input is $a^{[l-1]}$\n",
    "        - Output $a^{[l]}$, save $z^{[l]}, w^{[l]}, b^{[l]}, a^{[l-1]} $\n",
    "    \n",
    "- $Z^1$ is the output of the linear transformation of the initial input $A^1$ (the observations).\n",
    "- In successive layers, $A^l$ is the output from the previous hidden layer. \n",
    "- In all of these cases, $W^l$ is a matrix of weights to be optimized minimize the cost function. $b^l$ is also optimized but is a vector as opposed to a matrix.  \n",
    "\n",
    "- $g^l$ is the activation function which takes the output of this linear transformation and yields the input to the next hidden layer.  \n",
    "- $ Z^{[l]}= W^{[l]} A^{[l-1]} + b^{[l]}$\n",
    "\n",
    "- $ A^{[l]}= g^{[l]} ( Z^{[l]})$\n",
    "\n",
    "- Shape: here, $ Z^{[l]}, A^{[l]}$ both have a shape of $(n^{[l]}, m)$\n",
    "\n",
    "    - where $n$ the nodes in the layer $l$\n",
    "    \n",
    "    \n",
    "### Backward Propagation:\n",
    "- Once an output for the neural network given the current parameter weights has been calculated, we must back propogate to calculate the gradients of layer parameters with respect to the cost function.\n",
    "    - This will allow us to apply an optimization algorithm such as gradient descent in order to make small adjustments to the parameters in order to minimize our cost (and improve our predictions).\n",
    "    - Input $da ^{[l]}$\n",
    "    - Output $da^{[l-1]}$, $dW^{[l]}, db^{[l]}$\n",
    "\n",
    "\n",
    "- **The gradients for our respective parameters are given by:**\n",
    "   \n",
    "    - $ dZ^{[l]}= dA ^{[l]} * g^{[l]'} (Z^{[l]})$\n",
    "\n",
    "    - $ dW^{[l]} = \\dfrac{1}{m} dZ^{[l]}* A^{[l-1]T}$\n",
    "\n",
    "    - $ db^{[l]} = \\dfrac{1}{m} np.sum(dZ^{[l]}, axis=1, keepdims=True)$\n",
    "\n",
    "    - $ dA^{[l-1]} = W^{[l]T}*dZ^{[l]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF STUDY GROUP [?]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JkJpdZlv3VNe"
   },
   "source": [
    "# Intro to Keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LYecj1xvmvEU"
   },
   "source": [
    "## Keras Basics\n",
    "- Tensors dimensions:\n",
    "    - Scalars = 0D tensors\n",
    "    - Vectors = 1D tensors\n",
    "    - Matrices = 2D tensors\n",
    "    - 3D tensors\n",
    "- A tensor is defined by 3 characteristics:\n",
    "    - rank or number of axes\n",
    "    - the shape\n",
    "    - the data type\n",
    "- Tensor basics - properties (from [here](https://pgaleone.eu/tensorflow/2018/07/28/understanding-tensorflow-tensors-shape-static-dynamic/#tensors-the-basic)):\n",
    "    - name\n",
    "    - type:\n",
    "        - tf.float32, tf.int64, tf.string\n",
    "    - rank:\n",
    "        - the number of dimension or the tensor. \n",
    "        - scalar = 0, vector = 1, etc.\n",
    "    - shape:\n",
    "\n",
    "## Important Data Manipulations in numpy\n",
    "\n",
    "- **Unrowing matrices:**\n",
    "    - e.g. turning a matrix of 790 images, which are 64 x 64 pixels and in RBG (3 colors) a (790, 64, 64, 3) matrix  into a matrix with 1 row for each image a ( 64*64*3, 790) matrix\n",
    "    - img_unrow = img.reshape(790, -1).T\n",
    "        - reshape -1 essentially means \"figure out how many, based upon the dat'\n",
    "- **Increasing the rank:**\n",
    "    - Vector with `np.shape()` returns  `(790,)`\n",
    "    - `np.reshape(vector, (1, 790))`\n",
    "- **Tensor indexling/slicing**\n",
    "    - just as python, `tensor[start_idx : end_idx]`\n",
    "    - left inclusive, right exclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T00:06:06.950877Z",
     "start_time": "2020-02-12T00:06:00.483201Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "NbbVDhN0rDhV",
    "outputId": "4b5f722d-e72f-42bd-905b-b965aee215f8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Tensor shape: (60000, 28, 28)\n",
      "Tensor Slice [0:100] shape: (100, 28, 28)\n",
      "Tensor Slice [0:100] shape: (100, 28, 28)\n",
      "Tensor Slice [0:100] shape: (100, 28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tensor indexing example using images\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "digit = train_images[10] #Select an arbitrary case for our example\n",
    "\n",
    "#Checking the shape of our tensor (in this case, the image)\n",
    "print('Raw Tensor shape:', train_images.shape)\n",
    "\n",
    "#Now performing some slices of our image:\n",
    "print('Tensor Slice [0:100] shape:', train_images[:100].shape)\n",
    "\n",
    "#Equivalently\n",
    "print('Tensor Slice [0:100] shape:', train_images[:100, :, :].shape)\n",
    "\n",
    "#Or verbosely:\n",
    "print('Tensor Slice [0:100] shape:', train_images[:100, :28, :28].shape)\n",
    "\n",
    "\n",
    "plt.imshow(digit, cmap=plt.cm.binary) #Display an example image for context\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T00:06:07.117356Z",
     "start_time": "2020-02-12T00:06:06.952313Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "id": "6f_oIbrIrpn2",
    "outputId": "984d6db8-36d0-442f-bc7e-d98875c159fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sliced tensor shape (includes all images but only the lower right hand corner of each: (60000, 14, 14)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# In the above example, we sliced our tensor to obtain 100 of the 60,000 images. \n",
    "# You can also slice tensors along other axes. For example, \n",
    "# the 1st dimension is which image we are referring two,\n",
    "# while the 2nd and 3rd axis are the pixels of these images themselves.\n",
    "# For example, we could limit the images to the bottom right hand quadrant like this:\n",
    "lower_right_quadrant = train_images[:,14:,14:]\n",
    "print('Sliced tensor shape (includes all images but only the lower right hand corner of each:',\n",
    "      lower_right_quadrant.shape)\n",
    "plt.imshow(lower_right_quadrant[10], cmap=plt.cm.binary) #Display the 10th image from our sliced tensor.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ByGu747-rxb_"
   },
   "source": [
    "## Tensor Operations\n",
    "- **Element-wise**\n",
    "    - each element from one tensor added/etc to the corresponding elements\n",
    "- **Broadcasting**\n",
    "    - used for tensors of different dimensions\n",
    "    - in example below, when adding a (3,) vector to a (4,3) matrix, a copy of the vector is added to each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T00:06:07.126072Z",
     "start_time": "2020-02-12T00:06:07.118939Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "colab_type": "code",
    "id": "BalyDaI7rwso",
    "outputId": "36b199b5-0aa7-4a80-d2e4-87bc7f065e33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A - (shape(4, 3)):\n",
      " [[ 0  1  2]\n",
      " [ 3  4  5]\n",
      " [ 6  7  8]\n",
      " [ 9 10 11]]\n",
      "\n",
      "B -  (shape(3,)):\n",
      " [1 2 3]\n",
      "\n",
      "Updated A:\n",
      " [[ 1  3  5]\n",
      " [ 4  6  8]\n",
      " [ 7  9 11]\n",
      " [10 12 14]]\n"
     ]
    }
   ],
   "source": [
    "# Example Broadcasting\n",
    "import numpy as np\n",
    "A=np.array([[ 0,  1,  2],\n",
    " [ 3,  4,  5],\n",
    " [ 6,  7,  8],\n",
    " [ 9, 10, 11]] )\n",
    "print(f'A - (shape{np.shape(A)}):\\n',A)\n",
    "B = np.array([1, 2, 3 ])\n",
    "print(f'\\nB -  (shape{np.shape(B)}):\\n',B)\n",
    "# Broadcasting B to add to A\n",
    "A += B\n",
    "print(f'\\nUpdated A:\\n',A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NuWdUhfzulNy"
   },
   "source": [
    "- **Tensor dot:**\n",
    "    - taking the dot product as in linear algebra\n",
    "    - a sum of element-wise products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T00:06:07.133072Z",
     "start_time": "2020-02-12T00:06:07.128022Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "XrxIRtr9umVK",
    "outputId": "7e211537-24c5-4b32-8633-413ba8d5e638"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-  (3,):\n",
      " [1 2 3]\n",
      "\n",
      "B.B - ():\n",
      " 14\n"
     ]
    }
   ],
   "source": [
    "# Simple dot product \n",
    "B = np.array([1,2,3])\n",
    "print(f'B-  {np.shape(B)}:\\n',B)\n",
    "\n",
    "#Taking the dot product of B and itself is equivalent to\n",
    "#1*1 + 2*2 + 3*3 = 1 + 4 + 9 = 14\n",
    "BdotB = np.dot(B,B)\n",
    "print(f'\\nB.B - {np.shape(BdotB)}:\\n',BdotB)\n",
    "# np."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T00:06:07.142148Z",
     "start_time": "2020-02-12T00:06:07.134636Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177
    },
    "colab_type": "code",
    "id": "_VKfO6INvbkY",
    "outputId": "9622b289-7eed-453c-e2ae-4d26760cf88a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[ 0  1  2]\n",
      " [ 3  4  5]\n",
      " [ 6  7  8]\n",
      " [ 9 10 11]] \n",
      "\n",
      "B: [1 2 3] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 8, 26, 44, 62])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# More complicated example\n",
    "# Here the first element is the sum of the first row of A multiplied by B elementwise:  \n",
    "## 0*1 + 1*2 + 2*3 = 0 + 2 + 6 = 8  \n",
    "# Followed by the sum of the second row of A multiplied by B elementwise:  \n",
    "## 3*1 + 4*2 + 5*3 = 3 + 8 + 15 = 26\n",
    "# and so on.\n",
    "A = np.array(range(12)).reshape(4,3)\n",
    "print('A:\\n', A, '\\n')\n",
    "\n",
    "B = np.array([1,2,3])#.reshape(1, -1)\n",
    "print('B:', B, '\\n')\n",
    "\n",
    "np.dot(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bu64jh4K3oKt"
   },
   "source": [
    "# Basics of Building a Neural Network with Keras:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TNk4Uz27wVZc"
   },
   "source": [
    "**Basics of Building a Neural Network with Keras:**\n",
    "1. Import required modules\n",
    "2. Decide on a network architecture (have only discussed sequential thus far)\n",
    "3. Adding layers - specifying layer type, number of neurons, activation functions, and, optionally, the input shape.\n",
    "4. Compile the model:\n",
    "    - Specify optimiziers\n",
    "    - specify loss functions\n",
    "    - specify metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T00:06:08.168799Z",
     "start_time": "2020-02-12T00:06:07.144158Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Ary82Ge-wfTb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From //anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(10, activation = 'relu', input_shape=(64*64*3, 790 )))\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss='mse',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "58e3B6PDzIjZ"
   },
   "source": [
    "5. Training the model\n",
    "    - `model.fit(X_train, y_train, epochs=20,batch_size=512,validation_data=(x_val,y_val))`\n",
    "    \n",
    "    - **batches:**\n",
    "        - a set of N samples, processed independently in parallel\n",
    "        - a batch determines how many samples are fed through before back-propagation. \n",
    "        - model only updates after a batch is complete.\n",
    "        - ideally have as large of a batch as your hardware can handle without going out of memory.\n",
    "            - larger batches usually run faster than smaller ones for evaluation/prediction. \n",
    "    - **epoch:**\n",
    "        - arbitrary cutoff / \"one pass over the entire dataset\", useful for logging and periodic evaluation\n",
    "        - when using kera's `model.fit` parameters `validation_data` or `validation_split`, these evaluations run at the end of every epoch.\n",
    "        - Within Keras can add callbacksto be run at the end of an epoch. Examples of these are learning rate changes and model checkpointing (saving).\n",
    "        \n",
    "    - **`history =  model.fit()` creates history object with .history attribute.**\n",
    "        - `history.history()` returns a dictionary of metrics from each epoch. \n",
    "            - `history.history['loss']` and `history.history['acc']`\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T04:21:16.054894Z",
     "start_time": "2020-02-12T04:21:16.052790Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "YKYmTkXmzI_G"
   },
   "outputs": [],
   "source": [
    "# # Fitting the model\n",
    "# history = model.fit(train_images,train_labels, epochs=20,\n",
    "#                     batch_size=10, validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "99nQr20j2YFr"
   },
   "source": [
    "### Additional Keras - Getting Started Links:\n",
    "* https://keras.io/getting-started/\n",
    "* https://keras.io/getting-started/sequential-model-guide/#compilation\n",
    "* https://www.coursera.org/learn/deep-neural-network/lecture/BhJlm/rmsprop\n",
    "* https://www.coursera.org/learn/deep-neural-network/lecture/qcogH/mini-batch-gradient-descent\n",
    "* A full book on Keras by the author of Keras himself:  \n",
    "    https://www.manning.com/books/deep-learning-with-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u2NJkNCFN9kQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GOVrYSEuDHhD"
   },
   "source": [
    "# Using Keras for Text Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S_vvC98X20Zj"
   },
   "source": [
    "## Preprocessing Text with Keras\n",
    "[Link for Learn.co Keras Lab](https://github.com/jirvingphd/dsc-04-41-05-keras-lab-online-ds-ft-021119/tree/solution)\n",
    "\n",
    "- The Keras Lab uses a neural network to analyze the text inside of filed bank complaints.\n",
    "- The complaint's text preprocessing in this lab involves several steps:\n",
    "    1. create word vector counts ( a bag of words types of representation)\n",
    "    2. Change category labels to integers\n",
    "    3. Train-test-split the processed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T00:06:08.463792Z",
     "start_time": "2020-02-12T00:06:00.512Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213
    },
    "colab_type": "code",
    "id": "-IkgOmqY2XNc",
    "outputId": "3e34fee7-135b-4fd7-84f0-11b34fd7a72e"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Keras preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# Keras neural network basics\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "# Load in data\n",
    "data_url = 'https://raw.githubusercontent.com/jirvingphd/dsc-04-41-05-keras-lab-online-ds-ft-021119/solution/Bank_complaints.csv'\n",
    "df = pd.read_csv(data_url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T00:06:08.464825Z",
     "start_time": "2020-02-12T00:06:00.514Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "kaNiQGtE84Mc",
    "outputId": "fb65f9c3-a648-451a-99fe-fb76029fda61"
   },
   "outputs": [],
   "source": [
    "# Step 1. create word vector counts\n",
    "\n",
    "complaints = df[\"Consumer complaint narrative\"] \n",
    "tokenizer = Tokenizer(num_words=2000) #Initialize a tokenizer.\n",
    "\n",
    "tokenizer.fit_on_texts(complaints) #Fit it to the complaints\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(complaints) #Generate sequences\n",
    "print('sequences type:', type(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T00:06:08.465858Z",
     "start_time": "2020-02-12T00:06:00.517Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "QJb-TVqh89cd",
    "outputId": "1e5eeefe-c01b-4e81-f386-e25e87f45a40"
   },
   "outputs": [],
   "source": [
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary') #Similar to sequences, but returns a numpy array\n",
    "print('one_hot_results type:', type(one_hot_results))\n",
    "\n",
    "word_index = tokenizer.word_index #Useful if we wish to decode (more explanation below)\n",
    "\n",
    "print('Found %s unique tokens.' % len(word_index)) #Tokens are the number of unique words across the corpus\n",
    "\n",
    "\n",
    "print('Dimensions of our coded results:', np.shape(one_hot_results)) #Our coded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T00:06:08.466966Z",
     "start_time": "2020-02-12T00:06:00.520Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "colab_type": "code",
    "id": "lpbimdKa9J21",
    "outputId": "93e70fb7-20f8-4ca1-dc7d-75ae9a0c02f1"
   },
   "outputs": [],
   "source": [
    "reverse_index = {v:k for k,v in word_index.items()}\n",
    "comment_idx_to_preview = 19\n",
    "print('Original complaint text:')\n",
    "print(complaints[comment_idx_to_preview])\n",
    "print('\\n\\n')\n",
    "\n",
    "#The reverse_index cell block above must be complete in order for this cell block to successively execute.\n",
    "decoded_review = ' '.join([reverse_index.get(i) for i in sequences[comment_idx_to_preview]])\n",
    "# print('Decoded review from Tokenizer:')\n",
    "# print(decoded_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T00:06:08.468318Z",
     "start_time": "2020-02-12T00:06:00.523Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "TQeBPTAp9VLG",
    "outputId": "26e225d8-6c7e-4cab-e495-92ce5ddab068"
   },
   "outputs": [],
   "source": [
    "# Step 2: conveting descriptive categories into integers\n",
    "\n",
    "product = df[\"Product\"]\n",
    "\n",
    "le = preprocessing.LabelEncoder() #Initialize. le used as abbreviation fo label encoder\n",
    "le.fit(product)\n",
    "# print(\"Original class labels:\")\n",
    "# print(list(le.classes_))\n",
    "# print('\\n')\n",
    "product_cat = le.transform(product)  \n",
    "#list(le.inverse_transform([0, 1, 3, 3, 0, 6, 4])) #If you wish to retrieve the original descriptive labels post production\n",
    "\n",
    "# print('New product labels:')\n",
    "# print(product_cat)\n",
    "# print('\\n')\n",
    "\n",
    "# print('One hot labels; 7 binary columns, one for each of the categories.') #Each row will be all zeros except for the category for that observation.\n",
    "product_onehot = to_categorical(product_cat)\n",
    "# print(product_onehot)\n",
    "# print('\\n')\n",
    "# print('One hot labels shape:')\n",
    "# print(np.shape(product_onehot))\n",
    "\n",
    "# Step 3. Train-test-split\n",
    "import random\n",
    "random.seed(123)\n",
    "test_index = random.sample(range(1,10000), 1500)\n",
    "\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "print(\"Test label shape:\", np.shape(label_test))\n",
    "print(\"Train label shape:\", np.shape(label_train))\n",
    "print(\"Test shape:\", np.shape(test))\n",
    "print(\"Train shape:\", np.shape(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T00:06:08.469250Z",
     "start_time": "2020-02-12T00:06:00.525Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "o1gzmEH--Laq"
   },
   "outputs": [],
   "source": [
    "# Building neural network\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) \n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax')) # output layer with units = # of classes\n",
    "\n",
    "# Compil the network\n",
    "model.compile(optimizer='SGD', loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U-a3Ucli-Wlm"
   },
   "source": [
    "## Building/Compilng the neural network\n",
    "- For neural network:\n",
    "    - Use 2 hidden layers with the relu activation. layer one =50 units, layer two= 25\n",
    "    - **Becuase we are doing  multiclass (7 classes) problem, our final layer will use a softmax classifier to output 7 class probabilities per case.**\n",
    "\n",
    "- For compiling then model:\n",
    "    -  Loss function: 'categorical_crossentropy`\n",
    "    - Optimizer: stochastic gradient descent 'SGD'\n",
    "    - metric: 'accuracy'\n",
    "    \n",
    "- Fit fitting:\n",
    "    - Use 120 epochs\n",
    "    - use 256 batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T00:06:08.470361Z",
     "start_time": "2020-02-12T00:06:00.528Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "-rmU31MU_9Nn"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train,\n",
    "                    label_train,\n",
    "                    epochs=120,\n",
    "                    batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jsgxx8PNC-AE"
   },
   "source": [
    "## Visualizing and Evaluating Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T00:06:08.471582Z",
     "start_time": "2020-02-12T00:06:00.532Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "colab_type": "code",
    "id": "rnrM_JeJAAqa",
    "outputId": "5178b6be-51d3-4bbe-8cef-9df253aba8dd"
   },
   "outputs": [],
   "source": [
    "# Plotting the results from history\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "acc_values = history_dict['acc']\n",
    "# If we used a validation_set, could also plot:\n",
    "# val_acc_values = history_dict['val_acc']\n",
    "# val_loss_values = history_dict['val_loss']\n",
    "\n",
    "\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "fig,ax = plt.subplots(2,1, sharex=True, figsize=(6,6))\n",
    "ax[0].set_title('Training Results')\n",
    "ax[0].plot(epochs, loss_values, 'g', label='Training loss')\n",
    "# ax[0].plot(epochs, val_loss_values,'b',label='Validation loss')\n",
    "# ax[0].set_title('Training loss')\n",
    "# ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].legend()\n",
    "\n",
    "\n",
    "ax[1].plot(epochs, acc_values,'r', label='Training Acc')\n",
    "# ax[0].plot(epochs, val_acc_values,'b',label='Validation Acc')\n",
    "\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "ax[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "HfWTBNHLCGCg",
    "outputId": "67f92286-297d-4f6b-dbd0-37a8e9642e29"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T00:06:08.472774Z",
     "start_time": "2020-02-12T00:06:00.536Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "x4XNYJQxAeed",
    "outputId": "26693a30-ed62-4c3a-dc7d-1a84be7b4c6f"
   },
   "outputs": [],
   "source": [
    "# Making Predictions and Evaluating Performance\n",
    "\n",
    "y_hat_test = model.predict(test) #Your code here; Output (probability) predictions for the test set.\n",
    "\n",
    "# Evaluate training set\n",
    "results_train = model.evaluate(train, label_train)\n",
    "print(f'Training set: {results_train}\\n')\n",
    "\n",
    "# Evaluate testing set\n",
    "results_test = model.evaluate(test, label_test)\n",
    "print(f'Testing set: {results_test}\\n')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "My Flatiron Bootcamp Notes - Mod 4.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "325px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
