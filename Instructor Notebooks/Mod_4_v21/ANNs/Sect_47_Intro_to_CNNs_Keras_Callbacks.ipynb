{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Trd6RcQwP-m8"
   },
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "28h_NKJy7Yby"
   },
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9xi6_g9a7YTB"
   },
   "source": [
    "# STUDY GROUP RESOURCES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gp_2zRYT7YAz"
   },
   "source": [
    "\n",
    "- [NOTEBOOK: This Notebook on Google Colab](https://drive.google.com/file/d/1rxkuydyd_wNdiuKh22-e6mr-zlsRVC1N/view?usp=sharing)\n",
    "- [DATASET: Zip File for Dog vs Cats](https://drive.google.com/open?id=1WQ0fdJrNs5qVinJ_6rmVIIinOJD3p-n4)\n",
    "- [POWERPOINT: of Human Visual System](https://github.com/jirvingphd/fsds_100719_cohort_notes/blob/master/bio_neural_networks.pptx)\n",
    "- [VIDEO: Study Group Video Recording]()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pon77kz7oL0P"
   },
   "source": [
    "## LEARNING OBJECTIVES\n",
    "\n",
    "- Learn about the retina /human visual system\n",
    "- Relate human to CNNS\n",
    "- Discuss using colab/colab pro \n",
    "- Fitting, evaluating, saving CNNs\n",
    "- Transfer Learning & Pre-trained Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E7D1Ks2mhJim"
   },
   "source": [
    "# Loading in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MqhwVmwksJmu"
   },
   "source": [
    "- [Google Drive Link to zip file](https://drive.google.com/open?id=1WQ0fdJrNs5qVinJ_6rmVIIinOJD3p-n4 \n",
    "- https://medium.com/datadriveninvestor/speed-up-your-image-training-on-google-colab-dc95ea1491cf)\n",
    "\n",
    "```python\n",
    "from google.colab import drive\n",
    "drive.mount('/gdrive',force_remount=True)\n",
    "# %cd /gdrive\n",
    "%cd ~\n",
    "%cd ..\n",
    "\n",
    "\n",
    "import os,glob\n",
    "print(os.path.abspath(os.curdir))\n",
    "\n",
    "source_folder = r'/gdrive/My Drive/Datasets/'\n",
    "target_folder = r'/content/'\n",
    "file = glob.glob(source_folder+'*.zip',recursive=True)\n",
    "file=file[0]\n",
    "```\n",
    "\n",
    "```python\n",
    "## Unzip data\n",
    "zip_path = file\n",
    "\n",
    "!cp \"{zip_path}\" .\n",
    "\n",
    "!unzip -q dogs-vs-cats-sorted.zip\n",
    "!rm dogs-vs-cats-sorted.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "LZ5YIuJtmTsL",
    "outputId": "7142a19c-4fc2-4c9d-b934-ce7cbaf47b95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /gdrive\n",
      "/root\n",
      "/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive',force_remount=True)\n",
    "# %cd /gdrive/My\\ Drive\n",
    "%cd ~\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Tzlu4rAgMI6e",
    "outputId": "3e7c6f4b-359d-49c9-ef32-cb87018f44ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/gdrive/My Drive/Datasets/dogs-vs-cats-sorted.zip'"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os,glob\n",
    "\n",
    "print(os.path.abspath(os.curdir))\n",
    "\n",
    "source_folder = r'/gdrive/My Drive/Datasets/'\n",
    "# target_folder = r'/content/'\n",
    "# os.listdir(source_folder), os.listdir(target_folder)\n",
    "file = glob.glob(source_folder+'*.zip',recursive=True)[0]\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MnW9nB5yJ2W3"
   },
   "outputs": [],
   "source": [
    "zip_path = file\n",
    "\n",
    "!cp \"{zip_path}\" .\n",
    "\n",
    "!unzip -q dogs-vs-cats-sorted.zip\n",
    "!rm dogs-vs-cats-sorted.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "id": "U_HgT1oPKRFH",
    "outputId": "47e080ea-d32c-457f-f62f-672fb0b412ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['root',\n",
       " 'opt',\n",
       " 'mnt',\n",
       " 'lib64',\n",
       " 'srv',\n",
       " 'var',\n",
       " 'sys',\n",
       " 'run',\n",
       " 'proc',\n",
       " 'lib',\n",
       " 'usr',\n",
       " 'home',\n",
       " 'bin',\n",
       " 'etc',\n",
       " 'dev',\n",
       " 'tmp',\n",
       " 'media',\n",
       " 'sbin',\n",
       " 'boot',\n",
       " 'dogs-vs-cats-sorted',\n",
       " '__MACOSX',\n",
       " 'gdrive',\n",
       " '.dockerenv',\n",
       " 'datalab',\n",
       " 'tools',\n",
       " 'swift',\n",
       " 'dlib-19.18.0-cp36-cp36m-linux_x86_64.whl',\n",
       " 'tensorflow-2.1.0',\n",
       " 'content',\n",
       " 'dlib-19.18.0-cp27-cp27mu-linux_x86_64.whl',\n",
       " 'lib32']"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os,glob\n",
    "print(os.path.abspath(os.curdir))\n",
    "os.listdir()\n",
    "# os.chdir('My Drive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C7A_91wjihqa"
   },
   "source": [
    "# Functions from Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CyPrYuEkecxz"
   },
   "source": [
    "- Updated plot_keras_history (works with 'acc' or 'accuracy' metric name)\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9KjOLwwTqQtl"
   },
   "outputs": [],
   "source": [
    "# history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f8RY4Nr2ihgX"
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def plot_keras_history(history,figsize=(10,4),subplot_kws={}):\n",
    "    if hasattr(history,'history'):\n",
    "        history=history.history\n",
    "    figsize=(10,4)\n",
    "    subplot_kws={}\n",
    "\n",
    "    acc_keys = list(filter(lambda x: 'acc' in x,history.keys()))\n",
    "    loss_keys = list(filter(lambda x: 'loss' in x,history.keys()))\n",
    "\n",
    "    fig,axes=plt.subplots(ncols=2,figsize=figsize,**subplot_kws)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    y_labels= ['Accuracy','Loss']\n",
    "    for a, metric in enumerate([acc_keys,loss_keys]):\n",
    "        for i in range(len(metric)):\n",
    "            ax = pd.Series(history[metric[i]],\n",
    "                        name=metric[i]).plot(ax=axes[a],label=metric[i])\n",
    "    [ax.legend() for ax in axes]\n",
    "    [ax.xaxis.set_major_locator(mpl.ticker.MaxNLocator(integer=True)) for ax in axes]\n",
    "    [ax.set(xlabel='Epochs') for ax in axes]\n",
    "    plt.suptitle('Model Training Results',y=1.01)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(conf_matrix, classes = None, normalize=True,\n",
    "                          title='Confusion Matrix', cmap=\"Blues\",\n",
    "                          print_raw_matrix=False,\n",
    "                          fig_size=(4,4)):\n",
    "    \"\"\"Check if Normalization Option is Set to True. \n",
    "    If so, normalize the raw confusion matrix before visualizing\n",
    "    #Other code should be equivalent to your previous function.\n",
    "    Note: Taken from bs_ds and modified\n",
    "    - Can pass a tuple of (y_true,y_pred) instead of conf matrix.\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import sklearn.metrics as metrics\n",
    "    \n",
    "    ## make confusion matrix if given tuple of y_true,y_pred\n",
    "    if isinstance(conf_matrix, tuple):\n",
    "        y_true = conf_matrix[0].copy()\n",
    "        y_pred = conf_matrix[1].copy()\n",
    "        \n",
    "        if y_true.ndim>1:\n",
    "            y_true = y_true.argmax(axis=1)\n",
    "        if y_pred.ndim>1:\n",
    "            y_pred = y_pred.argmax(axis=1)\n",
    "        cm = metrics.confusion_matrix(y_true,y_pred)\n",
    "    else:\n",
    "        cm = conf_matrix\n",
    "        \n",
    "    ## Generate integer labels for classes\n",
    "    if classes is None:\n",
    "        classes = list(range(len(cm)))  \n",
    "        \n",
    "    ## Normalize data\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        fmt='.2f'\n",
    "    else:\n",
    "        fmt= 'd'\n",
    "        \n",
    "        \n",
    "    fontDict = {\n",
    "        'title':{\n",
    "            'fontsize':16,\n",
    "            'fontweight':'semibold',\n",
    "            'ha':'center',\n",
    "            },\n",
    "        'xlabel':{\n",
    "            'fontsize':14,\n",
    "            'fontweight':'normal',\n",
    "            },\n",
    "        'ylabel':{\n",
    "            'fontsize':14,\n",
    "            'fontweight':'normal',\n",
    "            },\n",
    "        'xtick_labels':{\n",
    "            'fontsize':10,\n",
    "            'fontweight':'normal',\n",
    "    #             'rotation':45,\n",
    "            'ha':'right',\n",
    "            },\n",
    "        'ytick_labels':{\n",
    "            'fontsize':10,\n",
    "            'fontweight':'normal',\n",
    "            'rotation':0,\n",
    "            'ha':'right',\n",
    "            },\n",
    "        'data_labels':{\n",
    "            'ha':'center',\n",
    "            'fontweight':'semibold',\n",
    "\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Create plot\n",
    "    fig,ax = plt.subplots(figsize=fig_size)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title,**fontDict['title'])\n",
    "    plt.colorbar()\n",
    "\n",
    "    tick_marks = classes#np.arange(len(classes))\n",
    "\n",
    "\n",
    "    plt.xticks(tick_marks, classes, **fontDict['xtick_labels'])\n",
    "    plt.yticks(tick_marks, classes,**fontDict['ytick_labels'])\n",
    "\n",
    "    # Determine threshold for b/w text\n",
    "    thresh = cm.max() / 2.\n",
    "\n",
    "    # fig,ax = plt.subplots()\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 color='darkgray',**fontDict['data_labels']) #color=\"white\" if cm[i, j] > thresh else \"black\"\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label',**fontDict['ylabel'])\n",
    "    plt.xlabel('Predicted label',**fontDict['xlabel'])\n",
    "\n",
    "    if print_raw_matrix:\n",
    "        print_title = 'Raw Confusion Matrix Counts:'\n",
    "        print('\\n',print_title)\n",
    "        print(conf_matrix)\n",
    "\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    return fig\n",
    "\n",
    "\n",
    "    \n",
    "def evaluate_model(y_true, y_pred,history=None):\n",
    "    from sklearn import metrics\n",
    "    if y_true.ndim>1:\n",
    "        y_true = y_true.argmax(axis=1)\n",
    "    if y_pred.ndim>1:\n",
    "        y_pred = y_pred.argmax(axis=1)   \n",
    "    try:    \n",
    "        if history is not None:\n",
    "            plot_keras_history(history)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    num_dashes=20\n",
    "    print('\\n')\n",
    "    print('---'*num_dashes)\n",
    "    print('\\tCLASSIFICATION REPORT:')\n",
    "    print('---'*num_dashes)\n",
    "    try:\n",
    "        print(metrics.classification_report(y_true,y_pred))\n",
    "        \n",
    "        fig = plot_confusion_matrix((y_true,y_pred))\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Error during model evaluation:\\n\\t{e}\")\n",
    "\n",
    "\n",
    "    \n",
    "class Timer():\n",
    "    def __init__(self, start=True,time_fmt='%m/%d/%y - %T'):\n",
    "        import tzlocal\n",
    "        import datetime as dt\n",
    "        \n",
    "        self.tz = tzlocal.get_localzone()\n",
    "        self.fmt= time_fmt\n",
    "        self._created = dt.datetime.now(tz=self.tz)\n",
    "        \n",
    "        if start:\n",
    "            self.start()\n",
    "            \n",
    "    def get_time(self):\n",
    "        import datetime as dt\n",
    "        return dt.datetime.now(tz=self.tz)\n",
    "\n",
    "        \n",
    "    def start(self,verbose=True):\n",
    "        self._laps_completed = 0\n",
    "        self.start = self.get_time()\n",
    "        if verbose: \n",
    "            print(f'[i] Timer started at {self.start.strftime(self.fmt)}')\n",
    "    \n",
    "    def stop(self, verbose=True):\n",
    "        self._laps_completed += 1\n",
    "        self.end = self.get_time()\n",
    "        self.elapsed = self.end -  self.start\n",
    "        if verbose: \n",
    "            print(f'[i] Timer stopped at {self.end.strftime(self.fmt)}')\n",
    "            print(f'  - Total Time: {self.elapsed}')\n",
    "    \n",
    "    \n",
    "    \n",
    "from sklearn.metrics import make_scorer\n",
    "def my_custom_scorer(y_true,y_pred,verbose=True):#,scoring='accuracy',verbose=True):\n",
    "    \"\"\"My custom score function to use with sklearn's GridSearchCV\n",
    "    Maximizes the average accuracy per class using a normalized confusion matrix\"\"\"\n",
    "\n",
    "    import sklearn.metrics as metrics\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import numpy as np\n",
    "\n",
    "    ## reduce dimensions of y_train and y_test\n",
    "    if y_true.ndim>1:            \n",
    "        y_true = y_true.argmax(axis=1)\n",
    "\n",
    "    if y_pred.ndim>1:\n",
    "        y_pred = y_pred.argmax(axis=1)\n",
    "        \n",
    "    evaluate_model(y_true,y_pred)\n",
    "    print('\\n\\n')\n",
    "    return metrics.accuracy_score(y_true,y_pred)\n",
    "\n",
    "\n",
    "\n",
    "def get_secret_password(file='/Users/jamesirving/.secret/gmail.json'):\n",
    "    with open(file) as file:\n",
    "        import json\n",
    "        gmail = json.loads(file.read())\n",
    "    # email_notification()\n",
    "    print(gmail.keys())\n",
    "    return gmail\n",
    "\n",
    "\n",
    "def email_notification(password_obj=None,subject='GridSearch Finished',\n",
    "                       msg='The GridSearch is now complete.'):\n",
    "    \"\"\"Sends email notification from gmail account using previously encrypyted password  object (an instance\n",
    "    of EncrypytedPassword). \n",
    "    Args:\n",
    "        password_obj (dict): Login info dict with keys: username,password.\n",
    "        subject (str):Text for subject line.\n",
    "        msg (str): Text for body of email. \n",
    "\n",
    "    Returns:\n",
    "        Prints `Email sent!` if email successful. \n",
    "    \"\"\"\n",
    "    if password_obj is None:\n",
    "        gmail = get_secret_password()\n",
    "    else:\n",
    "        assert ('username' in password_obj)&('password' in password_obj)\n",
    "        gmail = password_obj\n",
    "        \n",
    "    if isinstance(msg,str)==False:\n",
    "        msg=str(msg)\n",
    "        \n",
    "    \n",
    "    # import required packages\n",
    "    import smtplib\n",
    "    from email.mime.multipart import MIMEMultipart\n",
    "    from email.mime.text import MIMEText\n",
    "    from email.mime.image import MIMEImage\n",
    "    from email import encoders\n",
    "    \n",
    "\n",
    "    ## WRITE EMAIL\n",
    "    message = MIMEMultipart()\n",
    "    message['Subject'] =subject\n",
    "    message['To'] = gmail['username']\n",
    "    message['From'] = gmail['username']\n",
    "    message.attach(MIMEText(msg,'plain'))\n",
    "    text_message = message.as_string()\n",
    "\n",
    "\n",
    "    # Send email request\n",
    "    try:\n",
    "        with  smtplib.SMTP_SSL('smtp.gmail.com',465) as server:\n",
    "            \n",
    "            server.login(gmail['username'],gmail['password'])\n",
    "            server.sendmail(gmail['username'],gmail['username'], text_message)#text_message)\n",
    "            server.close()\n",
    "            print(f\"Email sent to {gmail['username']}!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('Something went wrong')\n",
    "       \n",
    "       \n",
    "       \n",
    "def prepare_gridsearch_report(grid_search,X_test,y_test,\n",
    "                              save_path = 'results/emails/'):\n",
    "    \"\"\"Creates a text report with grid search results \n",
    "    and saves it to disk. Text is returned and can be attached as \n",
    "    the `msg` param for email_notification'\"\"\"\n",
    "    ## Make folders for saving email contents\n",
    "    import os,sys\n",
    "    import sklearn.metrics as metrics\n",
    "    os.makedirs(save_path,exist_ok=True)\n",
    "    \n",
    "    ## Get time afor report\n",
    "    import datetime as dt\n",
    "    import tzlocal as tz\n",
    "    now = dt.datetime.now(tz.get_localzone())\n",
    "                  \n",
    "    time = now.strftime(\"%m/%d/%Y - %I:%M %p\")  \n",
    "    \n",
    "    ## filepaths for fig and report\n",
    "    fig_fpath = save_path+'confusion_matrix.png'\n",
    "    msg_text_path = save_path+'msg.txt'\n",
    "\n",
    "    \n",
    "    ## GET BEST PARAMS AND MODEL\n",
    "    best_params = str(grid_search.best_params_)\n",
    "    best_model = grid_search.best_estimator_#(grid.best_params_)\n",
    "    \n",
    "    # Get predictions\n",
    "    y_hat_test = best_model.predict(X_test)\n",
    "    \n",
    "    ## Get Classification report\n",
    "    report = metrics.classification_report(y_test.argmax(axis=1),y_hat_test)\n",
    "    \n",
    "    ## Get text confusion matrix\n",
    "    cm = np.round(metrics.confusion_matrix(y_test.argmax(axis=1),y_hat_test,normalize='true'),2)\n",
    "    cm_str = str(cm)\n",
    "\n",
    "          \n",
    "    ## Combine text for report\n",
    "    msg_text = [f'Grid Search Results from {time}:\\n']\n",
    "    msg_text = ['The best params were:\\n\\t']\n",
    "    msg_text.append(best_params)\n",
    "    msg_text.append('\\n\\n')\n",
    "    msg_text.append('Classification Report:\\n')\n",
    "    msg_text.append(report)\n",
    "    msg_text.append('\\n\\n')\n",
    "\n",
    "    msg_text.append('Confusion Matrix (normalized to true labels):\\n')\n",
    "    msg_text.append(cm_str)\n",
    "                  \n",
    "\n",
    "    \n",
    "    ## Save the text to file\n",
    "    with open(msg_text_path,'w+') as f:\n",
    "        f.writelines(msg_text)\n",
    "    print(f\"Message saved as {msg_text_path}\")\n",
    "                  \n",
    "    ## Load the (fixed) text from file\n",
    "    with open(msg_text_path,'r') as f:\n",
    "        txt = f.read()\n",
    "        \n",
    "    ## Plot and save confusion matrix\n",
    "    fig = plot_confusion_matrix((y_test,y_hat_test))\n",
    "    try:\n",
    "        fig.savefig(fig_fpath, dpi=300, facecolor='w', edgecolor='w', orientation='portrait',\n",
    "                    papertype=None, format=None, transparent=False, bbox_inches=None, pad_inches=0.1, frameon=None, metadata=None)\n",
    "        print(f\"Figure saved as {fig_fpath}\")           \n",
    "    except Exception as e:\n",
    "        print(f\"[!] ERROR saving figure:\\n\\t{e}\")\n",
    "        \n",
    "    return txt#,fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XX0sOBH8tKub"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M9kJLR7pgd5I"
   },
   "source": [
    "## Using Colab Pro "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "8XX6Sd5dpuOQ",
    "outputId": "9beef119-431f-4bf2-c91e-e028638c210d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar  5 17:34:58 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.59       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   38C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#https://colab.research.google.com/notebooks/pro.ipynb\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "  print('and then re-execute this cell.')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "CJCNXnmPqBC6",
    "outputId": "a306f738-aff7-4ca5-feef-7a38247b82e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your runtime has 27.4 gigabytes of available RAM\n",
      "\n",
      "You are using a high-RAM runtime!\n"
     ]
    }
   ],
   "source": [
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('To enable a high-RAM runtime, select the Runtime → \"Change runtime type\"')\n",
    "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
    "  print('re-execute this cell.')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8qbS0lSrghaO"
   },
   "source": [
    "## Installs & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:03:21.650842Z",
     "start_time": "2020-02-13T16:03:14.745924Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "KHc94KpTlbRj",
    "outputId": "4bd61760-6829-4868-a110-8f2a804c8059"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fsds_1007219  v0.7.16 loaded.  Read the docs: https://fsds.readthedocs.io/en/latest/ \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_af59f072_5f07_11ea_9620_0242ac1c0002\" ><caption>Loaded Packages and Handles</caption><thead>    <tr>        <th class=\"col_heading level0 col0\" >Handle</th>        <th class=\"col_heading level0 col1\" >Package</th>        <th class=\"col_heading level0 col2\" >Description</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                                <td id=\"T_af59f072_5f07_11ea_9620_0242ac1c0002row0_col0\" class=\"data row0 col0\" >dp</td>\n",
       "                        <td id=\"T_af59f072_5f07_11ea_9620_0242ac1c0002row0_col1\" class=\"data row0 col1\" >IPython.display</td>\n",
       "                        <td id=\"T_af59f072_5f07_11ea_9620_0242ac1c0002row0_col2\" class=\"data row0 col2\" >Display modules with helpful display and clearing commands.</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_af59f072_5f07_11ea_9620_0242ac1c0002row1_col0\" class=\"data row1 col0\" >fs</td>\n",
       "                        <td id=\"T_af59f072_5f07_11ea_9620_0242ac1c0002row1_col1\" class=\"data row1 col1\" >fsds_100719</td>\n",
       "                        <td id=\"T_af59f072_5f07_11ea_9620_0242ac1c0002row1_col2\" class=\"data row1 col2\" >Custom data science bootcamp student package</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_af59f072_5f07_11ea_9620_0242ac1c0002row2_col0\" class=\"data row2 col0\" >mpl</td>\n",
       "                        <td id=\"T_af59f072_5f07_11ea_9620_0242ac1c0002row2_col1\" class=\"data row2 col1\" >matplotlib</td>\n",
       "                        <td id=\"T_af59f072_5f07_11ea_9620_0242ac1c0002row2_col2\" class=\"data row2 col2\" >Matplotlib's base OOP module with formatting artists</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_af59f072_5f07_11ea_9620_0242ac1c0002row3_col0\" class=\"data row3 col0\" >plt</td>\n",
       "                        <td id=\"T_af59f072_5f07_11ea_9620_0242ac1c0002row3_col1\" class=\"data row3 col1\" >matplotlib.pyplot</td>\n",
       "                        <td id=\"T_af59f072_5f07_11ea_9620_0242ac1c0002row3_col2\" class=\"data row3 col2\" >Matplotlib's matlab-like plotting module</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_af59f072_5f07_11ea_9620_0242ac1c0002row4_col0\" class=\"data row4 col0\" >np</td>\n",
       "                        <td id=\"T_af59f072_5f07_11ea_9620_0242ac1c0002row4_col1\" class=\"data row4 col1\" >numpy</td>\n",
       "                        <td id=\"T_af59f072_5f07_11ea_9620_0242ac1c0002row4_col2\" class=\"data row4 col2\" >scientific computing with Python</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_af59f072_5f07_11ea_9620_0242ac1c0002row5_col0\" class=\"data row5 col0\" >pd</td>\n",
       "                        <td id=\"T_af59f072_5f07_11ea_9620_0242ac1c0002row5_col1\" class=\"data row5 col1\" >pandas</td>\n",
       "                        <td id=\"T_af59f072_5f07_11ea_9620_0242ac1c0002row5_col2\" class=\"data row5 col2\" >High performance data structures and tools</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_af59f072_5f07_11ea_9620_0242ac1c0002row6_col0\" class=\"data row6 col0\" >sns</td>\n",
       "                        <td id=\"T_af59f072_5f07_11ea_9620_0242ac1c0002row6_col1\" class=\"data row6 col1\" >seaborn</td>\n",
       "                        <td id=\"T_af59f072_5f07_11ea_9620_0242ac1c0002row6_col2\" class=\"data row6 col2\" >High-level data visualization library based on matplotlib</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fc59c924898>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Pandas .iplot() method activated.\n"
     ]
    }
   ],
   "source": [
    "!pip install pillow\n",
    "!pip install opencv-contrib-python\n",
    "!pip install -U fsds_100719\n",
    "\n",
    "\n",
    "from fsds_100719.imports import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:03:22.790837Z",
     "start_time": "2020-02-13T16:03:21.652166Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "colab_type": "code",
    "id": "2yML_wHNlbRn",
    "outputId": "7dd8f26e-d6c9-4f6d-b21b-ed9556400f86"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-65371b19f09a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    |from PIL import Image\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "|from PIL import Image\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AFAlTuXB8I4U"
   },
   "source": [
    "## Defining Folder Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:03:22.795516Z",
     "start_time": "2020-02-13T16:03:22.792300Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ykHe6fpjlbRq",
    "outputId": "b7c6f175-d184-457f-89ef-a7b7cac49e40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_set', 'training_set', 'single_prediction', '.DS_Store']"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## dataset\n",
    "base_folder = r'dogs-vs-cats-sorted/'#My Drive/Datasets/dogs-vs-cats-sorted/'\n",
    "os.listdir(base_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L5CNHxnagDjo"
   },
   "source": [
    "## Preparing Images Using .flow instead of flow_from_directory\n",
    "- https://discuss.analyticsvidhya.com/t/keras-image-preprocessing-using-flow-and-not-flow-from-directory/69460/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:03:22.799532Z",
     "start_time": "2020-02-13T16:03:22.796894Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "3vsPkzBelbRt"
   },
   "outputs": [],
   "source": [
    "# ## DOG VS CAT\n",
    "base_folder = r'dogs-vs-cats-sorted/'\n",
    "\n",
    "train_base_dir = base_folder+'training_set/'\n",
    "test_base_dir =base_folder+'test_set/' \n",
    "\n",
    "train_dogs = train_base_dir+'dogs/'\n",
    "train_cats = train_base_dir+'cats/'\n",
    "\n",
    "test_dogs = test_base_dir+'dogs/'\n",
    "test_cats = test_base_dir+'cats/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:03:22.834047Z",
     "start_time": "2020-02-13T16:03:22.804471Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "2gdysZbGlbRy"
   },
   "outputs": [],
   "source": [
    "import cv2,glob,os\n",
    "dog_train_files = glob.glob(train_dogs+'*.jpg')\n",
    "cat_train_files = glob.glob(train_cats+'*.jpg')\n",
    "all_train_files = [*dog_train_files,*cat_train_files]\n",
    "\n",
    "dog_test_files = glob.glob(test_dogs+'*.jpg')\n",
    "cat_test_files = glob.glob(test_cats+'*.jpg')\n",
    "all_test_files = [*dog_test_files,*cat_test_files]\n",
    "\n",
    "# print(len(img_filenames))\n",
    "# img_filenames[:10]\n",
    "\n",
    "all_filename_vars = [dog_train_files, cat_train_files,\n",
    "                        dog_test_files,cat_test_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:03:22.838772Z",
     "start_time": "2020-02-13T16:03:22.835323Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "FliRjzJilbR1"
   },
   "outputs": [],
   "source": [
    "def load_image_cv2(filename, RGB=True):\n",
    "    \"\"\"Loads image using cv2 and converts to either matplotlib-RGB (default)\n",
    "    or grayscale.\"\"\"\n",
    "    import cv2\n",
    "\n",
    "    IMG = cv2.imread(filename)\n",
    "    if RGB: cmap = cv2.COLOR_BGR2RGB\n",
    "    else: cmap=cv2.COLOR_BGR2GRAY\n",
    "    return cv2.cvtColor(IMG,cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hMrChDEtsGZx"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from keras.preprocessing import image\n",
    "from imageio import imread\n",
    "from skimage.transform import resize\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# defining a function to read images\n",
    "def read_img(img_path,target_size=(64, 64, 3)):\n",
    "    img = image.load_img(img_path, target_size=target_size)\n",
    "    img = image.img_to_array(img)\n",
    "\n",
    "    return img\n",
    "\n",
    "def load_train_test_val(dog_training_filenames, cat_training_filenames,\n",
    "                        dog_test_filenames,cat_test_filenames,\n",
    "                        img_size=(64,64,3),val_size=0.1):\n",
    "    \"\"\"Reads in training and test filenames to produce X and y data splits.\n",
    "    The validation set is intended to be used during training and is created\n",
    "    from the training images using train test split.\n",
    "\n",
    "    ylabels are encoded as 0=cat, 1=dog\n",
    "    Returns:  X_train, X_test, X_val, y_train, y_test,y_val\"\"\"\n",
    "    # \n",
    "    \n",
    "    display('[i] LOADING IMAGES')\n",
    " \n",
    "    train_img = []\n",
    "    train_label = []\n",
    "\n",
    "    for img_path in tqdm(dog_training_filenames):\n",
    "        train_img.append(read_img(img_path,target_size=img_size))\n",
    "        train_label.append(1)\n",
    "\n",
    "    for img_path in tqdm(cat_training_filenames):\n",
    "        train_img.append(read_img(img_path,target_size=img_size))\n",
    "        train_label.append(0)\n",
    "\n",
    "\n",
    "    test_img = []\n",
    "    test_label = []\n",
    "\n",
    "    for img_path in tqdm(dog_test_files):\n",
    "        test_img.append(read_img(img_path,target_size=img_size))\n",
    "        test_label.append(1)\n",
    "\n",
    "    for img_path in tqdm(dog_test_files):\n",
    "        test_img.append(read_img(img_path,target_size=img_size))\n",
    "        test_label.append(0)\n",
    "    # print('\\n',pd.Series(train_label).value_counts())\n",
    "\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X = np.array(train_img, np.float32)\n",
    "    y = np.array(train_label)\n",
    "\n",
    "    X_test = np.array(test_img, np.float32)\n",
    "    y_test = np.array(test_label)\n",
    "    X_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.1)\n",
    "    print('\\n[i] Length of Splits:')\n",
    "    print(f\"X_train={len(X_train)}, X_test={len(X_test)}, X_val={len(X_val)}\")\n",
    "    return X_train, X_test, X_val, y_train, y_test,y_val \n",
    "\n",
    "\n",
    "def train_test_val_datagens(X_train,X_test,X_val,y_train,y_test,y_val,\n",
    "                            BATCH_SIZE = 32, train_datagen_kws= dict(\n",
    "                                shear_range = 0.2, \n",
    "                                zoom_range = 0.2,\n",
    "                                horizontal_flip = True)):\n",
    "    \"\"\"Creates ImageDataGenerators for train,test,val data.\n",
    "    Returns: training_set,test_set,val_set\"\"\"\n",
    "    ## Create training and test data\n",
    "    from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "    train_datagen = ImageDataGenerator(rescale = 1./255,**train_datagen_kws)\n",
    "\n",
    "    test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "    val_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "    training_set = train_datagen.flow(X_train,y=y_train,batch_size=BATCH_SIZE)\n",
    "    test_set = test_datagen.flow(X_test,y=y_test,batch_size=BATCH_SIZE)\n",
    "    val_set = val_datagen.flow(X_val,y=y_val,batch_size=BATCH_SIZE)\n",
    "\n",
    "    return training_set,test_set,val_set\n",
    "\n",
    "\n",
    "\n",
    "def get_shapes_dict(training_set,verbose=True):\n",
    "    shapes = [\"Batchsize\", \"img_width\",\"img_height\",\"img_dim\"]\n",
    "    SHAPES = dict(zip(shapes, training_set[0][0].shape))\n",
    "    if verbose:\n",
    "        print('SHAPES DICT:')\n",
    "        print(SHAPES)\n",
    "        print(training_set[0][0].shape)\n",
    "        print('\\n[i] Labels for batch (0=cat,1=dog)')\n",
    "        print(training_set[0][1])\n",
    "    return SHAPES\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KZHkgQNd8hIw"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EmAH1aUT8g8X"
   },
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:28:11.019925Z",
     "start_time": "2020-02-13T16:28:10.679367Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "EJ32Dq1HlbSS",
    "outputId": "bd9e6afa-dd40-422c-ed9a-4921abadb97c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[i] LOADING IMAGES'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:09<00:00, 416.51it/s]\n",
      "100%|██████████| 4000/4000 [00:09<00:00, 426.76it/s]\n",
      "100%|██████████| 1000/1000 [00:02<00:00, 407.96it/s]\n",
      "100%|██████████| 1000/1000 [00:02<00:00, 418.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[i] Length of Splits:\n",
      "X_train=7200, X_test=2000, X_val=800\n",
      "SHAPES DICT:\n",
      "{'Batchsize': 128, 'img_width': 64, 'img_height': 64, 'img_dim': 3}\n",
      "(128, 64, 64, 3)\n",
      "\n",
      "[i] Labels for batch (0=cat,1=dog)\n",
      "[1 0 0 1 1 1 1 0 0 1 0 1 1 1 0 0 1 1 0 0 1 0 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1\n",
      " 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 1 1 0 0 0 1 1 1 1 0 1 1 1 1 0 1 0\n",
      " 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1\n",
      " 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "## USING FUNCTIONS TO LOAD IN IMAGES \n",
    "X_train,X_test,X_val,y_train,y_test,y_val = load_train_test_val(*all_filename_vars,\n",
    "                                                  val_size=0.1,img_size=(64,64,3))\n",
    "\n",
    "train_test_val_vars = [X_train,X_test,X_val,y_train,y_test,y_val ]\n",
    "\n",
    "training_set,test_set,val_set = train_test_val_datagens(*train_test_val_vars,\n",
    "                                                        BATCH_SIZE=128)\n",
    "    # X_train,X_test,X_val,y_train,y_test,y_val, BATCH_SIZE=128)    \n",
    "\n",
    "SHAPES = get_shapes_dict(training_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QLjQk_t--WUj",
    "outputId": "e0a0178a-303c-4149-f477-352cbbe32a38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_filename_vars) != 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "aQWzX5XWyGhZ",
    "outputId": "04a24b5c-2dd4-486f-fb60-c789a8a62210"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[i] LOADING IMAGES'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:09<00:00, 417.44it/s]\n",
      "100%|██████████| 4000/4000 [00:09<00:00, 419.20it/s]\n",
      "100%|██████████| 1000/1000 [00:02<00:00, 422.03it/s]\n",
      "100%|██████████| 1000/1000 [00:02<00:00, 419.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[i] Length of Splits:\n",
      "X_train=7200, X_test=2000, X_val=800\n",
      "SHAPES DICT:\n",
      "{'Batchsize': 64, 'img_width': 64, 'img_height': 64, 'img_dim': 3}\n",
      "(64, 64, 64, 3)\n",
      "\n",
      "[i] Labels for batch (0=cat,1=dog)\n",
      "[1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 1 0 1 1 0\n",
      " 0 0 1 1 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "def complete_image_processing(*train_test_filenames,img_size=(64,64,3),val_size=0.1,\n",
    "                              BATCH_SIZE = 32, train_datagen_kws= dict(\n",
    "                                shear_range = 0.2, \n",
    "                                zoom_range = 0.2,\n",
    "                                horizontal_flip = True),verbose=True):\n",
    "    \"\"\"Calls all 3 image prep functions and returns training,test,val datagens,\n",
    "    plus SHAPES dict.\"\"\"    \n",
    "    # img_params = list(locals())\n",
    "    # print(img_params[1:])\n",
    "    # if verbose:\n",
    "        # print('\\n[i] Creating training ImageDataGenerator using:')\n",
    "        # [print(f\"{img_params[i]}\") for i in range(len(img_params))];\n",
    "\n",
    "\n",
    "    # print(len(train_test_filenames))\n",
    "    if len(train_test_filenames) != 4:\n",
    "        raise Exception('Must provide 4 filenames dog_train_files, cat_train_files,dog_test_files,cat_test_files')\n",
    "    train_test_val_vars = load_train_test_val(\n",
    "        *train_test_filenames, val_size=0.1,img_size=img_size)\n",
    "\n",
    "    training_set,test_set,val_set = train_test_val_datagens(*train_test_val_vars,\n",
    "                                                            BATCH_SIZE=BATCH_SIZE)\n",
    "\n",
    "    SHAPES = get_shapes_dict(training_set)\n",
    "\n",
    "    # print(pd.Series())\n",
    "\n",
    "    return training_set,test_set,val_set,SHAPES\n",
    "\n",
    "\n",
    "\n",
    "training_set,test_set,val_set,SHAPES = \\\n",
    "complete_image_processing(*all_filename_vars, img_size=(64,64,3),BATCH_SIZE=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P1-7hb_lhVdU"
   },
   "source": [
    "# Using CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s3_BRyfOQ8fR"
   },
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:28:18.120169Z",
     "start_time": "2020-02-13T16:28:17.985574Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "5ZbaFM3xlbSa"
   },
   "outputs": [],
   "source": [
    "# # Part 1 - Building the CNN\n",
    "# clock = fs.jmi.Clock()\n",
    "# clock.tic('')\n",
    "# # Importing the Keras libraries and packages\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Conv2D\n",
    "# from keras.layers import MaxPooling2D\n",
    "# from keras.layers import Flatten\n",
    "# from keras.layers import Dense\n",
    "\n",
    "# # Initialising the CNN\n",
    "# classifier = Sequential()\n",
    "\n",
    "# # Step 1 - Convolution\n",
    "# classifier.add(Conv2D(SHAPES['Batchsize'], (3, 3),\n",
    "#                              input_shape = (SHAPES['img_width'],\n",
    "#                                             SHAPES['img_height'],\n",
    "#                                             SHAPES['img_dim']),\n",
    "#                              activation = 'relu'))\n",
    "\n",
    "# classifier.add(Conv2D(SHAPES['Batchsize'], (3, 3),\n",
    "#                       input_shape = (SHAPES['img_width'], \n",
    "#                                      SHAPES['img_height'],\n",
    "#                                      SHAPES['img_dim']),\n",
    "#                        activation = 'relu'))\n",
    "# # Step 2 - Pooling\n",
    "# classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# # Adding a second convolutional layer\n",
    "# classifier.add(Conv2D(SHAPES['Batchsize'], (3, 3),\n",
    "#                       activation = 'relu'))\n",
    "# classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# # Step 3 - Flattening\n",
    "# classifier.add(Flatten())\n",
    "\n",
    "# # Step 4 - Full connection\n",
    "# classifier.add(Dense(units = SHAPES['Batchsize'], activation = 'relu'))\n",
    "\n",
    "# classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "# # Compiling the CNN\n",
    "# classifier.compile(optimizer = 'adam', \n",
    "#                    loss = 'binary_crossentropy',\n",
    "#                    metrics = ['accuracy'])\n",
    "# print()\n",
    "# display(classifier.summary())\n",
    "# # Part 2 - Fitting the CNN to the images\n",
    "\n",
    "# history =classifier.fit_generator(training_set,\n",
    "#                          steps_per_epoch = 500,\n",
    "#                          epochs = 3,\n",
    "#                          validation_data = val_set,#test_set,\n",
    "#                          validation_steps =100,workers=-1)\n",
    "\n",
    "# clock.toc('')\n",
    "\n",
    "# y_hat_test = classifier.predict_classes(X_test).flatten()\n",
    "# # print(pd.Series(y_hat_test).value_counts(normalize=True))\n",
    "# evaluate_model(y_test,y_hat_test,history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-EDI9EcwGUKl"
   },
   "source": [
    "## Adding callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZdQvvc9kGwvD"
   },
   "outputs": [],
   "source": [
    "def cd_gdrive_mkdirs(model_subfolder='Datasets/Models/cat_vs_dog/'):\n",
    "    \"\"\"cd to /gdrive/My Drive/ to allow for saving files to google drive\n",
    "    Also makes all subfolders in 'model_subfolder'\"\"\"\n",
    "    \n",
    "    import os\n",
    "    ## To save to Gdrive, must first chdir to My Drive (so there's no spaces in fpath)\n",
    "    curdir = os.path.abspath(os.curdir)\n",
    "    gdrive_folder =r'/gdrive/My Drive/'\n",
    "\n",
    "    try:\n",
    "        os.chdir(gdrive_folder)\n",
    "    except Exception as e:\n",
    "        print(f'ERROR: {e}')\n",
    "\n",
    "    try:\n",
    "        os.makedirs(model_subfolder,exist_ok=True)\n",
    "        print('Directories created.')\n",
    "    except:\n",
    "        print('Error making directories')\n",
    "\n",
    "    return print(os.path.abspath(os.curdir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "id": "b1MAWrJWfr0F",
    "outputId": "32b1a5ab-4020-452d-fcfd-7d7e129808dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories created.\n",
      "/gdrive/My Drive\n",
      "\n",
      " Datasets/Models/cat_vs_dog/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['CNN_cat_dog_02142020.h5',\n",
       " 'CNN_cat_dog_02142020_model.h5',\n",
       " 'CNN_cat_dog_02142020_weights=.h5',\n",
       " 'CNN_cat_dog_02142020_model.json',\n",
       " 'CNN_cat_dog_02142020_weights.h5',\n",
       " 'weights-improvement-01-0.50.hdf5',\n",
       " 'weights-improvement-02-0.52.hdf5',\n",
       " 'weights-improvement-03-0.62.hdf5',\n",
       " 'weights-improvement-01-0.51.hdf5',\n",
       " 'weights-improvement-02-0.58.hdf5',\n",
       " 'weights-improvement-03-0.67.hdf5',\n",
       " 'weights-improvement-01-0.57.hdf5',\n",
       " 'weights-improvement-03-0.66.hdf5',\n",
       " 'weights-improvement-01-0.82.hdf5',\n",
       " 'weights-improvement-02-0.83.hdf5',\n",
       " 'weights-improvement-03-0.84.hdf5',\n",
       " 'weights-improvement-01-0.79.hdf5',\n",
       " 'weights-improvement-02-0.82.hdf5',\n",
       " 'weights-improvement-02-0.79.hdf5',\n",
       " 'weights-improvement-03-0.80.hdf5',\n",
       " 'weights-improvement-01-0.72.hdf5',\n",
       " 'callback_log.csv']"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_subfolder='Datasets/Models/cat_vs_dog/'\n",
    "cd_gdrive_mkdirs(model_subfolder=model_subfolder)\n",
    "\n",
    "print('\\n',model_subfolder)\n",
    "os.listdir(model_subfolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2r14qGFIVrfO"
   },
   "source": [
    "## Keras Callbacks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JtTKy31agUb6"
   },
   "source": [
    "- [Official Callback documentation](https://keras.io/callbacks/)\n",
    "- CallBacks You'll Definitely Want to Use\n",
    " - `keras.callbacks.ModelCheckpoint`\n",
    " - `keras.callbacks.EarlyStopping`\n",
    "\n",
    "- Callbacks worth further exploration\n",
    " - `keras.callbacks.callbacks.LearningRateScheduler`\n",
    " - `keras.callbacks`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6vnhHZeDgKth"
   },
   "source": [
    "def callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fr9qe6-VGWQU"
   },
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "\n",
    "def create_csvlogger(filename):\n",
    "    return CSVLogger(filename, separator=',', append=False)\n",
    "\n",
    "def create_checkpoint(monitor='val_acc',model_subfolder='Datasets/Models/cat_vs_dog/'):\n",
    "    filepath=model_subfolder+\"weights-improvement-{epoch:02d}-{\"+monitor+\":.2f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor=monitor, verbose=1, save_best_only=True, mode='max')\n",
    "    return checkpoint\n",
    "\n",
    "def create_early_stopping(monitor = 'val_acc',min_delta = 0, patience = 1,\n",
    "                          verbose = 1, restore_best_weights = True):\n",
    "\n",
    "    args = locals()\n",
    "    earlystop = EarlyStopping(**args)\n",
    "    return earlystop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "XoL5daY7WSnJ",
    "outputId": "19e78d66-17ce-4c2b-8f11-edf866605140"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories created.\n",
      "/gdrive/My Drive\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<keras.callbacks.ModelCheckpoint at 0x7ff94e98ea58>,\n",
       " <keras.callbacks.EarlyStopping at 0x7ff94e98eb70>,\n",
       " <keras.callbacks.CSVLogger at 0x7ff94e98eac8>]"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def get_callbacks():\n",
    "model_subfolder='Datasets/Models/cat_vs_dog/'\n",
    "cd_gdrive_mkdirs(model_subfolder=model_subfolder)\n",
    "callbacks_list = [create_checkpoint('val_acc'),\n",
    "                  create_early_stopping(),\n",
    "                  create_csvlogger(model_subfolder+'callback_log.csv')]\n",
    "callbacks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d6KhclYwHTMN"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "392eHJbeFPNl"
   },
   "source": [
    "## Model 2 + Callbacks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KpDmwIx4IiFG"
   },
   "outputs": [],
   "source": [
    "# # filepath=model_subfolder+\n",
    "# monitor = 'val_accuracy'\n",
    "# filepath=model_subfolder+\"weights-improvement-{epoch:02d}-{\"+monitor+\":.2f}.hdf5\"\n",
    "# filepath\n",
    "# model_subfolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "ENjCT-4wKqDX",
    "outputId": "004d2638-70a6-49ec-9eb4-14fbef7f46ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories created.\n",
      "/gdrive/My Drive\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<keras.callbacks.ModelCheckpoint at 0x7ff94ea72630>,\n",
       " <keras.callbacks.EarlyStopping at 0x7ff94ea72f28>,\n",
       " <keras.callbacks.CSVLogger at 0x7ff94ea72fd0>]"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_subfolder='Datasets/Models/cat_vs_dog/'\n",
    "cd_gdrive_mkdirs(model_subfolder=model_subfolder)\n",
    "callbacks_list = [create_checkpoint('val_acc'),\n",
    "                  create_early_stopping(),\n",
    "                  create_csvlogger(model_subfolder+'callback_log.csv')]\n",
    "callbacks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "uyphhqdq4lpv",
    "outputId": "3c5650c8-d794-4600-a9f6-13e4ef72991f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CLOCK STARTED @:    02/17/20 - 07:34:55 PM           Label:            --- \n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 60, 60, 64)        4864      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 58, 58, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 29, 29, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 27, 27, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 10816)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                692288    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 771,073\n",
      "Trainable params: 771,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "1000/1000 [==============================] - 91s 91ms/step - loss: 0.5885 - acc: 0.6688 - val_loss: 0.5128 - val_acc: 0.7426\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.74259, saving model to Datasets/Models/cat_vs_dog/weights-improvement-01-0.74.hdf5\n",
      "Epoch 2/3\n",
      "1000/1000 [==============================] - 79s 79ms/step - loss: 0.4343 - acc: 0.7950 - val_loss: 0.4575 - val_acc: 0.8037\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.74259 to 0.80366, saving model to Datasets/Models/cat_vs_dog/weights-improvement-02-0.80.hdf5\n",
      "Epoch 3/3\n",
      "1000/1000 [==============================] - 78s 78ms/step - loss: 0.3321 - acc: 0.8521 - val_loss: 0.5758 - val_acc: 0.7622\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.80366\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00003: early stopping\n",
      "--- TOTAL DURATION   =  4 min, 9.688 sec --- \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_283a2f6e_51bd_11ea_b456_0242ac1c0002 table, th {\n",
       "          text-align: center;\n",
       "    }    #T_283a2f6e_51bd_11ea_b456_0242ac1c0002row0_col1 {\n",
       "            width:  140px;\n",
       "        }    #T_283a2f6e_51bd_11ea_b456_0242ac1c0002row0_col2 {\n",
       "            width:  140px;\n",
       "        }</style><table id=\"T_283a2f6e_51bd_11ea_b456_0242ac1c0002\" ><caption>Summary Table of Clocked Processes</caption><thead>    <tr>        <th class=\"col_heading level0 col0\" >Lap #</th>        <th class=\"col_heading level0 col1\" >Start Time</th>        <th class=\"col_heading level0 col2\" >Duration</th>        <th class=\"col_heading level0 col3\" >Label</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                                <td id=\"T_283a2f6e_51bd_11ea_b456_0242ac1c0002row0_col0\" class=\"data row0 col0\" >TOTAL</td>\n",
       "                        <td id=\"T_283a2f6e_51bd_11ea_b456_0242ac1c0002row0_col1\" class=\"data row0 col1\" >02/17/20 - 07:34:55 PM</td>\n",
       "                        <td id=\"T_283a2f6e_51bd_11ea_b456_0242ac1c0002row0_col2\" class=\"data row0 col2\" >4 min, 9.688 sec</td>\n",
       "                        <td id=\"T_283a2f6e_51bd_11ea_b456_0242ac1c0002row0_col3\" class=\"data row0 col3\" ></td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ff94e81b860>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsgAAAEmCAYAAAB/DWYZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIiISJD/B5B4230n8DJaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "\tCLASSIFICATION REPORT:\n",
      "------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.10      0.17      1000\n",
      "           1       0.50      0.90      0.64      1000\n",
      "\n",
      "    accuracy                           0.50      2000\n",
      "   macro avg       0.50      0.50      0.41      2000\n",
      "weighted avg       0.50      0.50      0.41      2000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASgAAAEbCAYAAABp6RkaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Part 1 - Building the CNN\n",
    "clock = fs.jmi.Clock()\n",
    "clock.tic('')\n",
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Initialising the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Conv2D(SHAPES['Batchsize'], (5, 5),\n",
    "                             input_shape = (SHAPES['img_width'],\n",
    "                                            SHAPES['img_height'],\n",
    "                                            SHAPES['img_dim']),\n",
    "                             activation = 'relu'))\n",
    "\n",
    "classifier.add(Conv2D(SHAPES['Batchsize'], (3, 3),\n",
    "                      input_shape = (SHAPES['img_width'], \n",
    "                                     SHAPES['img_height'],\n",
    "                                     SHAPES['img_dim']),\n",
    "                       activation = 'relu'))\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding a second convolutional layer\n",
    "classifier.add(Conv2D(SHAPES['Batchsize'], (3, 3),\n",
    "                      activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(units = SHAPES['Batchsize'], activation = 'relu'))\n",
    "\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', \n",
    "                   loss = 'binary_crossentropy',\n",
    "                   metrics = ['accuracy'])\n",
    "print()\n",
    "display(classifier.summary())\n",
    "# Part 2 - Fitting the CNN to the images\n",
    "\n",
    "history =classifier.fit_generator(training_set,\n",
    "                         steps_per_epoch = 1000,\n",
    "                         epochs = 3,\n",
    "                         validation_data = val_set,#test_set,\n",
    "                         validation_steps = 500,workers=-1,\n",
    "                         callbacks=callbacks_list)\n",
    "\n",
    "clock.toc('')\n",
    "\n",
    "y_hat_test = classifier.predict_classes(X_test).flatten()\n",
    "# print(pd.Series(y_hat_test).value_counts(normalize=True))\n",
    "evaluate_model(y_test,y_hat_test,history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "grx0mylDLaTc",
    "outputId": "6a227f53-356c-4181-b115-c6acad2cc72d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'conv2d_1/kernel:0' shape=(5, 5, 3, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2d_1/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2d_2/kernel:0' shape=(3, 3, 64, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2d_2/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2d_3/kernel:0' shape=(3, 3, 64, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2d_3/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_1/kernel:0' shape=(10816, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_1/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_2/kernel:0' shape=(64, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_2/bias:0' shape=(1,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from keras.utils import \n",
    "model_loaded = classifier\n",
    "model_loaded.load_weights('Datasets/Models/cat_vs_dog/weights-improvement-01-0.72.hdf5')\n",
    "model_loaded.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VSK89u6iV1H5"
   },
   "source": [
    "### def save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cbkosi8QGibP"
   },
   "outputs": [],
   "source": [
    "def save_model(model,model_subfolder = 'Datasets/Models/cat_vs_dog/',\n",
    "               base_modelname = 'CNN_cat_dog_02142020', as_json=True,\n",
    "               return_fpaths=True,verbose=True):\n",
    "    import os\n",
    "    ## To save to Gdrive, must first chdir to My Drive (so there's no spaces in fpath)\n",
    "    curdir = os.path.abspath(os.curdir)\n",
    "\n",
    "    gdrive_folder =r'/gdrive/My Drive/'\n",
    "    model_subfolder = 'Datasets/Models/cat_vs_dog/'\n",
    "\n",
    "    try:\n",
    "        os.chdir(gdrive_folder)\n",
    "        os.makedirs(model_subfolder,exist_ok=True)\n",
    "    except Exception as e:\n",
    "        print(f'ERROR: {e}')\n",
    "\n",
    "    # os.listdir(model_subfolder)\n",
    "    # https://jovianlin.io/saving-loading-keras-models/\n",
    "    try:\n",
    "        weight_fpath = model_subfolder+base_modelname+'_weights.h5'\n",
    "        model.save_weights(weight_fpath, overwrite=True)\n",
    "\n",
    "        if as_json:\n",
    "            model_fpath = model_subfolder+base_modelname+'_model.json'\n",
    "            # Save the model architecture\n",
    "            with open(model_fpath, 'w') as f:\n",
    "                f.write(model.to_json())\n",
    "        else:\n",
    "            model_fpath = model_subfolder+base_modelname+'_model.h5'\n",
    "            model.save(model_fpath)\n",
    "        if verbose: \n",
    "            print(f\"[io] Model architecture saved as {model_fpath}\")\n",
    "            print(f\"[io] Model weights saved as {weight_fpath}\")\n",
    "        else:\n",
    "            print(f\"[io] Successfully saved model.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        import warnings\n",
    "        warnings.warn(f\"ERROR SAVING: {e}\")\n",
    "    if return_fpaths:\n",
    "        return model_fpath, weight_fpath\n",
    "\n",
    "\n",
    "\n",
    "def load_model(model_fpath,weight_fpath=None,as_json=True):\n",
    "    from keras.models import model_from_json\n",
    "    if (as_json == True) & (weight_fpath is None):\n",
    "        raise Exception('If using as_json=True, must provide ')\n",
    "\n",
    "    # Model reconstruction from JSON file\n",
    "    with open(model_fpath, 'r',encoding=\"utf8\") as f:\n",
    "        model2 = model_from_json(f.read())\n",
    "\n",
    "    # Load weights into the new model\n",
    "    model2.load_weights(weight_fpath)\n",
    "    display(model2.summary())\n",
    "    return model2\n",
    "\n",
    "# model_fpath,weight_fpath = save_model(model)\n",
    "# model_loaded = load_model(model_fpath,weight_fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vtZ-fSjUkHF0"
   },
   "source": [
    "# Making Modeling functions for gridsearching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UM7sFcOukGlq"
   },
   "outputs": [],
   "source": [
    "def build_model(SHAPES,filter_size=(3,3), pool_size=(2,2),dropout=True): \n",
    "    vars_ = locals()\n",
    "    print(f'[i] MODEL BUILT USING:\\n\\t{vars_}')\n",
    "    # Part 1 - Building the CNN\n",
    "\n",
    "    # Importing the Keras libraries and packages\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Conv2D\n",
    "    from keras.layers import MaxPooling2D\n",
    "    from keras.layers import Flatten\n",
    "    from keras.layers import Dense\n",
    "\n",
    "    # Initialising the CNN\n",
    "    classifier = Sequential()\n",
    "\n",
    "    # Step 1 - Convolution\n",
    "    classifier.add(Conv2D(SHAPES['Batchsize'], filter_size,\n",
    "                                input_shape = (SHAPES['img_width'], SHAPES['img_height'], SHAPES['img_dim']),\n",
    "                                activation = 'relu'))\n",
    "\n",
    "    classifier.add(Conv2D(SHAPES['Batchsize'], filter_size,\n",
    "                        input_shape = (SHAPES['img_width'], SHAPES['img_height'], SHAPES['img_dim']), activation = 'relu'))\n",
    "\n",
    "    # Step 2 - Pooling\n",
    "    classifier.add(MaxPooling2D(pool_size = pool_size))\n",
    "    if dropout:\n",
    "        classifier.add(Dropout(0.2))\n",
    "\n",
    "    # Adding a second convolutional layer\n",
    "    classifier.add(Conv2D(SHAPES['Batchsize'], filter_size, activation = 'relu'))\n",
    "    classifier.add(MaxPooling2D(pool_size = pool_size))\n",
    "\n",
    "    # Step 3 - Flattening\n",
    "    classifier.add(Flatten())\n",
    "\n",
    "    # Step 4 - Full connection\n",
    "    classifier.add(Dense(units = SHAPES['Batchsize'], activation = 'relu'))\n",
    "    classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "    # Compiling the CNN\n",
    "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy',\n",
    "                       metrics = ['accuracy'])\n",
    "    display(classifier.summary())\n",
    "    return classifier\n",
    "    # Part 2 - Fitting the CNN to the images\n",
    "\n",
    "\n",
    "\n",
    "def train_model(classifier,training_set, test_set, \n",
    "                params=dict(steps_per_epoch = 2000,\n",
    "                            epochs = 3, validation_steps = 500,\n",
    "                            workers=-1)):\n",
    "    vars = locals()\n",
    "    print(f'[i] Training model using\\n\\t{vars}\\n')\n",
    "    clock = Timer()\n",
    "    \n",
    "    history_ = classifier.fit_generator(training_set,\n",
    "                                        validation_data = test_set,\n",
    "                                        validation_steps=validation_steps,\n",
    "                                        **params)\n",
    "                        # workers=workers)\n",
    "\n",
    "    clock.stop()\n",
    "    return history_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w0QSQlduTX-I"
   },
   "outputs": [],
   "source": [
    "\n",
    "model_=build_model(SHAPES)\n",
    "history = train_model(model_,training_set,test_set)\n",
    "y_hat_val = model_.predict_classes(X_val).flatten()\n",
    "evaluate_model(y_val,y_hat_val,history=history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FtnmGCYraMWd"
   },
   "source": [
    "## Saving and Loading Models/Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w-w_gmXrrOFX"
   },
   "outputs": [],
   "source": [
    "## To save to Gdrive, must first chdir to My Drive (so there's no spaces in fpath)\n",
    "curdir = os.path.abspath(os.curdir)\n",
    "\n",
    "gdrive_folder =r'/gdrive/My Drive/'\n",
    "model_subfolder = 'Datasets/Models/cat_vs_dog/'\n",
    "\n",
    "try:\n",
    "    os.chdir(gdrive_folder)\n",
    "    os.makedirs(model_subfolder,exist_ok=True)\n",
    "except Exception as e:\n",
    "    print(f'ERROR: {e}')\n",
    "os.listdir(model_subfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AJmB5E5XVakv"
   },
   "outputs": [],
   "source": [
    "# def save_model(model,model_subfolder = 'Datasets/Models/cat_vs_dog/',\n",
    "#                base_modelname = 'CNN_cat_dog_02142020', as_json=True,\n",
    "#                return_fpaths=True,verbose=True):\n",
    "#     # https://jovianlin.io/saving-loading-keras-models/\n",
    "#     try:\n",
    "#         weight_fpath = model_subfolder+base_modelname+'_weights.h5'\n",
    "#         model.save_weights(weight_fpath, overwrite=True)\n",
    "\n",
    "#         if as_json:\n",
    "#             model_fpath = model_subfolder+base_modelname+'_model.json'\n",
    "#             # Save the model architecture\n",
    "#             with open(model_fpath, 'w') as f:\n",
    "#                 f.write(model.to_json())\n",
    "#         else:\n",
    "#             model_fpath = model_subfolder+base_modelname+'_model.h5'\n",
    "#             model.save(model_fpath)\n",
    "#         if verbose: \n",
    "#             print(f\"[io] Model architecture saved as {model_fpath}\")\n",
    "#             print(f\"[io] Model weights saved as {weight_fpath}\")\n",
    "#         else:\n",
    "#             print(f\"[io] Successfully saved model.\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         import warnings\n",
    "#         warnings.warn(f\"ERROR SAVING: {e}\")\n",
    "#     if return_fpaths:\n",
    "#         return model_fpath, weight_fpath\n",
    "\n",
    "# model_fpath,weight_fpath = save_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ehR3dQfiYX13"
   },
   "outputs": [],
   "source": [
    "# def load_model(model_fpath,weight_fpath=None,as_json=True):\n",
    "#     from keras.models import model_from_json\n",
    "#     if (as_json == True) & (weight_fpath is None):\n",
    "#         raise Exception('If using as_json=True, must provide ')\n",
    "\n",
    "#     # Model reconstruction from JSON file\n",
    "#     with open(model_fpath, 'r',encoding=\"utf8\") as f:\n",
    "#         model2 = model_from_json(f.read())\n",
    "\n",
    "#     # Load weights into the new model\n",
    "#     model2.load_weights(weight_fpath)\n",
    "#     display(model2.summary())\n",
    "#     return model2\n",
    "# model_loaded = load_model(model_fpath,weight_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_koIz3CKZ57v"
   },
   "outputs": [],
   "source": [
    "y_hat_val = model_loaded.predict_classes(X_val)\n",
    "evaluate_model(y_val,y_hat_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dszu4vFoS2Mp"
   },
   "source": [
    "## Transfer Learning\n",
    "\n",
    "https://www.kaggle.com/risingdeveloper/transfer-learning-in-keras-on-dogs-vs-cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:28:41.130469Z",
     "start_time": "2020-02-13T16:28:19.339734Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "B9xAQK1SlbSe"
   },
   "outputs": [],
   "source": [
    "# from keras.applications import InceptionResNetV2\n",
    "\n",
    "# conv_base = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(150,150,3))\n",
    "# conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:28:41.303392Z",
     "start_time": "2020-02-13T16:28:41.132620Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "kKag2f0OlbSh"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:28:48.792947Z",
     "start_time": "2020-02-13T16:28:41.304826Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "TKkdHFLxlbSj"
   },
   "outputs": [],
   "source": [
    "# from keras import layers\n",
    "# from keras import models\n",
    "# model = models.Sequential()\n",
    "# model.add(conv_base)\n",
    "# model.add(layers.Flatten())\n",
    "# model.add(layers.Dense(32, activation='relu'))#256\n",
    "# model.add(layers.Dense(1, activation='sigmoid'))  #Sigmoid function at the end because we have just two classes\n",
    "# # model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:28:48.798283Z",
     "start_time": "2020-02-13T16:28:48.794242Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "ave6CZillbSm"
   },
   "outputs": [],
   "source": [
    "# print('Number of trainable weights before freezing the conv base:', len(model.trainable_weights))\n",
    "# conv_base.trainable = False\n",
    "# print('Number of trainable weights after freezing the conv base:', len(model.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TIyRhBr60YAa"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:28:48.852453Z",
     "start_time": "2020-02-13T16:28:48.799636Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "5a_puKjUlbSo"
   },
   "outputs": [],
   "source": [
    "# from keras import optimizers\n",
    "\n",
    "# model.compile(loss='binary_crossentropy', optimizer=optimizers.RMSprop(lr=2e-5), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:28:48.856533Z",
     "start_time": "2020-02-13T16:28:48.853517Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "piJDSJIilbSq"
   },
   "outputs": [],
   "source": [
    "# len(training_set)*32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6lBRMvb47liA"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T16:42:17.733053Z",
     "start_time": "2020-02-13T16:32:15.737807Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "r6DHGjLplbSt"
   },
   "outputs": [],
   "source": [
    "# history = model.fit_generator(training_set,\n",
    "#                               steps_per_epoch = 2000,\n",
    "#                               epochs = 2, validation_data = test_set,\n",
    "#                               validation_steps = 500,workers=-1)\n",
    "# y_hat_val = model.predict_classes(X_val)\n",
    "# evaluate_model(y_val,y_hat_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BWhbeB2k3Who"
   },
   "outputs": [],
   "source": [
    "# pd.Series(y_hat_val).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LL9Tz-nq2GZD"
   },
   "outputs": [],
   "source": [
    "# save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q9n-JX7ZoHSS"
   },
   "source": [
    "## Lime \n",
    "\n",
    "- https://github.com/expectopatronum/code-snippets-blog/blob/master/python/201808_catdog_classifier_lime/analyse-cat-dog-classifier.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d9TQLN4voJMX"
   },
   "outputs": [],
   "source": [
    "# # !pip install lime\n",
    "# import lime\n",
    "# from lime import lime_image\n",
    "# from lime import lime_base\n",
    "# from lime.wrappers.scikit_image import SegmentationAlgorithm\n",
    "# from skimage.segmentation import mark_boundaries\n",
    "\n",
    "\n",
    "# explainer = lime_image.LimeImageExplainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "36ca0zaYoWsw"
   },
   "outputs": [],
   "source": [
    "# def explain_single_sample(dataset, idx):\n",
    "#     img_data = dataset[idx][0]\n",
    "#     data = img_data.reshape(IMG_SIZE,IMG_SIZE,3)\n",
    "#     model_out = model.predict([data])[0]\n",
    "#     label = 0\n",
    "#     label_name = \"cat\"\n",
    "#     if model_out[1] > 0.5:\n",
    "#         label = 1\n",
    "#         label_name = \"dog\"\n",
    "#     explanation = explainer.explain_instance(data, model.predict, top_labels=2, hide_color=None, num_samples=1000)\n",
    "#     temp, mask = explanation.get_image_and_mask(label, positive_only=True, num_features=5, hide_rest=True)\n",
    "#     fig, ax = plt.subplots(1,3)\n",
    "#     ax[0].imshow(img_data)\n",
    "#     #plt.subplot(1, 2, 1)\n",
    "#     ax[1].imshow(mark_boundaries(temp, mask))\n",
    "#     #plt.show()\n",
    "#     #plt.subplot(1, 2, 2)\n",
    "#     temp, mask = explanation.get_image_and_mask(label, positive_only=False, num_features=20, hide_rest=False)\n",
    "#     ax[2].imshow(mark_boundaries(temp, mask))\n",
    "#     plt.show()\n",
    "#     print(\"label: \", dataset[idx][1], \"prediction:\", label_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YQMdYErdlbSz"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z8ZNM1XglbS1"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jKbFWJq9lbS4"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TCADvLJ3lbS6"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "srbjybjqQ5m9"
   },
   "source": [
    "### Using os, shutil to create directories and copy files\n",
    "- from [Convolutional Neural Networks - Codealong](https://github.com/jirvingphd/dsc-04-43-03-convolutional-neural-networks-code-along-online-ds-ft-021119)\n",
    "\n",
    "- **first define the folders that currently contain the images get their filenames**\n",
    "\n",
    "```python\n",
    "import os, shutil\n",
    "\n",
    "# Define directories to be created:\n",
    "data_santa_dir = 'data/santa/'\n",
    "data_not_santa_dir = 'data/not_santa/'\n",
    "new_dir = 'split/'\n",
    "\n",
    "# Store the list of all the relevant training target images\n",
    "imgs_santa = [file for file in os.listdir(data_santa_dir) if file.endswith('.jpg')]\n",
    "print('There are',len(imgs_santa), 'santa images')\n",
    "\n",
    "# Store the list of all non-target images\n",
    "imgs_not_santa = [file for file in os.listdir(data_not_santa_dir) if file.endswith('.jpg')]\n",
    "print('There are', len(imgs_not_santa), 'images without santa')\n",
    "\n",
    "```\n",
    "\n",
    "- **Now create new directries and for training, testing, and validation images.**\n",
    "\n",
    "```python\n",
    "# Create the main folder for all of the new sub-folders\n",
    "os.mkdir(new_dir)\n",
    "\n",
    "# Create valid pathnames inside of the new_dir for training images\n",
    "train_folder = os.path.join(new_dir, 'train')\n",
    "train_santa = os.path.join(train_folder, 'santa')\n",
    "train_not_santa = os.path.join(train_folder, 'not_santa')\n",
    "\n",
    "# Create valid pathnames inside of the new_dir for testing images\n",
    "test_folder = os.path.join(new_dir, 'test')\n",
    "test_santa = os.path.join(test_folder, 'santa')\n",
    "test_not_santa = os.path.join(test_folder, 'not_santa')\n",
    "\n",
    "# Create valid pathnames inside of the new_dir for validation images\n",
    "val_folder = os.path.join(new_dir, 'validation')\n",
    "val_santa = os.path.join(val_folder, 'santa')\n",
    "val_not_santa = os.path.join(val_folder, 'not_santa')\n",
    "\n",
    "\n",
    "# Now create all of the folders defined above\n",
    "os.mkdir(test_folder)\n",
    "os.mkdir(test_santa)\n",
    "os.mkdir(test_not_santa)\n",
    "\n",
    "os.mkdir(train_folder)\n",
    "os.mkdir(train_santa)\n",
    "os.mkdir(train_not_santa)\n",
    "\n",
    "os.mkdir(val_folder)\n",
    "os.mkdir(val_santa)\n",
    "os.mkdir(val_not_santa)\n",
    "\n",
    "```\n",
    "\n",
    "- **Now that we have the folders, copy the desired # of images to the correct dataset folders**\n",
    "\n",
    "```python\n",
    "# The user decided to put 271 images in the training set, 100 in the validation set, and 90 in the test set\n",
    "# train santa\n",
    "imgs = imgs_santa[:271]\n",
    "for img in imgs:\n",
    "    origin = os.path.join(data_santa_dir, img)\n",
    "    destination = os.path.join(train_santa, img)\n",
    "    shutil.copyfile(origin, destination)\n",
    "    \n",
    "# validation santa\n",
    "imgs = imgs_santa[271:371]\n",
    "for img in imgs:\n",
    "    origin = os.path.join(data_santa_dir, img)\n",
    "    destination = os.path.join(val_santa, img)\n",
    "    shutil.copyfile(origin, destination)\n",
    "    \n",
    "# test santa\n",
    "imgs = imgs_santa[371:]\n",
    "for img in imgs:\n",
    "    origin = os.path.join(data_santa_dir, img)\n",
    "    destination = os.path.join(test_santa, img)\n",
    "    shutil.copyfile(origin, destination)\n",
    "    \n",
    "## REPEATED FOR FOR THE NON-SANTA IMAGES - NOT SHOWN   \n",
    "```\n",
    "\n",
    "- Now that we have images in separate directories, we can use the Kera's ImageDataGenerators .flow_from_directory() method.\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "# get all the data in the directory split/test (180 images), and reshape them\n",
    "test_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        test_folder, \n",
    "        target_size=(64, 64), batch_size = 180) \n",
    "# ...do the same for train and val (not shown)\n",
    "\n",
    "\n",
    "# create the data sets\n",
    "train_images, train_labels = next(train_generator)\n",
    "\n",
    "# Make sure things worked\n",
    "print (\"Number of training samples: \" + str(m_train))\n",
    "print (\"train_images shape: \" + str(train_images.shape))\n",
    "\n",
    "\n",
    "# Reshape the training images to have one column(/row?) for each image\n",
    "train_img = train_images.reshape(train_images.shape[0], -1)\n",
    "print(train_img.shape)\n",
    "\n",
    "\n",
    "# Reshape the labels to match the data\n",
    "train_y = np.reshape(train_labels[:,0], (542,1))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jUgyK0HRQBxT"
   },
   "source": [
    "## Building CNN From Scratch Lab\n",
    "- https://github.com/learn-co-students/dsc-04-43-04-building-a-cnn-from-scratch-online-ds-ft-021119/tree/solution\n",
    "- CNN's are great for image processing\n",
    "### Image Data\n",
    "\n",
    "```python\n",
    "import os #for listdir()\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "train_dir = 'chest_xray_downsampled/train'\n",
    "validation_dir = 'chest_xray_downsampled/val/'\n",
    "test_dir = 'chest_xray_downsampled/test/'\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Train_generator example\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        train_dir,\n",
    "        # All images will be resized to 150x150\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "```\n",
    "- **Images are store in ImageDataGenerators**\n",
    "    - Generally rescale to... intensity values? of 1./255\n",
    "    - load in files with ImageDataGenerator.flow_from_directory( :\n",
    "        - directory\n",
    "        - the target_size (the size to convert all images to)\n",
    "        - batch_size\n",
    "        - class_mode \n",
    "            - Note: selection of loss function determines chocie.\n",
    "                - If using 'binary_crossentropy' for binary classification, use class_mode='binary'\n",
    "                \n",
    "- ImageDataGenerators are also used for augmenting data. \n",
    "\n",
    "```python\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "```\n",
    "\n",
    "### Setting Up Initial Network\n",
    "```python\n",
    "from keras import models, layers, optimizers\n",
    "# or if want to do exact layers:\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense)\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# Initialize sequential model\n",
    "model = Sequential()\n",
    "```\n",
    "**1A) A CNN should start with a Conv2D later**\n",
    "\n",
    "```python\n",
    "layers.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
    "```\n",
    "\n",
    "- Conv2D layers parameters (to change):\n",
    "    - filters:  # of samples to take from each image (kind of like # of neurons?) (e.g. filters=32)\n",
    "    - kernel_size: size (in pixels) of the filters (e.g kernel_size=(3,3)\n",
    "    - activation: activation function to use (e.g. 'relu')\n",
    "        \n",
    "**1B) MaxPooling2D layers following Conv2D layers**\n",
    "```python\n",
    "layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)\n",
    "```\n",
    "\n",
    "-  MaxPooling2D parameters:\n",
    "    -  pool_size: factor by which to downscale. \n",
    "        - e.g.pool_size=(2,2) will half information in vertical and horizational direction\n",
    "     \n",
    "**1C,optional) Add a Dropout layer to avoid overfitting:** [Udemy course suggestion]\n",
    "```python\n",
    "layers.Dropout(rate, noise_shape=None, seed=None)\n",
    "```\n",
    "- Dropout parameters:\n",
    "    - rate = 0.25 (used by udemy course)\n",
    "\n",
    "**2) Repeat: Continue layering combinations of Conv2D / MaxPooling2D layers** (Dropout too?):\n",
    "- Later layers will need larger # of filters to detect more abstract patterns.\n",
    "\n",
    "```python\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "```\n",
    "\n",
    "**3A) Flatten Data Before Passing on to Dense Layers for Classification/Learning**\n",
    "```python\n",
    "layers.Flatten(data_format =None)\n",
    "```\n",
    "**3B)  Add Dense layers at the end of the convolutional base for learning:**\n",
    "```python\n",
    "layers.Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None))\n",
    "```\n",
    "- Will only need to worry about basic parameters:\n",
    "    - Units:\n",
    "        - Larger #, used for the actual learning.\n",
    "    - Activation\n",
    "        - User choice, 'relu' is always good.\n",
    "        \n",
    "**3C) Add final Dense layer to determine output classification**\n",
    "- Add  a final small Dense layer (depending on number of classes?)\n",
    "    - For binary classification:\n",
    "        - units: 1\n",
    "        - activation: 'sigmoid'\n",
    "        \n",
    "``` python\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "```\n",
    "\n",
    "**4) Compile the model, selecting loss function, optimizer, and metric**\n",
    "\n",
    "- Loss Function:\n",
    "    - For binary classifications, use 'binary_crossentropy'\n",
    "- Optimizer:\n",
    "    - Use RMSProp,\n",
    "        -Specify learning rate: ```lr = 1e-4```\n",
    "- Metrics:\n",
    "    - Use 'acc' for accuracy\n",
    "```python\n",
    "keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "# Compile Model\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer=optimizers.RMSProp(lr=1e-4),\n",
    "              metrics=['acc'])\n",
    "\n",
    "#Set the model to train; \n",
    "import datetime\n",
    "start=datetime.datetime.now()\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=30,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50)\n",
    "\n",
    "end=datetime.datetime.now()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8rz5t0lMYCfd"
   },
   "source": [
    "# Use Pretrained CNNs\n",
    "\n",
    "## Pretrained Networks Overview\n",
    "- Pretrained networks have already been trained on large pools of data, and have their weights frozen.\n",
    "    - They enable deep learning on fairly small image datasets\n",
    "    - a 'small' dataset is less than tens of thousands or hundreds of thousands of images\n",
    "\n",
    "- Pretrained networks can be used in whole or only specific parts, depending on your need/data.\n",
    "    - The shallower the layer of neurons, the more generic its features are. \n",
    "        - Therefore even if you data is very different, you can still use the lower layers for basic feature extraction.\n",
    "    - The deeper the layer, the more abstract its features are.\n",
    "        - so you may want to unfreeze the deeper/higher order classificaiton layers and re-train the network on your images. \n",
    "<br><br>\n",
    "### Where to find the pre-trained networks\n",
    "- **Pretrained Networks are available in [Keras.applications](https://keras.io/applications/)**\n",
    "    - This list of pretained models are for image classification. \n",
    "        - DenseNet\n",
    "        - InceptionResNetV2\n",
    "        - InceptionV3\n",
    "        - MobileNet\n",
    "        - NASNet\n",
    "        - ResNet50\n",
    "        - VGG16\n",
    "        - *VGG19* - used in labs\n",
    "        - Xception\n",
    "\n",
    "    - You can import these networks and use it as a function with 2 arguments:\n",
    "        1. `weights`\n",
    "            - Determines which data source's training data weights to use.\n",
    "            - ex:  `weights='imagenet'\n",
    "        2. `include_top`\n",
    "            - determines whetehr or not to include the fully-connected layer at the top of the network\n",
    "```python\n",
    "from keras.applications import MobileNet\n",
    "conv_base = MobileNet(weights='imagenet', include_top=True)\n",
    "```\n",
    "\n",
    "### How to use pretrained networks for feature extraction or for fine-tuning\n",
    "\n",
    "**You'll learn about two ways to use pre-trained networks:**\n",
    "- **Feature extraction**: here, you use the representations learned by a previous network to extract interesting features from new samples. \n",
    "    - Method 1) Use the convolutional base layers and run your data to detect the basic features, and save the output data, which is then run a new dense classifier, which is trained from scratch.  \n",
    "        - (+) It is fast\n",
    "        - (-) but cant use data augmentation. \n",
    "        - Note:  If your images are very different from the pretraining datasets, you may want to only use _part_ of the convolutional base but a _new_ densely connected classifier\n",
    "    - Method 2) Extend the conv_base by adding dense layers on top, running everything together. \n",
    "        - (+) allows for data sugmentation\n",
    "        - (-) extremely time-consuming and requires GPU\n",
    "  \n",
    "- **Fine-tuning**: when finetuning, you'll \"unfreeze\" a few top layers from the convolutional base of the model and train them again together with the densely connected classifier layers of the model. \n",
    "    - Note that you are changing the parts of the convolutional layers here that were used to detect the more abstract features.\n",
    "    - By doing this, you can make your model more relevant for the classification problem at hand.\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "* http://cs231n.stanford.edu/syllabus.html\n",
    "* https://www.dlology.com/blog/gentle-guide-on-how-yolo-object-localization-works-with-keras/\n",
    "* https://www.dlology.com/blog/gentle-guide-on-how-yolo-object-localization-works-with-keras-part-2/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VHwZgQ9mQAu3"
   },
   "outputs": [],
   "source": [
    "from keras.applications import MobileNet\n",
    "conv_base = MobileNet(weights='imagenet',\n",
    "                  include_top = True)\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OStc-MxyrNMY"
   },
   "source": [
    "# Using  Pretrained Networks - Codealong\n",
    "## Theory/Tips\n",
    "\n",
    "## Code \n",
    "\n",
    "### Feature Extraction Method 1:\n",
    "\n",
    "```python\n",
    "from keras.applications import VGG19\n",
    "cnn_base = VGG19(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(64, 64, 3))\n",
    "cnn_base.summary()\n",
    "\n",
    "# ---\n",
    "\n",
    "def extract_features(directory, sample_amount):\n",
    "    features = np.zeros(shape=(sample_amount, 2, 2, 512)) \n",
    "    labels = np.zeros(shape=(sample_amount))\n",
    "    generator = datagen.flow_from_directory(\n",
    "        directory, target_size=(64, 64), \n",
    "        batch_size = 10, \n",
    "        class_mode='binary')\n",
    "    i=0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = cnn_base.predict(inputs_batch)\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch \n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        i = i + 1\n",
    "        if i * batch_size >= sample_amount:\n",
    "            break\n",
    "    return features, labels\n",
    "\n",
    "# ---\n",
    "\n",
    "# you should be able to divide sample_amount by batch_size!!\n",
    "train_features, train_labels = extract_features(train_folder, 540) \n",
    "validation_features, validation_labels = extract_features(val_folder, 200) \n",
    "test_features, test_labels = extract_features(test_folder, 180)\n",
    "\n",
    "train_features = np.reshape(train_features, (540, 2 * 2 * 512))\n",
    "validation_features = np.reshape(validation_features, (200, 2 * 2 * 512))\n",
    "test_features = np.reshape(test_features, (180, 2 * 2 * 512))\n",
    "\n",
    "# ---\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_dim=2 * 2 * 512))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(train_features, train_labels,\n",
    "                    epochs=20,\n",
    "                    batch_size=10,\n",
    "                    validation_data=(validation_features, validation_labels))\n",
    "\n",
    "results_test = model.evaluate(test_features, test_labels)\n",
    "\n",
    "# ---\n",
    "\n",
    "train_acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epch = range(1, len(train_acc) + 1)\n",
    "plt.plot(epch, train_acc, 'g.', label='Training Accuracy')\n",
    "plt.plot(epch, val_acc, 'g', label='Validation acc')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epch, train_loss, 'r.', label='Training loss')\n",
    "plt.plot(epch, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#---\n",
    "\n",
    "```\n",
    "\n",
    "## Feature Extraction Method 2\n",
    " - this method is much more costly, but allows us to use data augmentation\n",
    " \n",
    "- The process:\n",
    "    1. Add the pretrained model as the first layer\n",
    "    2. Add some dense layers as a classifier on top\n",
    "    3. Freeze the convolutional base\n",
    "        - This will prevent the weights from changing. \n",
    "        - The layer.trainable attribute indicates if a layer is frozen\n",
    "    4. Train the model. \n",
    "    \n",
    "    \n",
    "```python\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(cnn_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(132, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# ---\n",
    "\n",
    "#You can check whether a layer is trainable (or alter its setting) through the layer.trainable attribute:\n",
    "for layer in model.layers:\n",
    "    print(layer.name, layer.trainable)\n",
    "    \n",
    "#Similarly, we can check how many trainable weights are in the model:\n",
    "print(len(model.trainable_weights))\n",
    "\n",
    "# ---\n",
    "\n",
    "# Freeze the conv base\n",
    "cnn_base.trainable = False\n",
    "\n",
    "\n",
    "# ---\n",
    "\n",
    "# get all the data in the directory split/train (542 images), and reshape them\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_folder, \n",
    "        target_size=(64, 64), \n",
    "        batch_size= 20,\n",
    "        class_mode= 'binary') \n",
    "\n",
    "# get all the data in the directory split/validation (200 images), and reshape them\n",
    "val_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        val_folder, \n",
    "        target_size=(64, 64), \n",
    "        batch_size = 20,\n",
    "        class_mode= 'binary')\n",
    "\n",
    "# get all the data in the directory split/test (180 images), and reshape them\n",
    "test_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        test_folder, \n",
    "        target_size=(64, 64), \n",
    "        batch_size = 180,\n",
    "        class_mode= 'binary')\n",
    "\n",
    "test_images, test_labels = next(test_generator)\n",
    "\n",
    "# ---\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              metrics=['acc'])\n",
    "\n",
    "# ---\n",
    "\n",
    "history = model.fit_generator(\n",
    "              train_generator,\n",
    "              steps_per_epoch= 27,\n",
    "              epochs = 10,\n",
    "              validation_data = val_generator,\n",
    "              validation_steps = 10)\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d0sjUefMuIs0"
   },
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ey5E6in_uGHO"
   },
   "source": [
    "Up till now, we have frozen the entire convolutional base. Again, it cannot be stressed enough how important this is before fine tuning the weights of the later layers of this base. Without training a classifier on the frozen base first, there will be too much noise in the model and initial epochs will overwrite any useful representations encoded in the pretrained model. That said, now that we have tuned a classifier to the frozen base, we can now unfreeze a few of the deeper layers from this base and further fine tune them to our problem scenario. In practice, this is apt to be particularly helpful where adapted models span new domain categories. For example, if the pretrained model is on cats and dogs and this is adapted to a problem specific to cats (a very relatively similar domain) there is apt to be little performance gain from fine tuning. On the other hand, if the problem domain is more substantially different, additional gains are more likely in adjusting these more abstract layers of the convolutional base. With that, let's take a look at how to unfreeze and fine tune these later layers.\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "cnn_base.trainable = True\n",
    "\n",
    "# ---\n",
    "\n",
    "cnn_base.trainable = True\n",
    "set_trainable = False\n",
    "for layer in cnn_base.layers:\n",
    "    if layer.name == 'block5_conv1':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "        \n",
    "        \n",
    "# ---\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                      optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "# ---\n",
    "history = model.fit_generator(\n",
    "              train_generator,\n",
    "              steps_per_epoch= 27,\n",
    "              epochs = 10,\n",
    "              validation_data = val_generator,\n",
    "              validation_steps = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mtyhyKPOrgY5"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dkMVqngpLsKC"
   },
   "source": [
    "# NLP Content from Flation Data Science Bootcamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lReWoVYsTbwE"
   },
   "source": [
    "## Word Embeddings Lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oBEb-dY-Lweq"
   },
   "source": [
    "- [Solution on Github](https://github.com/jirvingphd/dsc-04-45-04-generating-word-embeddings-lab-online-ds-ft-021119/tree/solution)\n",
    "\n",
    "- Use `nltk.word_tokenize` to tokenize new headlines data\n",
    "    - Can use `dataframe['column'].map(word_tokenize)` to tokenize a specific column in a df.\n",
    "    - After tokenization, leave in original order. \n",
    "    \n",
    "- Use `gensim.models.Word2Vec`\n",
    "    - [gensim website](https://radimrehurek.com/gensim/)\n",
    "    \n",
    "    \n",
    "- Instantiate a Word2Vec model:<br> `model = Word2Vec(data, size=100, window=5, min_count=1, workers=4)`\n",
    "    - `data` = text\n",
    "    - `size` =size of the embedding vectors to create\n",
    "    - `window` = # of words to include in sliding window\n",
    "    - `min_count` = number of times a word must appear to be counted\n",
    "    - `wokers` = number of threads to use during training\n",
    "    \n",
    "- `model.train(data, total_examples = model.corpus_count, epochs=10)`\n",
    "\n",
    "- Now can use the model.wv dictionary for methods\n",
    "    - `wv = model.wv`\n",
    "    \n",
    "    \n",
    "- **Get Word Similarity**\n",
    "    - Return the Most Similar Words \n",
    "        - `wv.most_similar('Texas')`\n",
    "         \n",
    "    - Return the Least Similar Words (not so meaningful)\n",
    "        - `wv.most_similar(negative='Texas')`\n",
    "\n",
    "- **To Get a Word's Vector**\n",
    "    - Use wv as a dictionary\n",
    "      - `wv['Texas']`\n",
    "      \n",
    "- **To Get All Word Vectors**\n",
    "    - `wv.vectors`\n",
    "        \n",
    "- **To Perform Word *Arithmetic***\n",
    "    - i.e. 'king' - 'man' + 'woman'\n",
    "    - Words to add should be `positive=`, words to subtract are `negative=`\n",
    "    - `wv.most_similar(positive=['king','woman'], negative=['man'])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hs8MNYwNL7ka"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KIiiqS1hMC_x"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/')\n",
    "\n",
    "file = '/content/gdrive/My Drive/Colab Notebooks/datasets/News_Category_Dataset_v2.json'\n",
    "df = pd.read_json(file, lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "poAr1lwHOiZK"
   },
   "outputs": [],
   "source": [
    "# Concatenate description and headline.\n",
    "df['combined_text'] = df.headline+' '+df.short_description\n",
    "\n",
    "# Tokenize the combined_text column.\n",
    "data = df['combined_text'].map(word_tokenize)\n",
    "\n",
    "# Preview first 5\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XaQMDoG0Ox2y"
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(data, size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "model.train(data, total_examples=model.corpus_count, epochs=10)\n",
    "\n",
    "\n",
    "wv = model.wv\n",
    "wv.most_similar('Texas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O1pgxlGeTK7G"
   },
   "outputs": [],
   "source": [
    "wv.most_similar(negative='Texas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dOCWkeEmTRai"
   },
   "outputs": [],
   "source": [
    "wv.most_similar(positive=['king','woman'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y7QKTYb9OiOK"
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5t5C_rbCTdd5"
   },
   "source": [
    "## Classification with Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QRaVtOWsTjVS"
   },
   "source": [
    "### Using Pretrained Word Vectors with GloVe\n",
    "- Best to load a top-tier industry-standard word mdels\n",
    "    - Most common is Global Vectors for Word Representation (GloVe) from the Stanford NLP Group.\n",
    "    - Loading in weights removes the need to instantiate a Word2Vec model. \n",
    "- **Instead, the process is:**\n",
    "    1. Get the total vocabulary of our data\n",
    "    2. Download and unzip the GloVe file needed from Standford NLP\n",
    "    3. Read the GloVe file, save only vectors for words in our dataset.\n",
    "    \n",
    "\n",
    "    \n",
    "- **File must be downloaded manually:**\n",
    "    - Must download the zip file for the Stanford Groups pretrained vectors [here.](https://nlp.stanford.edu/projects/glove/)(Lab used the smallest one (which is still 6 B words)\n",
    "    - Place the downloaded file directly into same folder as jupyter notebook.\n",
    "- **To use in python;**    \n",
    "    - First, must tokenize the words in the vocab as done above with nltk.\n",
    "    - Second, must turn the vocabulary into a `set`:\n",
    "        `total_vocabulary = set(word for headline in data for word in headline)`\n",
    "    - Third, load in the glove file, check for and keep only the words that are inside of total_vocabulary\n",
    "```python\n",
    "glove = {}\n",
    "with open('glove.6B.50d.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode('utf-8')\n",
    "        if word in total_vocabulary:\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove[word] = vector\n",
    "```\n",
    "- The code above has created a dictionary called `glove, which contains all of the vectors for our data's vocabulary.\n",
    "\n",
    "### Mean Word Embeddings\n",
    "\n",
    "- Just loading in vectors does not describe sentence, only individual words.\n",
    "- To classify text, we need to calculate ***Mean Word Embeddings***. \n",
    "    - Simply get the vector for every word in a sentence and take the average.\n",
    "    - Mean Word vectors will always match the size of each individual word, no matter how many words appear in a sentence. \n",
    "    - Can easily put text into a form for Supervised Learning models, such as Support Vector Machines or Gradient Boosted Trees. \n",
    "    \n",
    "    \n",
    "### Coding a Custom Vectorizer Class (compatible with sklearn)\n",
    "\n",
    "### Deep Learning & Embedding Layers\n",
    "\n",
    "- Mean word embeddings lose some of the meaning, which is why **Sequence Models** exist. \n",
    "    - Recurrent Neural Networks \n",
    "    - Long Short Term Memory Cells\n",
    "- For deep learning, add **Embedding Layers** into the network.\n",
    "\n",
    "- **Embedding Layer Requirements**\n",
    "    - Learn the word embeddings for our data 'on the fly', get the benefits of Word2Vec without needing to train a Word2Vec model separately.\n",
    "    - Embedding Layers must always be the FIRST layer, immediately below the Input() layer.\n",
    "    - All words in the text must be integer-encoded (each word its own unique integer)\n",
    "    - Size of the embedding layer MUST be greater than the total vocabulary size\n",
    "        - First parameter denotes vocab size, the second parameter denotes word vector size. \n",
    "    - The size of sequences passed must be set when creating the layer. \n",
    "    \n",
    "###  Embedding Layers in Keras\n",
    "- [Classification with Word Embeddings - Lab ](https://github.com/learn-co-students/dsc-04-45-06-classification-with-word-embeddings-lab-online-ds-ft-021119)\n",
    "- [Keras Documentation](https://keras.io/layers/embeddings/)\n",
    "\n",
    "```python\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, LSTM, Embedding\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "y = pd.get_dummies(target).values\n",
    "```\n",
    "- **Preprocessing data for use with embedding layer:**\n",
    "    - Tokenize each example\n",
    "    - Convert to sequences\n",
    "    - Pad the sequences so same length\n",
    "\n",
    "```python\n",
    "tokenizer = text.Tokenizer(num_words=20000) # limiting to first 20000 words in vocab\n",
    "tokenizer.fit_on_texts(list(df.combined_text))\n",
    "list_tokenized_headlines = tokenizer.texts_to_sequences(df.combined_text)\n",
    "X_t = sequence.pad_sequences(list_tokenized_headlines, maxlen=100)\n",
    "\n",
    "```\n",
    "\n",
    "- **Setting up the network architecture:**\n",
    "    - Input layer first\n",
    "    - Embedding layer second\n",
    "        - pass size of vocab, embedding_size\n",
    "        - embedding_size = 128\n",
    "    - LSTM layer third\n",
    "        - Followed by a GlobalMaxPooling1D layer\n",
    "        - Followed by  Dropout layer\n",
    "    - Dense layer for classification (activation='relu')\n",
    "        - Followed by another Dropout layer\n",
    "    - Final Dense layer\n",
    "        - Number of neurons = # of possible classes.\n",
    "        - activation = 'softmax'\n",
    "\n",
    "```python\n",
    "\n",
    "embedding_size = 128\n",
    "input_ = Input(shape=(100,))\n",
    "x = Embedding(20000, embedding_size)(input_)\n",
    "x = LSTM(25, return_sequences=True)(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(50, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "# There are 41 different possible classes, so we use 41 neurons in our output layer\n",
    "x = Dense(41, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_, outputs=x)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_t, y, epochs=2, batch_size=32, validation_split=0.1)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### Lab's W2vVectorizer Class (edited for bs_ds compatibility)\n",
    "- Original did not have import numpy statements and did not accept glove during \\_\\_init__ (but still expected it to be present)\n",
    "```python\n",
    "class W2vVectorizer(object):\n",
    "    \"\"\"From Learn.co Text Classification with Word Embeddings Lab.\n",
    "    An sklearn-comaptible class containing the vectors for the fit Word2Vec.\"\"\"\n",
    "\n",
    "    def __init__(self, w2v, glove):\n",
    "        # takes in a dictionary of words and vectors as input\n",
    "        import numpy as np\n",
    "\n",
    "        self.w2v = w2v\n",
    "        if len(w2v) == 0:\n",
    "            self.dimensions = 0\n",
    "        else:\n",
    "            self.dimensions = len(w2v[next(iter(glove))])\n",
    "\n",
    "    # Note from Mike: Even though it doesn't do anything, it's required that this object implement a fit method or else\n",
    "    # It can't be used in a sklearn Pipeline.\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        import numpy as np\n",
    "        return np.array([\n",
    "            np.mean([self.w2v[w] for w in words if w in self.w2v]\n",
    "                   or [np.zeros(self.dimensions)], axis=0) for words in X])\n",
    "```\n",
    "\n",
    "#### With W2vVectorizer, can use in sklearn Pipelines:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf =  Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(glove)),\n",
    "              (\"Random Forest\", RandomForestClassifier(n_estimators=100, verbose=True))])\n",
    "svc = Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(glove)),\n",
    "                ('Support Vector Machine', SVC())])\n",
    "lr = Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(glove)),\n",
    "              ('Logistic Regression', LogisticRegression())])\n",
    "\n",
    "# ---\n",
    "models = [('Random Forest', rf),\n",
    "          (\"Support Vector Machine\", svc),\n",
    "          (\"Logistic Regression\", lr)]\n",
    "# ---\n",
    "scores = [(name, cross_val_score(model, data, target, cv=2).mean()) for name, model, in models]\n",
    "scores\n",
    "# ---\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Sect_47_Intro_to_CNNs- Keras Callbacks.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "325px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
