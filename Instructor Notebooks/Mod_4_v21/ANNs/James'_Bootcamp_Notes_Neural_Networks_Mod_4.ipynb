{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ByfUTZodQsND"
   },
   "source": [
    "# Neural Networks - Intro to Deep Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlvqq-TbP5zO"
   },
   "source": [
    "    \n",
    "- **The purpose of a neural network is to model $\\hat y \\approx y$ by minimizing loss/cost functions using gradient descent.**\n",
    "\n",
    "- Neural networks are very good with unstructured data. (images, audio)\n",
    "\n",
    "- **Networks are comprised of sequential layers of neurons/nodes.**\n",
    "    - Each neuron applies a **linear transformation** and an **activation function** and outputs result to all neurons in the next layer.\n",
    "    - Minimizing Loss functions by adjusting parameters (weights and bias) of each connection using gradient descent (forward and back propagation).\n",
    "\n",
    "- **Activation functions** control the output of a neuron.($\\hat y =f_{activation}(x)$ )\n",
    "    - Most basic activation function is sigmoid functin ($\\hat y =\\sigma(x)$)\n",
    "    - Choice of activation function controls the size/range of the output.\n",
    "- **Linear transformations** ( $z = w^T x + b$ ) are used control the output of the activation function .\n",
    "    - where $w^T $ is the weight(/coefficient), $x$ is the input, and  $b$ is a bias. \n",
    "        - weights: \n",
    "        - bias:\n",
    "        \n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-40-02-introduction-to-neural-networks-online-ds-ft-021119/master/figures/log_reg.png\">\n",
    "\n",
    "- **Loss functions** ($\\mathcal{L}(\\hat y, y) $)  measure inconsistency between predicted ($\\hat y$) and actual $y$\n",
    "    - will be optimized using gradient descent\n",
    "    - defined over 1 traning sample\n",
    "- **Cost functions** takes the average loss over all of the samples.\n",
    "    - $J(w,b) = \\displaystyle\\frac{1}{l}\\displaystyle\\sum^l_{i=1}\\mathcal{L}(\\hat y^{(i)}, y^{(i)})$\n",
    "    - where $l$ is the number of samples\n",
    "\n",
    "- **Forward propagation** is the calculating  loss and cost functions.\n",
    "- **Back propagation** involves using gradient descent to update the values for  $w$ and $b$.\n",
    "    - $w := w- \\alpha\\displaystyle \\frac{dJ(w)}{dw}$ <br><br>\n",
    "    - $b := b- \\alpha\\displaystyle \\frac{dJ(b)}{db}$\n",
    "\n",
    "        - where $ \\displaystyle \\frac{dJ(w)}{dw}$ and $\\displaystyle \\frac{dJ(b)}{db}$ represent the *slope* of the function $J$ with respect to $w$ and $b$ respectively\n",
    "        - $\\alpha$ denote the *learning rate*. \n",
    "\n",
    "### Using the chain rule for updating parameters with sigmoid activation function example:\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-40-02-introduction-to-neural-networks-online-ds-ft-021119/master/figures/log_reg_deriv.png\" >\n",
    "- $\\displaystyle \\frac{dJ(w,b)}{dw_i} = \\displaystyle\\frac{1}{l}\\displaystyle\\sum^l_{i=1} \\frac{d\\mathcal{L}(\\hat y^{(i)}, y^{(i)})}{dw_i}$\n",
    " \n",
    " \n",
    "- For each training sample $1,...,l$ you'll need to compute:\n",
    "\n",
    "    - $ z^{(i)} = w^T x^ {(i)} +b $\n",
    "\n",
    "    - $\\hat y^{(i)} = \\sigma (z^{(i)})$\n",
    "\n",
    "    - $dz^{(i)} = \\hat y^{(i)}- y^{(i)}$\n",
    "\n",
    "- Then, you'll need to make update:\n",
    "\n",
    "    - $J_{+1} = - [y^{(i)} \\log (\\hat y^{(i)}) + (1-y^{(i)}) \\log(1-\\hat y^{(i)})$ (for the sigmoid function)\n",
    "\n",
    "    - $dw_{1, +1}^{(i)} = x_1^{(i)} * dz^{(i)}$\n",
    "\n",
    "    - $dw_{2, +1}^{(i)} = x_2^{(i)} * dz^{(i)}$\n",
    "\n",
    "    - $db_{+1}^{(i)} =  dz^{(i)}$\n",
    "\n",
    "    - $\\dfrac{J}{m}$, $\\dfrac{dw_1}{m}$, $\\dfrac{dw_1}{m}$, $\\dfrac{db}{m}$\n",
    "\n",
    "- After that, update: \n",
    "\n",
    "    $w_1 := w_1 - \\alpha dw_1$\n",
    "\n",
    "    $w_2 := w_2 - \\alpha dw_2$\n",
    "\n",
    "    $b := b - \\alpha db$\n",
    "\n",
    "    repeat until convergence!\n",
    "    \n",
    "    \n",
    "\n",
    "## Activation Functions (will call $f_a$ here)\n",
    "- **sigmoid:**<br><img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-40-04-deeper-neural-networks-online-ds-ft-021119/master/index_files/index_33_1.png\" width=200>\n",
    "    - $ f_a=\\dfrac{1}{1+ \\exp(-z)}$\n",
    "    - outputs 0 to +1\n",
    "    \n",
    "- **tanh (hyperbolic tan):**<br><img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-40-04-deeper-neural-networks-online-ds-ft-021119/master/index_files/index_36_1.png\" width=200> <br>(Note:title is incorrect)\n",
    "    - $f_a = =\\dfrac{\\exp(z)- \\exp(-z)}{\\exp(z)+ \\exp(-z)}$\n",
    "    - outputs -1 to +1\n",
    "    - Generally works well in intermediate layers\n",
    "    - one of most popular functions\n",
    "    \n",
    "- **arctan**<br><img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-40-04-deeper-neural-networks-online-ds-ft-021119/master/index_files/index_40_1.png\" width=200>\n",
    "    -  similar qualities as tanh, but slope is more gentle than tanh\n",
    "    - outputs ~ 1.6 to 1.6\n",
    "    \n",
    "-  **Rectified Linear Unit (relu):**<br><img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-40-04-deeper-neural-networks-online-ds-ft-021119/master/index_files/index_43_1.png\" width=200>\n",
    "    - most popular activation function\n",
    "    - Activation is exactly 0 when Z <0\n",
    "    - Makes taking directives slightly cumbersome\n",
    "    - $f_a=\\max(0,z)$\n",
    "- **leaky_relu:**<br><img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-40-04-deeper-neural-networks-online-ds-ft-021119/master/index_files/index_46_1.png\" width=200>\n",
    "    -  altered version of relu where the activatiom is slightly negative when $z<0$\n",
    "    - $f_a=\\max(0.001*z,z)$\n",
    "\n",
    "### Additional Resources\n",
    " - Good summary article with helpful visuals:\n",
    "    - https://towardsdatascience.com/multi-layer-neural-networks-with-sigmoid-function-deep-learning-for-rookies-2-bf464f09eb7f \n",
    "- Interactive Visualizer for Activation Functions\n",
    "    - https://dashee87.github.io/deep%20learning/visualising-activation-functions-in-neural-networks/\n",
    "    \n",
    " ### Section Summary:\n",
    " - Compared to more tradistional statistics and ML techniques, neural networks perform particulary well when using unstructured data\n",
    " - Apart from densely connected networks, otyher types of neural networks include convolutional neural networks, recurrent neural networks, and generative adversarial neural networks.\n",
    " - When working with image data, it's important to understand how image data is stored when working with them in Python\n",
    "- Single Layer Neural Network with a sigmoid activation function very similar to logistic regression.\n",
    "- Backward and forward propagation are used to estimate the so-called \"model weights\"\n",
    "- Adding more layers to neural networks can substantially increase model performance\n",
    "- Several activations can be used in model nodes, you can explore with different types and evaluate how it affects performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IA6ZLbuoQv8D"
   },
   "source": [
    "# Deeper Neural Networks:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hHZ0q-QYomgR"
   },
   "source": [
    "       \n",
    "- **Advantages:**\n",
    "    - largely eliminates need for feature engineering\n",
    "    - multiple levels of information processing in one networking.\n",
    "        - Ex: for images:\n",
    "            - First layer detects edges\n",
    "            - second layer gorups edges and detects patterns\n",
    "            - more layers group even bigger parts together\n",
    "        - Ex: for audio:\n",
    "            - first layer: low level wave features\n",
    "            - second: basic units of sounds (\"phonemes\")\n",
    "            - third: word recognition\n",
    "            - fourth: sentence recognition\n",
    "    \n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-40-04-deeper-neural-networks-online-ds-ft-021119/master/figures/small_deeper.png\">\n",
    "\n",
    "- Networks are comprised of sequential layers of neurons/nodes.\n",
    "    - \\# of layers = hidden+output layer\n",
    "        - The input layer is not counted as formal layer.\n",
    "    - All layers except the final are _hidden layers_.\n",
    "\n",
    "\n",
    "## NOTATION:\n",
    "Generally, the output of layer $j$ is denoted as $a^{[j]}$.\n",
    "\n",
    "**For our 2-layer neural network above, this means that:**\n",
    "\n",
    "- $x = a^{[0]}$  as x is what comes out of the input layer\n",
    "- $a^{[1]} = \\begin{bmatrix} a^{[1]}_1  \\\\ a^{[1]}_2 \\\\ a^{[1]}_3  \\\\\\end{bmatrix}$ is the value generated by the hidden layer\n",
    "- $\\hat y =  a^{[2]}$, the output layer will generate a value $a^{[2]}$, which is equal to $\\hat y$.\n",
    "\n",
    "\n",
    "<br>For the **first node** in the hidden layer:\n",
    "- The linear transformation that occurs is:  $ z^{[1]}_1 = w^{[1]}_1 x +b^{[1]}_1$,\n",
    "    - Where $w$ = the weight, and $b$ = bias\n",
    "\n",
    "- For **all nodes** in the hidden layer:\n",
    "    - $ z^{[1]}_1 = w^{[1]}_1 x +b^{[1]}_1$ and  $a^{[1]}_1= f(z^{[1]}_1)$\n",
    "\n",
    "    - $ z^{[1]}_2 = w^{[1]}_2 x +b^{[1]}_2$ and $a^{[1]}_2= f(z^{[1]}_2)$\n",
    "\n",
    "    - $ z^{[1]}_3 = w^{[1]}_3 x +b^{[1]}_3$ and $a^{[1]}_3= f(z^{[1]}_3)$\n",
    "\n",
    "The **dimensions** of the elements:\n",
    "\n",
    "- $w^{[1]} = \\begin{bmatrix} w^{[1]}_{1,1}  & w^{[1]}_{2,1} & w^{[1]}_{3,1}  \\\\ w^{[1]}_{1,2}  & w^{[1]}_{2,2} & w^{[1]}_{3,2}\\end{bmatrix}$\n",
    "    - where, eg. $w^{[1]}_{1,2}$ denotes the weight of the arrow going **from $x_2$ into the first node** of the hidden layer. \n",
    "\n",
    "\n",
    "- When multiplying the transpose of this matrix (making it a 2 x 3 matrix) \n",
    "    - $w^{[1]T}_1$ with $x = \\begin{bmatrix} x_1  \\\\x_2\\end{bmatrix}$ and add $b^{[1]} = \\begin{bmatrix} b^{[1]}_1  \\\\b^{[1]}_2 \\\\ b^{[1]}_3 \\end{bmatrix}$,\n",
    "    - we obtain $z^{[1]} = \\begin{bmatrix} z^{[1]}_1  \\\\z^{[1]}_2 \\\\ z^{[1]}_3 \\end{bmatrix}$.\n",
    "\n",
    "----\n",
    "\n",
    "- The activation function is   $a^{[1]}_1= f(z^{[1]}_1)$.\n",
    "$w^{[1]}_{1,2}$\n",
    "\n",
    "$w^{[1]} = \\begin{bmatrix} w^{[1]}_{1,1}  & w^{[1]}_{2,1} & w^{[1]}_{3,1}  \\\\ w^{[1]}_{1,2}  & w^{[1]}_{2,2} & w^{[1]}_{3,2}\\end{bmatrix}$ \n",
    "\n",
    "[Reminder: $x = \\begin{bmatrix} x_1  \\\\x_2\\end{bmatrix} \\equiv a^{[0]}$ and that $a^{[2]} = \\hat y$ ]\n",
    "\n",
    "- Then, given input $x$:\n",
    "\n",
    "    - $z^{[1]} = w^{[1]T} a^{[0]} + b^{[1]}$\n",
    "\n",
    "    - $a^{[1]} = f(z^{[1]})$\n",
    "\n",
    "    - $z^{[2]} = w^{[2]T} a^{[1]} + b^{[2]}$\n",
    "\n",
    "    - $a^{[2]} = f(z^{[1]})$\n",
    "    \n",
    "    \n",
    "- When adding in several training samples ($i$), these become:\n",
    "    - $z^{[1](i)} = w^{[1]T} a^{[0](i)} + b^{[0]}$\n",
    "\n",
    "    - $a^{[1](i)} = f(z^{[1](i)})$\n",
    "\n",
    "    - $z^{[2](i)} = w^{[2]T} a^{[1](i)} + b^{[2]}$\n",
    "\n",
    "    - $a^{[2](i)} = f(z^{[1](i)})$\n",
    "    \n",
    "    \n",
    "### Process Summary\n",
    "- We begin by defining a model architecture which includes the number of hidden layers, activation functions (sigmoid or relu) and the number of units in each of these.  \n",
    "- We then initialize parameters for each of these layers (typically randomly). After the initial parameters are set, forward propagation evaluates the model giving a prediction, which is then used to evaluate a cost function. Forward propogation involves evaluating each layer and then piping this output into the next layer. \n",
    "- Each layer consists of a linear transformation and an activation function.  The parameters for the linear transformation in **each** layer include $W^l$ and $b^l$. The output of this linear transformation is represented by $Z^l$. This is then fed through the activation function (again, for each layer) giving us an output $A^l$ which is the input for the next layer of the model.  \n",
    "- After forward propogation is completed and the cost function is evaluated, backpropogation is used to calculate gradients of the initial parameters with respect to this cost function. Finally, these gradients are then used in an optimization algorithm, such as gradient descent, to make small adjustments to the parameters and the entire process of forward propogation, back propogation and parameter adjustments is repeated until the modeller is satisfied with the results.\n",
    "\n",
    "\n",
    "### Parameter Summary (Deep Networks Lesson):\n",
    "Notation for when there are $L$ layers present (and $l$ is current layer)\n",
    "\n",
    "**Parameters for the linear transformation: **  \n",
    "\n",
    "$W^{[l]}: (n^{[l]}, n^{[l-1]})$\n",
    "\n",
    "$b^{[l]}: (n^{[l]}, 1)$\n",
    "\n",
    "$dW^{[l]}: (n^{[l]}, n^{[l-1]})$\n",
    "\n",
    "$db^{[l]}: (n^{[l]}, 1)$\n",
    "\n",
    "**Parameters for the activation function**  \n",
    "\n",
    "$ a^{[l]}, z^{[l]}: (n^{[l]}, 1)$\n",
    "\n",
    "$ Z^{[l]}, A^{[l]}: (n^{[l]}, m)$\n",
    "\n",
    "$ dZ^{[l]}, dA^{[l]}: (n^{[l]}, m)$\n",
    "\n",
    "\n",
    "\n",
    "### Forward propagation:\n",
    "- Process: \n",
    "    - evaluating a cost function associated with the output of the neural network by successively calculating the output of each layer given initial parameter values, and passing this output on to the next layer until a finalized output has been calculated and the cost function can then be evaluated..\n",
    "        - Input is $a^{[l-1]}$\n",
    "        - Output $a^{[l]}$, save $z^{[l]}, w^{[l]}, b^{[l]}, a^{[l-1]} $\n",
    "    \n",
    "- $Z^1$ is the output of the linear transformation of the initial input $A^1$ (the observations).\n",
    "- In successive layers, $A^l$ is the output from the previous hidden layer. \n",
    "- In all of these cases, $W^l$ is a matrix of weights to be optimized minimize the cost function. $b^l$ is also optimized but is a vector as opposed to a matrix.  \n",
    "\n",
    "- $g^l$ is the activation function which takes the output of this linear transformation and yields the input to the next hidden layer.  \n",
    "- $ Z^{[l]}= W^{[l]} A^{[l-1]} + b^{[l]}$\n",
    "\n",
    "- $ A^{[l]}= g^{[l]} ( Z^{[l]})$\n",
    "\n",
    "- Shape: here, $ Z^{[l]}, A^{[l]}$ both have a shape of $(n^{[l]}, m)$\n",
    "\n",
    "    - where $n$ the nodes in the layer $l$\n",
    "    \n",
    "    \n",
    "### Backward Propagation:\n",
    "- Once an output for the neural network given the current parameter weights has been calculated, we must back propogate to calculate the gradients of layer parameters with respect to the cost function.\n",
    "    - This will allow us to apply an optimization algorithm such as gradient descent in order to make small adjustments to the parameters in order to minimize our cost (and improve our predictions).\n",
    "    - Input $da ^{[l]}$\n",
    "    - Output $da^{[l-1]}$, $dW^{[l]}, db^{[l]}$\n",
    "\n",
    "\n",
    "- **The gradients for our respective parameters are given by:**\n",
    "   \n",
    "    - $ dZ^{[l]}= dA ^{[l]} * g^{[l]'} (Z^{[l]})$\n",
    "\n",
    "    - $ dW^{[l]} = \\dfrac{1}{m} dZ^{[l]}* A^{[l-1]T}$\n",
    "\n",
    "    - $ db^{[l]} = \\dfrac{1}{m} np.sum(dZ^{[l]}, axis=1, keepdims=True)$\n",
    "\n",
    "    - $ dA^{[l-1]} = W^{[l]T}*dZ^{[l]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JkJpdZlv3VNe"
   },
   "source": [
    "# Intro to Keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LYecj1xvmvEU"
   },
   "source": [
    "## Keras Basics\n",
    "- Tensors dimensions:\n",
    "    - Scalars = 0D tensors\n",
    "    - Vectors = 1D tensors\n",
    "    - Matrices = 2D tensors\n",
    "    - 3D tensors\n",
    "- A tensor is defined by 3 characteristics:\n",
    "    - rank or number of axes\n",
    "    - the shape\n",
    "    - the data type\n",
    "- Tensor basics - properties (from [here](https://pgaleone.eu/tensorflow/2018/07/28/understanding-tensorflow-tensors-shape-static-dynamic/#tensors-the-basic)):\n",
    "    - name\n",
    "    - type:\n",
    "        - tf.float32, tf.int64, tf.string\n",
    "    - rank:\n",
    "        - the number of dimension or the tensor. \n",
    "        - scalar = 0, vector = 1, etc.\n",
    "    - shape:\n",
    "\n",
    "## Important Data Manipulations in numpy\n",
    "\n",
    "- **Unrowing matrices:**\n",
    "    - e.g. turning a matrix of 790 images, which are 64 x 64 pixels and in RBG (3 colors) a (790, 64, 64, 3) matrix  into a matrix with 1 row for each image a ( 64*64*3, 790) matrix\n",
    "    - img_unrow = img.reshape(790, -1).T\n",
    "        - reshape -1 essentially means \"figure out how many, based upon the dat'\n",
    "- **Increasing the rank:**\n",
    "    - Vector with `np.shape()` returns  `(790,)`\n",
    "    - `np.reshape(vector, (1, 790))`\n",
    "- **Tensor indexling/slicing**\n",
    "    - just as python, `tensor[start_idx : end_idx]`\n",
    "    - left inclusive, right exclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "NbbVDhN0rDhV",
    "outputId": "4b5f722d-e72f-42bd-905b-b965aee215f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Tensor shape: (60000, 28, 28)\n",
      "Tensor Slice [0:100] shape: (100, 28, 28)\n",
      "Tensor Slice [0:100] shape: (100, 28, 28)\n",
      "Tensor Slice [0:100] shape: (100, 28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tensor indexing example using images\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "digit = train_images[10] #Select an arbitrary case for our example\n",
    "\n",
    "#Checking the shape of our tensor (in this case, the image)\n",
    "print('Raw Tensor shape:', train_images.shape)\n",
    "\n",
    "#Now performing some slices of our image:\n",
    "print('Tensor Slice [0:100] shape:', train_images[:100].shape)\n",
    "\n",
    "#Equivalently\n",
    "print('Tensor Slice [0:100] shape:', train_images[:100, :, :].shape)\n",
    "\n",
    "#Or verbosely:\n",
    "print('Tensor Slice [0:100] shape:', train_images[:100, :28, :28].shape)\n",
    "\n",
    "\n",
    "plt.imshow(digit, cmap=plt.cm.binary) #Display an example image for context\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "id": "6f_oIbrIrpn2",
    "outputId": "984d6db8-36d0-442f-bc7e-d98875c159fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sliced tensor shape (includes all images but only the lower right hand corner of each: (60000, 14, 14)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# In the above example, we sliced our tensor to obtain 100 of the 60,000 images. \n",
    "# You can also slice tensors along other axes. For example, \n",
    "# the 1st dimension is which image we are referring two,\n",
    "# while the 2nd and 3rd axis are the pixels of these images themselves.\n",
    "# For example, we could limit the images to the bottom right hand quadrant like this:\n",
    "lower_right_quadrant = train_images[:,14:,14:]\n",
    "print('Sliced tensor shape (includes all images but only the lower right hand corner of each:',\n",
    "      lower_right_quadrant.shape)\n",
    "plt.imshow(lower_right_quadrant[10], cmap=plt.cm.binary) #Display the 10th image from our sliced tensor.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ByGu747-rxb_"
   },
   "source": [
    "## Tensor Operations\n",
    "- **Element-wise**\n",
    "    - each element from one tensor added/etc to the corresponding elements\n",
    "- **Broadcasting**\n",
    "    - used for tensors of different dimensions\n",
    "    - in example below, when adding a (3,) vector to a (4,3) matrix, a copy of the vector is added to each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "colab_type": "code",
    "id": "BalyDaI7rwso",
    "outputId": "36b199b5-0aa7-4a80-d2e4-87bc7f065e33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A - (shape(4, 3)):\n",
      " [[ 0  1  2]\n",
      " [ 3  4  5]\n",
      " [ 6  7  8]\n",
      " [ 9 10 11]]\n",
      "\n",
      "B -  (shape(3,)):\n",
      " [1 2 3]\n",
      "\n",
      "Updated A:\n",
      " [[ 1  3  5]\n",
      " [ 4  6  8]\n",
      " [ 7  9 11]\n",
      " [10 12 14]]\n"
     ]
    }
   ],
   "source": [
    "# Example Broadcasting\n",
    "import numpy as np\n",
    "A=np.array([[ 0,  1,  2],\n",
    " [ 3,  4,  5],\n",
    " [ 6,  7,  8],\n",
    " [ 9, 10, 11]] )\n",
    "print(f'A - (shape{np.shape(A)}):\\n',A)\n",
    "B = np.array([1, 2, 3 ])\n",
    "print(f'\\nB -  (shape{np.shape(B)}):\\n',B)\n",
    "# Broadcasting B to add to A\n",
    "A += B\n",
    "print(f'\\nUpdated A:\\n',A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NuWdUhfzulNy"
   },
   "source": [
    "- **Tensor dot:**\n",
    "    - taking the dot product as in linear algebra\n",
    "    - a sum of element-wise products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "XrxIRtr9umVK",
    "outputId": "7e211537-24c5-4b32-8633-413ba8d5e638"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-  (3,):\n",
      " [1 2 3]\n",
      "\n",
      "B.B - ():\n",
      " 14\n"
     ]
    }
   ],
   "source": [
    "# Simple dot product \n",
    "B = np.array([1,2,3])\n",
    "print(f'B-  {np.shape(B)}:\\n',B)\n",
    "\n",
    "#Taking the dot product of B and itself is equivalent to\n",
    "#1*1 + 2*2 + 3*3 = 1 + 4 + 9 = 14\n",
    "BdotB = np.dot(B,B)\n",
    "print(f'\\nB.B - {np.shape(BdotB)}:\\n',BdotB)\n",
    "# np."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177
    },
    "colab_type": "code",
    "id": "_VKfO6INvbkY",
    "outputId": "9622b289-7eed-453c-e2ae-4d26760cf88a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      " [[ 0  1  2]\n",
      " [ 3  4  5]\n",
      " [ 6  7  8]\n",
      " [ 9 10 11]] \n",
      "\n",
      "B: [1 2 3] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 8, 26, 44, 62])"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# More complicated example\n",
    "# Here the first element is the sum of the first row of A multiplied by B elementwise:  \n",
    "## 0*1 + 1*2 + 2*3 = 0 + 2 + 6 = 8  \n",
    "# Followed by the sum of the second row of A multiplied by B elementwise:  \n",
    "## 3*1 + 4*2 + 5*3 = 3 + 8 + 15 = 26\n",
    "# and so on.\n",
    "A = np.array(range(12)).reshape(4,3)\n",
    "print('A:\\n', A, '\\n')\n",
    "\n",
    "B = np.array([1,2,3])#.reshape(1, -1)\n",
    "print('B:', B, '\\n')\n",
    "\n",
    "np.dot(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bu64jh4K3oKt"
   },
   "source": [
    "# Basics of Building a Neural Network with Keras:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TNk4Uz27wVZc"
   },
   "source": [
    "**Basics of Building a Neural Network with Keras:**\n",
    "1. Import required modules\n",
    "2. Decide on a network architecture (have only discussed sequential thus far)\n",
    "3. Adding layers - specifying layer type, number of neurons, activation functions, and, optionally, the input shape.\n",
    "4. Compile the model:\n",
    "    - Specify optimiziers\n",
    "    - specify loss functions\n",
    "    - specify metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ary82Ge-wfTb"
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(10, activation = 'relu', input_shape=(64*64*3, 790 )))\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss='mse',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "58e3B6PDzIjZ"
   },
   "source": [
    "5. Training the model\n",
    "    - `model.fit(X_train, y_train, epochs=20,batch_size=512,validation_data=(x_val,y_val))`\n",
    "    \n",
    "    - **batches:**\n",
    "        - a set of N samples, processed independently in parallel\n",
    "        - a batch determines how many samples are fed through before back-propagation. \n",
    "        - model only updates after a batch is complete.\n",
    "        - ideally have as large of a batch as your hardware can handle without going out of memory.\n",
    "            - larger batches usually run faster than smaller ones for evaluation/prediction. \n",
    "    - **epoch:**\n",
    "        - arbitrary cutoff / \"one pass over the entire dataset\", useful for logging and periodic evaluation\n",
    "        - when using kera's `model.fit` parameters `validation_data` or `validation_split`, these evaluations run at the end of every epoch.\n",
    "        - Within Keras can add callbacksto be run at the end of an epoch. Examples of these are learning rate changes and model checkpointing (saving).\n",
    "        \n",
    "    - **`history =  model.fit()` creates history object with .history attribute.**\n",
    "        - `history.history()` returns a dictionary of metrics from each epoch. \n",
    "            - `history.history['loss']` and `history.history['acc']`\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YKYmTkXmzI_G"
   },
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "history = model.fit(train_images,train_labels, epochs=20,\n",
    "                    batch_size=10, validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "99nQr20j2YFr"
   },
   "source": [
    "### Additional Keras - Getting Started Links:\n",
    "* https://keras.io/getting-started/\n",
    "* https://keras.io/getting-started/sequential-model-guide/#compilation\n",
    "* https://www.coursera.org/learn/deep-neural-network/lecture/BhJlm/rmsprop\n",
    "* https://www.coursera.org/learn/deep-neural-network/lecture/qcogH/mini-batch-gradient-descent\n",
    "* A full book on Keras by the author of Keras himself:  \n",
    "    https://www.manning.com/books/deep-learning-with-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GOVrYSEuDHhD"
   },
   "source": [
    "# Using Keras for Text Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S_vvC98X20Zj"
   },
   "source": [
    "## Preprocessing Text with Keras\n",
    "[Link for Learn.co Keras Lab](https://github.com/jirvingphd/dsc-04-41-05-keras-lab-online-ds-ft-021119/tree/solution)\n",
    "\n",
    "- The Keras Lab uses a neural network to analyze the text inside of filed bank complaints.\n",
    "- The complaint's text preprocessing in this lab involves several steps:\n",
    "    1. create word vector counts ( a bag of words types of representation)\n",
    "    2. Change category labels to integers\n",
    "    3. Train-test-split the processed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213
    },
    "colab_type": "code",
    "id": "-IkgOmqY2XNc",
    "outputId": "3e34fee7-135b-4fd7-84f0-11b34fd7a72e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>Navient has sytematically and illegally failed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My wife became eligible for XXXX Loan Forgiven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
       "3  Student loan  Navient has sytematically and illegally failed...\n",
       "4  Student loan  My wife became eligible for XXXX Loan Forgiven..."
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Keras preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# Keras neural network basics\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "# Load in data\n",
    "data_url = 'https://raw.githubusercontent.com/jirvingphd/dsc-04-41-05-keras-lab-online-ds-ft-021119/solution/Bank_complaints.csv'\n",
    "df = pd.read_csv(data_url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "kaNiQGtE84Mc",
    "outputId": "fb65f9c3-a648-451a-99fe-fb76029fda61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences type: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Step 1. create word vector counts\n",
    "\n",
    "complaints = df[\"Consumer complaint narrative\"] \n",
    "tokenizer = Tokenizer(num_words=2000) #Initialize a tokenizer.\n",
    "\n",
    "tokenizer.fit_on_texts(complaints) #Fit it to the complaints\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(complaints) #Generate sequences\n",
    "print('sequences type:', type(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "QJb-TVqh89cd",
    "outputId": "1e5eeefe-c01b-4e81-f386-e25e87f45a40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot_results type: <class 'numpy.ndarray'>\n",
      "Found 50110 unique tokens.\n",
      "Dimensions of our coded results: (60000, 2000)\n"
     ]
    }
   ],
   "source": [
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary') #Similar to sequences, but returns a numpy array\n",
    "print('one_hot_results type:', type(one_hot_results))\n",
    "\n",
    "word_index = tokenizer.word_index #Useful if we wish to decode (more explanation below)\n",
    "\n",
    "print('Found %s unique tokens.' % len(word_index)) #Tokens are the number of unique words across the corpus\n",
    "\n",
    "\n",
    "print('Dimensions of our coded results:', np.shape(one_hot_results)) #Our coded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "colab_type": "code",
    "id": "lpbimdKa9J21",
    "outputId": "93e70fb7-20f8-4ca1-dc7d-75ae9a0c02f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original complaint text:\n",
      "I have already filed several complaints about AES/PHEAA. I was notified by a XXXX XXXX let @ XXXX, who pretended to be from your office, he said he was from CFPB. I found out this morning he is n't from your office, but is actually works at XXXX. \n",
      "\n",
      "This has wasted weeks of my time. They AES/PHEAA confirmed and admitted ( see attached transcript of XXXX, conversation at XXXX ( XXXX ) with XXXX that proves they verified the loans are not mine ) the student loans they had XXXX, and collected on, and reported negate credit reporting in my name are in fact, not mine. \n",
      "They conclued their investigation on XXXX admitting they made a mistake and have my name on soneone elses loans. I these XXXX loans total {$10000.00}, original amount. My XXXX loans I got was total {$3500.00}. We proved by providing AES/PHEAA, this with my original promissary notes I located recently, the XXXX of my college provided AES/PHEAA with their original shoeinf amounts of my XXXX loans which show different dates and amounts, the dates and amounts are not even close to matching these loans they have in my name, The original lender, XXXX XXXX Bank notifying AES/PHEAA, they never issued me a student loan, and original Loan Guarantor, XXXX, notifying AES/PHEAA, they never were guarantor of my loans. \n",
      "\n",
      "XXXX straight forward. But today, this person, XXXX XXXX, told me they know these loans are not mine, and they refuse to remove my name off these XXXX loan 's and correct their mistake, essentially forcing me to pay these loans off, bucause in XXXX they sold the loans to XXXX loans. \n",
      "\n",
      "This is absurd, first protruding to be this office, and then refusing to correct their mistake. \n",
      "\n",
      "Please for the love of XXXX will soneone from your office call me at XXXX, today. I am a XXXX vet and they are knowingly discriminating against me. \n",
      "Pretending to be you.\n",
      "\n",
      "\n",
      "\n",
      "Decoded review from Tokenizer:\n",
      "i have already filed several complaints about aes i was notified by a xxxx xxxx let xxxx who to be from your office he said he was from cfpb i found out this morning he is n't from your office but is actually works at xxxx this has weeks of my time they aes confirmed and admitted see attached of xxxx conversation at xxxx xxxx with xxxx that they verified the loans are not mine the student loans they had xxxx and on and reported credit reporting in my name are in fact not mine they their investigation on xxxx they made a mistake and have my name on loans i these xxxx loans total 10000 00 original amount my xxxx loans i got was total 00 we by providing aes this with my original notes i located recently the xxxx of my college provided aes with their original amounts of my xxxx loans which show different dates and amounts the dates and amounts are not even close to these loans they have in my name the original lender xxxx xxxx bank notifying aes they never issued me a student loan and original loan xxxx notifying aes they never were of my loans xxxx forward but today this person xxxx xxxx told me they know these loans are not mine and they refuse to remove my name off these xxxx loan 's and correct their mistake essentially me to pay these loans off in xxxx they sold the loans to xxxx loans this is first to be this office and then refusing to correct their mistake please for the of xxxx will from your office call me at xxxx today i am a xxxx and they are against me to be you\n"
     ]
    }
   ],
   "source": [
    "reverse_index = {v:k for k,v in word_index.items()}\n",
    "comment_idx_to_preview = 19\n",
    "print('Original complaint text:')\n",
    "print(complaints[comment_idx_to_preview])\n",
    "print('\\n\\n')\n",
    "\n",
    "#The reverse_index cell block above must be complete in order for this cell block to successively execute.\n",
    "decoded_review = ' '.join([reverse_index.get(i) for i in sequences[comment_idx_to_preview]])\n",
    "# print('Decoded review from Tokenizer:')\n",
    "# print(decoded_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "TQeBPTAp9VLG",
    "outputId": "26e225d8-6c7e-4cab-e495-92ce5ddab068"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test label shape: (1500, 7)\n",
      "Train label shape: (58500, 7)\n",
      "Test shape: (1500, 2000)\n",
      "Train shape: (58500, 2000)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: conveting descriptive categories into integers\n",
    "\n",
    "product = df[\"Product\"]\n",
    "\n",
    "le = preprocessing.LabelEncoder() #Initialize. le used as abbreviation fo label encoder\n",
    "le.fit(product)\n",
    "# print(\"Original class labels:\")\n",
    "# print(list(le.classes_))\n",
    "# print('\\n')\n",
    "product_cat = le.transform(product)  \n",
    "#list(le.inverse_transform([0, 1, 3, 3, 0, 6, 4])) #If you wish to retrieve the original descriptive labels post production\n",
    "\n",
    "# print('New product labels:')\n",
    "# print(product_cat)\n",
    "# print('\\n')\n",
    "\n",
    "# print('One hot labels; 7 binary columns, one for each of the categories.') #Each row will be all zeros except for the category for that observation.\n",
    "product_onehot = to_categorical(product_cat)\n",
    "# print(product_onehot)\n",
    "# print('\\n')\n",
    "# print('One hot labels shape:')\n",
    "# print(np.shape(product_onehot))\n",
    "\n",
    "# Step 3. Train-test-split\n",
    "import random\n",
    "random.seed(123)\n",
    "test_index = random.sample(range(1,10000), 1500)\n",
    "\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "print(\"Test label shape:\", np.shape(label_test))\n",
    "print(\"Train label shape:\", np.shape(label_train))\n",
    "print(\"Test shape:\", np.shape(test))\n",
    "print(\"Train shape:\", np.shape(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o1gzmEH--Laq"
   },
   "outputs": [],
   "source": [
    "# Building neural network\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) \n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax')) # output layer with units = # of classes\n",
    "\n",
    "# Compil the network\n",
    "model.compile(optimizer='SGD', loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U-a3Ucli-Wlm"
   },
   "source": [
    "## Building/Compilng the neural network\n",
    "- For neural network:\n",
    "    - Use 2 hidden layers with the relu activation. layer one =50 units, layer two= 25\n",
    "    - **Becuase we are doing  multiclass (7 classes) problem, our final layer will use a softmax classifier to output 7 class probabilities per case.**\n",
    "\n",
    "- For compiling then model:\n",
    "    -  Loss function: 'categorical_crossentropy`\n",
    "    - Optimizer: stochastic gradient descent 'SGD'\n",
    "    - metric: 'accuracy'\n",
    "    \n",
    "- Fit fitting:\n",
    "    - Use 120 epochs\n",
    "    - use 256 batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-rmU31MU_9Nn"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train,\n",
    "                    label_train,\n",
    "                    epochs=120,\n",
    "                    batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jsgxx8PNC-AE"
   },
   "source": [
    "## Visualizing and Evaluating Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "colab_type": "code",
    "id": "rnrM_JeJAAqa",
    "outputId": "5178b6be-51d3-4bbe-8cef-9df253aba8dd"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAGoCAYAAAATsnHAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBG1JJKggCIKgLYkEFQRBELQl/wDSrQ8p86VHbwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the results from history\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "acc_values = history_dict['acc']\n",
    "# If we used a validation_set, could also plot:\n",
    "# val_acc_values = history_dict['val_acc']\n",
    "# val_loss_values = history_dict['val_loss']\n",
    "\n",
    "\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "fig,ax = plt.subplots(2,1, sharex=True, figsize=(6,6))\n",
    "ax[0].set_title('Training Results')\n",
    "ax[0].plot(epochs, loss_values, 'g', label='Training loss')\n",
    "# ax[0].plot(epochs, val_loss_values,'b',label='Validation loss')\n",
    "# ax[0].set_title('Training loss')\n",
    "# ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].legend()\n",
    "\n",
    "\n",
    "ax[1].plot(epochs, acc_values,'r', label='Training Acc')\n",
    "# ax[0].plot(epochs, val_acc_values,'b',label='Validation Acc')\n",
    "\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "ax[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "HfWTBNHLCGCg",
    "outputId": "67f92286-297d-4f6b-dbd0-37a8e9642e29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "x4XNYJQxAeed",
    "outputId": "26693a30-ed62-4c3a-dc7d-1a84be7b4c6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58500/58500 [==============================] - 2s 40us/step\n",
      "Training set: [0.3301095386188254, 0.8831111111111111]\n",
      "\n",
      "1500/1500 [==============================] - 0s 43us/step\n",
      "Testing set: [0.2650634905695915, 0.9226666668256124]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Making Predictions and Evaluating Performance\n",
    "\n",
    "y_hat_test = model.predict(test) #Your code here; Output (probability) predictions for the test set.\n",
    "\n",
    "# Evaluate training set\n",
    "results_train = model.evaluate(train, label_train)\n",
    "print(f'Training set: {results_train}\\n')\n",
    "\n",
    "# Evaluate testing set\n",
    "results_test = model.evaluate(test, label_test)\n",
    "print(f'Testing set: {results_test}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GKw2nWdMDKR6"
   },
   "source": [
    "# Network Regularlization & Normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lbzweQKTDPTl"
   },
   "source": [
    "## Overview - Regularization\n",
    "- Bias vs variance trade-off\n",
    "- Using test, train, and vali splits. \n",
    "- Prevent overfitting by adding regularization methods (L1, L2, dropout)\n",
    "- Optimizing and training time reduction by normalizing inputs\n",
    "    - Normalizing inputs can drasticaly decrease computation time, and prevent vanishing/exploding graidents. \n",
    "    \n",
    "### Hyperparameters to Tune\n",
    "- Number of hidden units\n",
    "- Number of layers\n",
    "- Learning rate ( $\\alpha$)\n",
    "- Activation function\n",
    "\n",
    "### Training, Validation, and Test Sets\n",
    "- The fact that there are so many hyperparameters to tune calls for a formalized and unbiased approach to testing/training sets.\n",
    "- We will use 3 sets when running, selecting, and validating a model:\n",
    "    - Training set: for training the alogrithm\n",
    "    - Validation set: to decide which model will be the final one after parameter tuning\n",
    "    - Testing set: after choosing final  the final model, use the test set for an inbiased estimate of performance. \n",
    "- Set sizes:\n",
    "    - With big data, your dev and test sets don't necessarily need to be 20-30% of all the data. \n",
    "    - You can choose test and hold-out sets that are of size 1-5%. \n",
    "        - eg. 96% train, 2% hold-out, 2% test set. \n",
    "    - It is **VERY IMPORTANT** to make sure holdout and test sample come from the same distribution: eg. same resolution of santa pictures. \n",
    "    \n",
    "### Bias vs Variance \n",
    "- A model with high bias may result in underfitting.\n",
    "    - <img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-42-02-tuning-neural-networks-with-regularization-online-ds-ft-021119/master/figures/underfitting.png\" width=200>\n",
    "- A model with high variance may result in overfitting. \n",
    "    - <img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-42-02-tuning-neural-networks-with-regularization-online-ds-ft-021119/master/figures/overfitting.png\" width=200>\n",
    "\n",
    "- In deep learning, there is less of a bias-variance trad-off vs simpler models. \n",
    "\n",
    "**Rules of thumb re: bias/variance trade-off:**\n",
    "\n",
    "| High Bias? (training performance) | high variance? (validation performance)  |\n",
    "|---------------|-------------|\n",
    "| Use a bigger network|    More data     |\n",
    "| Train longer | Regularization   |\n",
    "| Look for other existing NN architextures |Look for other existing NN architextures |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BlPs5Wd8GrNg"
   },
   "source": [
    "### L1 & L2 Regularlization\n",
    "- These methods of regularizaiton do so by penalizing coefficients(regression) or weights(neural networks),\n",
    "    - L1 & L2 exist in regression models as well. There, L1='Lasso Regressions' , L2='Ridge regression'\n",
    "\n",
    "- **L1 & L2 regularization add a term to the cost function.**\n",
    "\n",
    "$$Cost function = Loss (say, binary cross entropy) + Regularization term$$\n",
    "\n",
    "$$ J (w^{[1]},b^{[1]},...,w^{[L]},b^{[L]}) = \\dfrac{1}{m} \\sum^m_{i=1}\\mathcal{L}(\\hat y^{(i)}, y^{(i)})+ \\dfrac{\\lambda}{2m}\\sum^L_{l=1}||w^{[l]}||^2$$\n",
    "\n",
    "    - where $\\lambda$ is the regularization parameter. \n",
    "    - The difference between  L1 vs L2 is that L1 is just the sum of the weights whereas L2 is the sum of the _square_of the weights.  \n",
    "\n",
    "- **L1 Regularization:**\n",
    "    $$ Cost function = Loss + \\frac{\\lambda}{2m} * \\sum ||w||$$\n",
    "    - Uses the absolute value of weights and may reduce the weights down to 0. \n",
    "    \n",
    "        \n",
    "- **L2 Regularization:**:\n",
    "    $$ Cost function = Loss + \\frac{\\lambda}{2m} * \\sum ||w||^2$$\n",
    "    - Also known as weight decay, as it forces weights to decay towards zero, but never exactly 0.. \n",
    "    \n",
    "-  Regularization term $||w^{[l]}||^2 _F$  is  A.K.A. The Frobenius Norm\n",
    "    - $||w^{[l]}||^2 = \\sum^{n^{[l-1]}}_{i=1} \\sum^{n^{[l]}}_{j=1} (w_{ij}^{[l]})^2$\n",
    "\n",
    "    \n",
    "- **CHOOSING L1 OR L2:**\n",
    "    - L1 is very useful when trying to compress a model. (since weights can decreae to 0)\n",
    "    - L2 is generally preferred otherwise.\n",
    "    \n",
    "- **USING L1/L2 IN KERAS:**\n",
    "    - Add a kernel_regulaizer to a  layer.\n",
    "```python \n",
    "from keras import regularizers\n",
    "model.add(Dense(64, input_dim=64, kernel_regularizer=regularizers.l2(0.01))\n",
    "```\n",
    "    - here 0.01 = $\\lambda$\n",
    "\n",
    "### Dropout Regularization\n",
    "- Uses a specified probablity to random leave out a node from a ---epoch?\n",
    "\n",
    "\n",
    "- **USING DROPOUT IN KERAS:**\n",
    "    - Dropout layers are located in keras.layers.core \n",
    "    - Specify probably of being exlcuded/dropped out.\n",
    "```python\n",
    "from keras.layers.core import Dropout\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(output_dim=hidden1_num_units, input_dim=input_num_units, activation='relu'))\n",
    "model.add(layers.core.Dropout(Dropout(0.25))                              \n",
    "```\n",
    "\n",
    "### Data Augmentation (not covered in class)\n",
    "- Simplest way to reduce overfitting is to increase the size of the training data.\n",
    "- Difficult to do with large datasets, but can be implemented with images as shown below:\n",
    "- **For augmenting image data:**\n",
    "    - Can alter the images already present in the training data by shifting, shearing, scaling, rotating.<br><br> <img src =\"https://www.dropbox.com/s/9i1hl3quwo294jr/data_augmentation_example.png?raw=1\" width=300>\n",
    "    - This usually provides a big leap in improving the accuracy of the model. It can be considered as a mandatory trick in order to improve our predictions.\n",
    "\n",
    "- **In Keras:**\n",
    "    - `ImageDataGenerator` contains several augmentations available.\n",
    "    - Example below:\n",
    "    \n",
    "```python\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(horizontal flip=True)\n",
    "datagen.fit(train)\n",
    "```\n",
    "### Early Stopping (not covered in class)\n",
    "- Monitor performance for decrease or plateau in performance, terminate process when given criteria is reached.\n",
    "\n",
    "- **In Keras:**\n",
    "    - Can be applied using the [callbacks function](https://keras.io/callbacks/)\n",
    "```python    \n",
    "from keras.callbacks import EarlyStopping\n",
    "EarlyStopping(monitor='val_err', patience=5)\n",
    "```\n",
    "    - 'Monitor' denotes quanitity to check\n",
    "    - 'val_err' denotes validation error\n",
    "    - 'pateience' denotes # of epochs without improvement before stopping.\n",
    "        - Be careful, as sometimes models _will_ continue to improve after a stagnant period\n",
    "\n",
    "### Reference Links I found:\n",
    "- https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/\n",
    "- http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y0QuBXKwRlbc"
   },
   "source": [
    "## Overview - Network Optimization\n",
    "### Normalization\n",
    "- Normalizing to a consistent scale (typically 0 to 1) improves performance, but also ensures the process will converge to a stable solution. \n",
    "\n",
    "- Methods:\n",
    "    - Z-Score (subtracting mean, normalize by standard deviation)\n",
    "    \n",
    "#### Reference Links\n",
    "- https://www.coursera.org/lecture/deep-neural-network/normalizing-inputs-lXv6U\n",
    "\n",
    "### Changing Initial Parameters\n",
    "- The more input features into layer $l$, the smaller we want each weight $w_i$ to be.\n",
    "- Rule of thumb:\n",
    "    - $Var(w_i) = 1/n$ or $2/n$\n",
    "- A common initilization strategy for the relu activation functions is:\n",
    "\n",
    "    * $w^{[l]}$ `= np.random.randn(shape)*np.sqrt(2/n_(l-1))`\n",
    "    \n",
    "## Optimization:\n",
    "Alternatives to gradient descent that do not oscillate as much as g.d.:\n",
    "### Gradient Descent with Momentum:\n",
    "- Comutes an exponentially weighted average of the gradients to use.\n",
    "    - will dampen oscillations and improve performance.\n",
    "- How to:\n",
    "    -  Calculate current batch's moving averages for the derivatives of $W$ and$b$\n",
    "        - Compute $V_{dw} = \\beta V_{dw} + (1-\\beta)dW$\n",
    "        - $V_{db} = \\beta V_{db} + (1-\\beta)db$ \n",
    "        - So updated terms become\n",
    "            - $W:= W- \\alpha Vdw$\n",
    "            -$b:= b- \\alpha Vdb$\n",
    "    -  Generally, $\\beta=0.9$ is a good hyperparameter value.\n",
    "    \n",
    "### RMSprop\n",
    "- \"Root mean square\" prop\n",
    "- Slow down learning in one direction and speed it up in another.\n",
    "    - In the direction where we want to learn fast, the corresponding S will be small, so dividing by a small number. \n",
    "    - In the direction where we will want to learn slow, the corresponding S will be relatively large, and updates will be smaller. \n",
    "- How to:\n",
    "    - On each iteration, use exponentially weighted average again:\n",
    "        - exponentially weighted average of the squares of the derivatives\n",
    "        - $S_{dw} = \\beta S_{dw} + (1-\\beta)dW^2$\n",
    "        - $S_{db} = \\beta S_{dw} + (1-\\beta)db^2$\n",
    "        - So that:\n",
    "            - $W:= W- \\alpha \\dfrac{dw}{\\sqrt{S_{dw}}}$\n",
    "            - $b:= b- \\alpha \\dfrac{db}{\\sqrt{S_{db}}}$\n",
    "    - Often, add small $\\epsilon$ in the denominator to make sure that you don't end up dividing by 0.\n",
    "\n",
    "\n",
    "### Adam Optimization Algorithm\n",
    "- Adaptive Moment Estimation - essentially combines both methods above.\n",
    "- Works very well in most situations.\n",
    "- How to: \n",
    "    - Initialize: $V_{dw}=0, S_{dw}=0, V_{db}=0, S_{db}=0$.\n",
    "    - For each teration: compute $dW, db$ using the current mini-batch.\n",
    "        -  $V_{dw} = \\beta_1 V_{dw} + (1-\\beta_1)dW$, $V_{db} = \\beta_1 V_{db} + (1-\\beta_1)db$ \n",
    "        -  $S_{dw} = \\beta_2 S_{dw} + (1-\\beta_2)dW^2$, $S_{db} = \\beta_2 S_{db} + (1-\\beta_2)db^2$ \n",
    "        \n",
    "- As with  momentum and then RMSprop. We need to perform a correction! This is sometimes also done in RSMprop, but definitely here too.\n",
    "    - $V^{corr}_{dw}= \\dfrac{V_{dw}}{1-\\beta_1^t}$, $V^{corr}_{db}= \\dfrac{V_{db}}{1-\\beta_1^t}$\n",
    "\n",
    "    - $S^{corr}_{dw}= \\dfrac{S_{dw}}{1-\\beta_2^t}$, $S^{corr}_{db}= \\dfrac{S_{db}}{1-\\beta_2^t}$\n",
    "\n",
    "    - $W:= W- \\alpha \\dfrac{V^{corr}_{dw}}{\\sqrt{S^{corr}_{dw}+\\epsilon}}$ and\n",
    "\n",
    "    - $b:= b- \\alpha \\dfrac{V^{corr}_{db}}{\\sqrt{S^{corr}_{db}+\\epsilon}}$ \n",
    "\n",
    "\n",
    "### Learning Rate Decay\n",
    "- Learning rate decreases across epochs.\n",
    "    - $\\alpha = \\dfrac{1}{1+\\text{decay_rate * epoch_nb}}* \\alpha_0$\n",
    "\n",
    "- other methods:\n",
    "    - $\\alpha = 0.97 ^{\\text{epoch_nb}}* \\alpha_0$ (or exponential decay)<br>OR:\n",
    "    - $\\alpha = \\dfrac{k}{\\sqrt{\\text{epoch_nb}}}* \\alpha_0$<br> OR:\n",
    "    - Manual decay.\n",
    "    \n",
    "    \n",
    "    \n",
    "### HYPERPARAMETER TUNING:\n",
    "Most important:\n",
    "- $\\alpha$\n",
    "\n",
    "Important next:\n",
    "- $\\beta$ (momentum)\n",
    "- Number of hidden units\n",
    "- mini-batch-size\n",
    "\n",
    "Finally:\n",
    "- Number of layers\n",
    "- Learning rate decay\n",
    "\n",
    "Almost never tuned:\n",
    "- $\\beta_1$, $\\beta_2$, $\\epsilon$ (Adam)\n",
    "\n",
    "- Tip: Don't use a grid, because hard to say in advance which hyperparameters will be important.\n",
    "\n",
    "\n",
    "### OPTIMIZAITON REFS:\n",
    "- https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "- https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n",
    "- https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/\n",
    "- https://stackoverflow.com/questions/37232782/nan-loss-when-training-regression-network https://www.springboard.com/blog/free-public-data-sets-data-science-project/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Trd6RcQwP-m8"
   },
   "source": [
    "---\n",
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TN2YFPhxYm5h"
   },
   "source": [
    "## Overview - CNNs\n",
    "\n",
    "CNNs have certain features that identify patterns in images because of  \"convolution operation\" including:\n",
    "\n",
    "- Dense layers learn global patterns in their input feature space\n",
    "\n",
    "- Convolution layers learn local patterns, and this leads to the following interesting features:\n",
    "    - Unlike dense networks, local patterns recognized can be recognized _anywhere_ in the images. \n",
    "    - Deeper convolutional neural networks can learn spatial hierarchies.\n",
    "        - A first layer will learn small local patterns, a second layer will learn larger patterns using features of the first layer patterns, etc. \n",
    "    \n",
    "- Because of these properties, CNNs are great for tasks like:\n",
    "    - Image classification\n",
    "    - Object detection in images\n",
    "    - Picture neural style transfer\n",
    "\n",
    "## How Convolution works:\n",
    "\n",
    " <img src =\"https://raw.githubusercontent.com/jirvingphd/dsc-04-43-02-convolutional-neural-networks-online-ds-ft-021119/master/architecture-cnn.png\">\n",
    " \n",
    " - **The Convolution Operation:**\n",
    "    - Detect complex biilding blocks or features that can aid in a larger task. \n",
    "        - i.e. detecting vertical or horizontal edges (example below)<br>\n",
    "        <img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-43-02-convolutional-neural-networks-online-ds-ft-021119/master/conv.png\" width=200><br>\n",
    "        - Here, the 3 x 3 matrix on the right represents the _filter_  used to perform a convolution operation to detect horizntal edges.\n",
    "        - The 5x5 image on the left  would by intensity values between 1-255(or rescaled to 1-10)\n",
    "    - **In Keras this is perfomed by a `Conv2d` layer.**\n",
    "        - Applies the convolution filter to every possible 3x3 region posible (for above example) <br>\n",
    " <img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-43-02-convolutional-neural-networks-online-ds-ft-021119/master/convolution-layer-a.png\" width=400>\n",
    "\n",
    "### **Padding:**\n",
    "- Some issues arise using filteres on images.\n",
    "    - Image shrinks with each convolution layer.\n",
    "        - Examples:\n",
    "            - Start with a 5x5 image and use a 3x3 convolution matrix, result is 3x3 image. \n",
    "            - Start with a 10x10 image and use a 3x3 conv matrix, result is 8x8 image. \n",
    "    - The pixels around the edges are used much less in the outputs due to the filter. \n",
    "- Padding solves these issues \n",
    "    - Adding one layer of pixels around the edges preserves the image size with a 3x3 filter. \n",
    "    - Example: padding our 5x5 image (so it becomes a 6x6 image), we can now use a 3x3 conv filter and the resulting image is stil 5x5.\n",
    "- Padding terminology:\n",
    "    - 'Valid' - no padding.\n",
    "    - 'Same' - padding such that the output is the same size. \n",
    "\n",
    "- **Strided convolutions**\n",
    "    - Stride = how the filter moves over the image. \n",
    "        - Increasing stride so that it moves 2 pixels over instead of 1 results in a smaller # of locations.\n",
    "        - Rarely used in practice, but important option for some models. \n",
    " \n",
    "### Convolutions with color images: \n",
    "- Let's assume a 7x7 RGB, which becomes a 7 x 7 x 3 tensor. \n",
    "    - Will need to use a filter that has the third dimension as 3. \n",
    "    - Can use to detect features on individual color channels.\n",
    "- Can deconvolve with several 3D filters, then stack the output results together. \n",
    "\n",
    "- **Advantage: your image can be huge and the amount of parameters only depends on how many _filters_ used.**\n",
    "    - Lets say we have 20 3x3x3(color) images \n",
    "        - Would have $20 * 27 + a_bias_for_each_filter (1*20) = 560$ parameters\n",
    "\n",
    "- **Notation:**\n",
    "\n",
    "    - $f^{[l]}$ = size of the filter\n",
    "    - $p^{[l]}$ = padding\n",
    "    - $s^{[l]}$ = amount of stride\n",
    "    - $ n_c^{[l]}$ = number of filters\n",
    "    - filter: $f^{[l]}$ x $f^{[l]}$ x $ n_c^{[l-1]}$\n",
    "\n",
    "\n",
    "- Input =  $n_h^{[l-1]} * n_w^{[l-1]} * n_c^{[l-1]}$\n",
    "- Output = $n_h^{[l]} * n_w^{[l]} * n_c^{[l]}$\n",
    "\n",
    "- Height and width are given by:\n",
    "\n",
    "    - $n_h^{[l]}= \\Bigr\\lfloor\\dfrac{n_h^{[l-1]}+2p^{[l]}-f^{[l]}}{s^{[l]}}+1\\Bigr\\rfloor$\n",
    "\n",
    "    - $n_w^{[l]}= \\Bigr\\lfloor\\dfrac{n_w^{[l-1]}+2p^{[l]}-f^{[l]}}{s^{[l]}}+1\\Bigr\\rfloor$\n",
    "\n",
    "\n",
    "- Activations: $a^{[l]}$ is of dimension $ n_h^{[l]} * n_w^{[l]} * n_c^{[l]} $\n",
    "\n",
    "## Pooling Layers\n",
    "- After Conv layers, intended to significantly downsample the previous convolution layer.\n",
    "    - After conv layers detect edges/patterns, poooling layers will take a summary of the convoltuions from a larger section.\n",
    "    - Most common pooling layer is MaxPooling\n",
    "        - Takes the maximum of all convolutions from a larger area of the iamge. \n",
    "        - Works better than average pooling.\n",
    "- Downsampling is essential for viable executin times. \n",
    "\n",
    "- **MaxPooling Hyperparameters:**\n",
    "    - Parameter names:\n",
    "        - filter_size (f)\n",
    "        - stride (S)\n",
    "    - Common combinations:\n",
    "        - f=2, s=2\n",
    "        - f=3, s=2\n",
    "        \n",
    "### Additional Resources\n",
    "\n",
    "* https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html\n",
    "* https://datascience.stackexchange.com/questions/16463/what-is-are-the-default-filters-used-by-keras-convolution2d\n",
    "* https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks\n",
    "* https://www.coursera.org/learn/convolutional-neural-networks/lecture/A9lXL/simple-convolutional-network-example\n",
    "* https://www.coursera.org/learn/convolutional-neural-networks/lecture/uRYL1/cnn-example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "buNyHkx59tPn"
   },
   "source": [
    "# Notes on working with images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3cuDaFkcTJ6A"
   },
   "source": [
    "## Loading Images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kkFaiFyX9xky"
   },
   "source": [
    "\n",
    "### Using `matplotlib.pyplot` vs `Keras.preprocessing.image`\n",
    "\n",
    "- Using matplotlib:\n",
    "```python\n",
    "imgPlt = plt.imread('person3_virus_16.jpeg')`\n",
    "plt.imshow(img)\n",
    "```\n",
    "- Using Keras\n",
    "    - taken from visualizing intermediate activations lab\n",
    "\n",
    "```python\n",
    "# Load the image file\n",
    "from keras.preprocessing import image\n",
    "img_file = 'person3_virus_16.jpeg'\n",
    "img = image.load_img(img_file)\n",
    "\n",
    "# Create a tensor from the img\n",
    "img_tensor = image.img_to_array(img)\n",
    "img_tensor = np.expand_dims(img_tensor, axis=0) # Not explained\n",
    "normalize to pixel intensity values\n",
    "img_tensor/=255 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jx6eT01wTNxL"
   },
   "source": [
    "## Splitting Images into separate folders for keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "srbjybjqQ5m9"
   },
   "source": [
    "### Using os, shutil to create directories and copy files\n",
    "- from [Convolutional Neural Networks - Codealong](https://github.com/jirvingphd/dsc-04-43-03-convolutional-neural-networks-code-along-online-ds-ft-021119)\n",
    "\n",
    "- **first define the folders that currently contain the images get their filenames**\n",
    "\n",
    "```python\n",
    "import os, shutil\n",
    "\n",
    "# Define directories to be created:\n",
    "data_santa_dir = 'data/santa/'\n",
    "data_not_santa_dir = 'data/not_santa/'\n",
    "new_dir = 'split/'\n",
    "\n",
    "# Store the list of all the relevant training target images\n",
    "imgs_santa = [file for file in os.listdir(data_santa_dir) if file.endswith('.jpg')]\n",
    "print('There are',len(imgs_santa), 'santa images')\n",
    "\n",
    "# Store the list of all non-target images\n",
    "imgs_not_santa = [file for file in os.listdir(data_not_santa_dir) if file.endswith('.jpg')]\n",
    "print('There are', len(imgs_not_santa), 'images without santa')\n",
    "\n",
    "```\n",
    "\n",
    "- **Now create new directries and for training, testing, and validation images.**\n",
    "\n",
    "```python\n",
    "# Create the main folder for all of the new sub-folders\n",
    "os.mkdir(new_dir)\n",
    "\n",
    "# Create valid pathnames inside of the new_dir for training images\n",
    "train_folder = os.path.join(new_dir, 'train')\n",
    "train_santa = os.path.join(train_folder, 'santa')\n",
    "train_not_santa = os.path.join(train_folder, 'not_santa')\n",
    "\n",
    "# Create valid pathnames inside of the new_dir for testing images\n",
    "test_folder = os.path.join(new_dir, 'test')\n",
    "test_santa = os.path.join(test_folder, 'santa')\n",
    "test_not_santa = os.path.join(test_folder, 'not_santa')\n",
    "\n",
    "# Create valid pathnames inside of the new_dir for validation images\n",
    "val_folder = os.path.join(new_dir, 'validation')\n",
    "val_santa = os.path.join(val_folder, 'santa')\n",
    "val_not_santa = os.path.join(val_folder, 'not_santa')\n",
    "\n",
    "\n",
    "# Now create all of the folders defined above\n",
    "os.mkdir(test_folder)\n",
    "os.mkdir(test_santa)\n",
    "os.mkdir(test_not_santa)\n",
    "\n",
    "os.mkdir(train_folder)\n",
    "os.mkdir(train_santa)\n",
    "os.mkdir(train_not_santa)\n",
    "\n",
    "os.mkdir(val_folder)\n",
    "os.mkdir(val_santa)\n",
    "os.mkdir(val_not_santa)\n",
    "\n",
    "```\n",
    "\n",
    "- **Now that we have the folders, copy the desired # of images to the correct dataset folders**\n",
    "\n",
    "```python\n",
    "# The user decided to put 271 images in the training set, 100 in the validation set, and 90 in the test set\n",
    "# train santa\n",
    "imgs = imgs_santa[:271]\n",
    "for img in imgs:\n",
    "    origin = os.path.join(data_santa_dir, img)\n",
    "    destination = os.path.join(train_santa, img)\n",
    "    shutil.copyfile(origin, destination)\n",
    "    \n",
    "# validation santa\n",
    "imgs = imgs_santa[271:371]\n",
    "for img in imgs:\n",
    "    origin = os.path.join(data_santa_dir, img)\n",
    "    destination = os.path.join(val_santa, img)\n",
    "    shutil.copyfile(origin, destination)\n",
    "    \n",
    "# test santa\n",
    "imgs = imgs_santa[371:]\n",
    "for img in imgs:\n",
    "    origin = os.path.join(data_santa_dir, img)\n",
    "    destination = os.path.join(test_santa, img)\n",
    "    shutil.copyfile(origin, destination)\n",
    "    \n",
    "## REPEATED FOR FOR THE NON-SANTA IMAGES - NOT SHOWN   \n",
    "```\n",
    "\n",
    "- Now that we have images in separate directories, we can use the Kera's ImageDataGenerators .flow_from_directory() method.\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "# get all the data in the directory split/test (180 images), and reshape them\n",
    "test_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        test_folder, \n",
    "        target_size=(64, 64), batch_size = 180) \n",
    "# ...do the same for train and val (not shown)\n",
    "\n",
    "\n",
    "# create the data sets\n",
    "train_images, train_labels = next(train_generator)\n",
    "\n",
    "# Make sure things worked\n",
    "print (\"Number of training samples: \" + str(m_train))\n",
    "print (\"train_images shape: \" + str(train_images.shape))\n",
    "\n",
    "\n",
    "# Reshape the training images to have one column(/row?) for each image\n",
    "train_img = train_images.reshape(train_images.shape[0], -1)\n",
    "print(train_img.shape)\n",
    "\n",
    "\n",
    "# Reshape the labels to match the data\n",
    "train_y = np.reshape(train_labels[:,0], (542,1))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T6KbsPbcgNt-"
   },
   "source": [
    "# Convolutional Neural Networks with Keras "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YLVf8sG6UOTT"
   },
   "source": [
    "## Section 43 recap\n",
    "\n",
    "* CNN are often preferred over densely connected networks when the goal is to perform image classification\n",
    "* Using CNNs instead of densely connected networks can drastically decrease the amount of parameters needed (and as a consequence, increase the speed)\n",
    "* When building a CNN, the end of the network architecture always connects back to a densely connected network to perform classification\n",
    "* At the most basic level, a convolution is particularly good at detecting small image features, such as horizontal or vertical edges\n",
    "* Convolutions use *filter* to detect these small patterns, and these filters are generally of size 3x3 or 5x5.\n",
    "* In CNNs, padding can be used to prevent shrinkage and to make sure pixels at the edge of an image deserve the necessary attention\n",
    "* To understand CNN structures and how the dimensions change in each layer, it is important to understand how filters work and how the number of filters affect the network structure.\n",
    "* When visualizing intermediate layers in a CNN, you noticed that images become more abstract with each layer we add to a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jUgyK0HRQBxT"
   },
   "source": [
    "## Building CNN From Scratch Lab\n",
    "- https://github.com/learn-co-students/dsc-04-43-04-building-a-cnn-from-scratch-online-ds-ft-021119/tree/solution\n",
    "- CNN's are great for image processing\n",
    "### Image Data\n",
    "\n",
    "```python\n",
    "import os #for listdir()\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "train_dir = 'chest_xray_downsampled/train'\n",
    "validation_dir = 'chest_xray_downsampled/val/'\n",
    "test_dir = 'chest_xray_downsampled/test/'\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Train_generator example\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        train_dir,\n",
    "        # All images will be resized to 150x150\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "```\n",
    "- **Images are store in ImageDataGenerators**\n",
    "    - Generally rescale to... intensity values? of 1./255\n",
    "    - load in files with ImageDataGenerator.flow_from_directory( :\n",
    "        - directory\n",
    "        - the target_size (the size to convert all images to)\n",
    "        - batch_size\n",
    "        - class_mode \n",
    "            - Note: selection of loss function determines chocie.\n",
    "                - If using 'binary_crossentropy' for binary classification, use class_mode='binary'\n",
    "                \n",
    "- ImageDataGenerators are also used for augmenting data. \n",
    "\n",
    "```python\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "```\n",
    "\n",
    "### Setting Up Initial Network\n",
    "```python\n",
    "from keras import models, layers, optimizers\n",
    "# or if want to do exact layers:\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense)\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# Initialize sequential model\n",
    "model = Sequential()\n",
    "```\n",
    "**1A) A CNN should start with a Conv2D later**\n",
    "\n",
    "```python\n",
    "layers.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
    "```\n",
    "\n",
    "- Conv2D layers parameters (to change):\n",
    "    - filters:  # of samples to take from each image (kind of like # of neurons?) (e.g. filters=32)\n",
    "    - kernel_size: size (in pixels) of the filters (e.g kernel_size=(3,3)\n",
    "    - activation: activation function to use (e.g. 'relu')\n",
    "        \n",
    "**1B) MaxPooling2D layers following Conv2D layers**\n",
    "```python\n",
    "layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)\n",
    "```\n",
    "\n",
    "-  MaxPooling2D parameters:\n",
    "    -  pool_size: factor by which to downscale. \n",
    "        - e.g.pool_size=(2,2) will half information in vertical and horizational direction\n",
    "     \n",
    "**1C,optional) Add a Dropout layer to avoid overfitting:** [Udemy course suggestion]\n",
    "```python\n",
    "layers.Dropout(rate, noise_shape=None, seed=None)\n",
    "```\n",
    "- Dropout parameters:\n",
    "    - rate = 0.25 (used by udemy course)\n",
    "\n",
    "**2) Repeat: Continue layering combinations of Conv2D / MaxPooling2D layers** (Dropout too?):\n",
    "- Later layers will need larger # of filters to detect more abstract patterns.\n",
    "\n",
    "```python\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "```\n",
    "\n",
    "**3A) Flatten Data Before Passing on to Dense Layers for Classification/Learning**\n",
    "```python\n",
    "layers.Flatten(data_format =None)\n",
    "```\n",
    "**3B)  Add Dense layers at the end of the convolutional base for learning:**\n",
    "```python\n",
    "layers.Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None))\n",
    "```\n",
    "- Will only need to worry about basic parameters:\n",
    "    - Units:\n",
    "        - Larger #, used for the actual learning.\n",
    "    - Activation\n",
    "        - User choice, 'relu' is always good.\n",
    "        \n",
    "**3C) Add final Dense layer to determine output classification**\n",
    "- Add  a final small Dense layer (depending on number of classes?)\n",
    "    - For binary classification:\n",
    "        - units: 1\n",
    "        - activation: 'sigmoid'\n",
    "        \n",
    "``` python\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "```\n",
    "\n",
    "**4) Compile the model, selecting loss function, optimizer, and metric**\n",
    "\n",
    "- Loss Function:\n",
    "    - For binary classifications, use 'binary_crossentropy'\n",
    "- Optimizer:\n",
    "    - Use RMSProp,\n",
    "        -Specify learning rate: ```lr = 1e-4```\n",
    "- Metrics:\n",
    "    - Use 'acc' for accuracy\n",
    "```python\n",
    "keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "# Compile Model\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer=optimizers.RMSProp(lr=1e-4),\n",
    "              metrics=['acc'])\n",
    "\n",
    "#Set the model to train; \n",
    "import datetime\n",
    "start=datetime.datetime.now()\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=30,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50)\n",
    "\n",
    "end=datetime.datetime.now()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8rz5t0lMYCfd"
   },
   "source": [
    "# Use Pretrained CNNs\n",
    "\n",
    "## Pretrained Networks Overview\n",
    "- Pretrained networks have already been trained on large pools of data, and have their weights frozen.\n",
    "    - They enable deep learning on fairly small image datasets\n",
    "    - a 'small' dataset is less than tens of thousands or hundreds of thousands of images\n",
    "\n",
    "- Pretrained networks can be used in whole or only specific parts, depending on your need/data.\n",
    "    - The shallower the layer of neurons, the more generic its features are. \n",
    "        - Therefore even if you data is very different, you can still use the lower layers for basic feature extraction.\n",
    "    - The deeper the layer, the more abstract its features are.\n",
    "        - so you may want to unfreeze the deeper/higher order classificaiton layers and re-train the network on your images. \n",
    "<br><br>\n",
    "### Where to find the pre-trained networks\n",
    "- **Pretrained Networks are available in [Keras.applications](https://keras.io/applications/)**\n",
    "    - This list of pretained models are for image classification. \n",
    "        - DenseNet\n",
    "        - InceptionResNetV2\n",
    "        - InceptionV3\n",
    "        - MobileNet\n",
    "        - NASNet\n",
    "        - ResNet50\n",
    "        - VGG16\n",
    "        - *VGG19* - used in labs\n",
    "        - Xception\n",
    "\n",
    "    - You can import these networks and use it as a function with 2 arguments:\n",
    "        1. `weights`\n",
    "            - Determines which data source's training data weights to use.\n",
    "            - ex:  `weights='imagenet'\n",
    "        2. `include_top`\n",
    "            - determines whetehr or not to include the fully-connected layer at the top of the network\n",
    "```python\n",
    "from keras.applications import MobileNet\n",
    "conv_base = MobileNet(weights='imagenet', include_top=True)\n",
    "```\n",
    "\n",
    "### How to use pretrained networks for feature extraction or for fine-tuning\n",
    "\n",
    "**You'll learn about two ways to use pre-trained networks:**\n",
    "- **Feature extraction**: here, you use the representations learned by a previous network to extract interesting features from new samples. \n",
    "    - Method 1) Use the convolutional base layers and run your data to detect the basic features, and save the output data, which is then run a new dense classifier, which is trained from scratch.  \n",
    "        - (+) It is fast\n",
    "        - (-) but cant use data augmentation. \n",
    "        - Note:  If your images are very different from the pretraining datasets, you may want to only use _part_ of the convolutional base but a _new_ densely connected classifier\n",
    "    - Method 2) Extend the conv_base by adding dense layers on top, running everything together. \n",
    "        - (+) allows for data sugmentation\n",
    "        - (-) extremely time-consuming and requires GPU\n",
    "  \n",
    "- **Fine-tuning**: when finetuning, you'll \"unfreeze\" a few top layers from the convolutional base of the model and train them again together with the densely connected classifier layers of the model. \n",
    "    - Note that you are changing the parts of the convolutional layers here that were used to detect the more abstract features.\n",
    "    - By doing this, you can make your model more relevant for the classification problem at hand.\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "* http://cs231n.stanford.edu/syllabus.html\n",
    "* https://www.dlology.com/blog/gentle-guide-on-how-yolo-object-localization-works-with-keras/\n",
    "* https://www.dlology.com/blog/gentle-guide-on-how-yolo-object-localization-works-with-keras-part-2/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "VHwZgQ9mQAu3",
    "outputId": "343aed57-15ae-4324-da54-43cc159db7e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_224_tf.h5\n",
      "17227776/17225924 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import MobileNet\n",
    "conv_base = MobileNet(weights='imagenet',\n",
    "                  include_top = True)\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OStc-MxyrNMY"
   },
   "source": [
    "# Using  Pretrained Networks - Codealong\n",
    "## Theory/Tips\n",
    "\n",
    "## Code \n",
    "\n",
    "### Feature Extraction Method 1:\n",
    "\n",
    "```python\n",
    "from keras.applications import VGG19\n",
    "cnn_base = VGG19(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(64, 64, 3))\n",
    "cnn_base.summary()\n",
    "\n",
    "# ---\n",
    "\n",
    "def extract_features(directory, sample_amount):\n",
    "    features = np.zeros(shape=(sample_amount, 2, 2, 512)) \n",
    "    labels = np.zeros(shape=(sample_amount))\n",
    "    generator = datagen.flow_from_directory(\n",
    "        directory, target_size=(64, 64), \n",
    "        batch_size = 10, \n",
    "        class_mode='binary')\n",
    "    i=0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = cnn_base.predict(inputs_batch)\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch \n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        i = i + 1\n",
    "        if i * batch_size >= sample_amount:\n",
    "            break\n",
    "    return features, labels\n",
    "\n",
    "# ---\n",
    "\n",
    "# you should be able to divide sample_amount by batch_size!!\n",
    "train_features, train_labels = extract_features(train_folder, 540) \n",
    "validation_features, validation_labels = extract_features(val_folder, 200) \n",
    "test_features, test_labels = extract_features(test_folder, 180)\n",
    "\n",
    "train_features = np.reshape(train_features, (540, 2 * 2 * 512))\n",
    "validation_features = np.reshape(validation_features, (200, 2 * 2 * 512))\n",
    "test_features = np.reshape(test_features, (180, 2 * 2 * 512))\n",
    "\n",
    "# ---\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_dim=2 * 2 * 512))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(train_features, train_labels,\n",
    "                    epochs=20,\n",
    "                    batch_size=10,\n",
    "                    validation_data=(validation_features, validation_labels))\n",
    "\n",
    "results_test = model.evaluate(test_features, test_labels)\n",
    "\n",
    "# ---\n",
    "\n",
    "train_acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epch = range(1, len(train_acc) + 1)\n",
    "plt.plot(epch, train_acc, 'g.', label='Training Accuracy')\n",
    "plt.plot(epch, val_acc, 'g', label='Validation acc')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epch, train_loss, 'r.', label='Training loss')\n",
    "plt.plot(epch, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#---\n",
    "\n",
    "```\n",
    "\n",
    "## Feature Extraction Method 2\n",
    " - this method is much more costly, but allows us to use data augmentation\n",
    " \n",
    "- The process:\n",
    "    1. Add the pretrained model as the first layer\n",
    "    2. Add some dense layers as a classifier on top\n",
    "    3. Freeze the convolutional base\n",
    "        - This will prevent the weights from changing. \n",
    "        - The layer.trainable attribute indicates if a layer is frozen\n",
    "    4. Train the model. \n",
    "    \n",
    "    \n",
    "```python\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(cnn_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(132, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# ---\n",
    "\n",
    "#You can check whether a layer is trainable (or alter its setting) through the layer.trainable attribute:\n",
    "for layer in model.layers:\n",
    "    print(layer.name, layer.trainable)\n",
    "    \n",
    "#Similarly, we can check how many trainable weights are in the model:\n",
    "print(len(model.trainable_weights))\n",
    "\n",
    "# ---\n",
    "\n",
    "# Freeze the conv base\n",
    "cnn_base.trainable = False\n",
    "\n",
    "\n",
    "# ---\n",
    "\n",
    "# get all the data in the directory split/train (542 images), and reshape them\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_folder, \n",
    "        target_size=(64, 64), \n",
    "        batch_size= 20,\n",
    "        class_mode= 'binary') \n",
    "\n",
    "# get all the data in the directory split/validation (200 images), and reshape them\n",
    "val_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        val_folder, \n",
    "        target_size=(64, 64), \n",
    "        batch_size = 20,\n",
    "        class_mode= 'binary')\n",
    "\n",
    "# get all the data in the directory split/test (180 images), and reshape them\n",
    "test_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        test_folder, \n",
    "        target_size=(64, 64), \n",
    "        batch_size = 180,\n",
    "        class_mode= 'binary')\n",
    "\n",
    "test_images, test_labels = next(test_generator)\n",
    "\n",
    "# ---\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              metrics=['acc'])\n",
    "\n",
    "# ---\n",
    "\n",
    "history = model.fit_generator(\n",
    "              train_generator,\n",
    "              steps_per_epoch= 27,\n",
    "              epochs = 10,\n",
    "              validation_data = val_generator,\n",
    "              validation_steps = 10)\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d0sjUefMuIs0"
   },
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ey5E6in_uGHO"
   },
   "source": [
    "Up till now, we have frozen the entire convolutional base. Again, it cannot be stressed enough how important this is before fine tuning the weights of the later layers of this base. Without training a classifier on the frozen base first, there will be too much noise in the model and initial epochs will overwrite any useful representations encoded in the pretrained model. That said, now that we have tuned a classifier to the frozen base, we can now unfreeze a few of the deeper layers from this base and further fine tune them to our problem scenario. In practice, this is apt to be particularly helpful where adapted models span new domain categories. For example, if the pretrained model is on cats and dogs and this is adapted to a problem specific to cats (a very relatively similar domain) there is apt to be little performance gain from fine tuning. On the other hand, if the problem domain is more substantially different, additional gains are more likely in adjusting these more abstract layers of the convolutional base. With that, let's take a look at how to unfreeze and fine tune these later layers.\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "cnn_base.trainable = True\n",
    "\n",
    "# ---\n",
    "\n",
    "cnn_base.trainable = True\n",
    "set_trainable = False\n",
    "for layer in cnn_base.layers:\n",
    "    if layer.name == 'block5_conv1':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "        \n",
    "        \n",
    "# ---\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                      optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "# ---\n",
    "history = model.fit_generator(\n",
    "              train_generator,\n",
    "              steps_per_epoch= 27,\n",
    "              epochs = 10,\n",
    "              validation_data = val_generator,\n",
    "              validation_steps = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mtyhyKPOrgY5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "James' Bootcamp Notes - Neural Networks - Mod 4.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "320px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
