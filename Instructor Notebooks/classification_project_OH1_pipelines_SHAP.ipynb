{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 31: Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To revisit Linear (Algebra) Equations and revisit the relationship between $y=mx+b$ and $y= w^TX+B$.\n",
    "- To understand how support vector machine attempts to separate groups.\n",
    "- Discuss the advantages / disadvantages of SVMs\n",
    "\n",
    "- To understand the math notation of SVMs\n",
    "\n",
    "- Apply SVMs in the SVMs In scikit Learn Lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BLOG POSTS/ARTICLES\n",
    "    - [Towards Data Science - SVM Simply Explained](https://towardsdatascience.com/support-vector-machine-simply-explained-fee28eba5496)\n",
    "- STUDY GROUP RECORDINGS:\n",
    "    - [Support Vector Machines - Victor](https://www.youtube.com/watch?v=_QmnoubpU3Q&list=PLVoXE6pv5LIg4WOllQ4rNPi9BtvtVMb78&index=5)\n",
    "    - [The Kernel Trick - Victor](https://www.youtube.com/watch?v=mnN74NI4Gqk&list=PLVoXE6pv5LIg4WOllQ4rNPi9BtvtVMb78&index=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-16T05:01:50.120Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U fsds_100719\n",
    "from fsds_100719.imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to Our Iowa Prisoners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-16T05:01:50.123Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from fsds_100719.imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### previous functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-16T05:01:50.126Z"
    }
   },
   "outputs": [],
   "source": [
    "# fs.quick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-16T05:01:50.128Z"
    }
   },
   "outputs": [],
   "source": [
    "## a timer to record how long a process takes\n",
    "class Timer():\n",
    "    ## def init\n",
    "    def __init__(self,format_=\"%m/%d/%y - %I:%M %p\"):\n",
    "        import tzlocal\n",
    "        self.tz = tzlocal.get_localzone()\n",
    "        self.fmt = format_\n",
    "        \n",
    "        self.created_at = self.get_time()# get time\n",
    "    \n",
    "    ## def get time method\n",
    "    def get_time(self):\n",
    "        import datetime as dt\n",
    "        return dt.datetime.now(self.tz)\n",
    "\n",
    "    ## def start\n",
    "    def start(self):\n",
    "        time = self.get_time()\n",
    "        self.start = time\n",
    "        print(f\"[i] Timer started at {self.start.strftime(self.fmt)}\")\n",
    "        \n",
    "        \n",
    "\n",
    "    ## def stop\n",
    "    def stop(self):\n",
    "        time = self.get_time()\n",
    "        self.end = time\n",
    "        print(f\"[i] Timer ended at {self.end.strftime(self.fmt)}\")\n",
    "        print(f\"- Total time = {self.end-self.start}\")\n",
    "timer = Timer()\n",
    "print(timer.created_at)\n",
    "timer.start()\n",
    "timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-16T05:01:50.130Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_importance(tree, top_n=20,figsize=(10,10)):\n",
    "    df_importance = pd.Series(tree.feature_importances_,index=X_train.columns)\n",
    "    df_importance.sort_values(ascending=True).tail(top_n).plot(\n",
    "        kind='barh',figsize=figsize)\n",
    "    return df_importance\n",
    "\n",
    "## Write a fucntion to evalute the model\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "def evaluate_model(y_true, y_pred,X_true,clf):\n",
    "    \n",
    "    ## Classification Report / Scores \n",
    "    print(metrics.classification_report(y_true,y_pred))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,4),ncols=2)\n",
    "    metrics.plot_confusion_matrix(clf,X_true,y_true,cmap=\"Greens\",\n",
    "                                  normalize='true',ax=ax[0])\n",
    "    ax[0].set(title='Confusion Matrix')\n",
    "    y_score = clf.predict_proba(X_true)[:,1]\n",
    "\n",
    "    fpr,tpr,thresh = metrics.roc_curve(y_true,y_score)\n",
    "    # print(f\"ROC-area-under-the-curve= {}\")\n",
    "    roc_auc = round(metrics.auc(fpr,tpr),3)\n",
    "    ax[1].plot(fpr,tpr,color='darkorange',label=f'ROC Curve (AUC={roc_auc})')\n",
    "    ax[1].plot([0,1],[0,1],ls=':')\n",
    "    ax[1].legend()\n",
    "    ax[1].grid()\n",
    "    ax[1].set(ylabel='True Positive Rate',xlabel='False Positive Rate',\n",
    "          title='Receiver operating characteristic (ROC) Curve')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    try: \n",
    "        df_important = plot_importance(clf)\n",
    "    except:\n",
    "        df_important = None\n",
    "    \n",
    "#     return df_important\n",
    "## visualize the decision tree\n",
    "def visualize_tree(tree,feature_names=None,class_names=['0','1'],\n",
    "                   kws={},save_filename=None,format_='png',save_and_show=False):\n",
    "    \"\"\"Visualizes a sklearn tree using sklearn.tree.export_graphviz\"\"\"\n",
    "    from sklearn.tree import export_graphviz\n",
    "    from IPython.display import SVG\n",
    "    import graphviz #import Source\n",
    "    from IPython.display import display\n",
    "    \n",
    "    if feature_names is None:\n",
    "        feature_names=X_train.columns\n",
    "\n",
    "    tree_viz_kws =  dict(out_file=None,rounded=True, rotate=False, filled = True)\n",
    "    tree_viz_kws.update(kws)\n",
    "\n",
    "    # tree.export_graphviz(dt) #if you wish to save the output to a dot file instead\n",
    "    tree_data=export_graphviz(tree,feature_names=feature_names, \n",
    "                                   class_names=class_names,**tree_viz_kws)\n",
    "    graph = graphviz.Source(tree_data,format=format_)#'png')\n",
    "    \n",
    "    if save_filename is not None:\n",
    "        graph.render(save_filename)\n",
    "        if save_and_show:\n",
    "            display(graph)\n",
    "        else:\n",
    "            print(f'[i] Tree saved as {save_filename}.{format_}')\n",
    "    else:\n",
    "        display(graph)\n",
    "\n",
    "#     display(SVG(graph.pipe(format=format_)))#'svg')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-16T05:01:50.133Z"
    }
   },
   "outputs": [],
   "source": [
    "df = fs.datasets.load_iowa_prisoners(vers='clean',read_csv_kwds={'index_col':0})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-16T05:01:50.135Z"
    }
   },
   "outputs": [],
   "source": [
    "## Drop unwanted cols\n",
    "df= df.drop(columns=['yr_released','report_year'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-16T05:01:50.138Z"
    }
   },
   "outputs": [],
   "source": [
    "## Check null values\n",
    "import missingno\n",
    "missingno.matrix(df)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-16T05:01:50.140Z"
    }
   },
   "outputs": [],
   "source": [
    "df['race_ethnicity'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-16T05:01:50.142Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining Dictionary Map for race_ethnicity categories\n",
    "race_ethnicity_map = {'White - Non-Hispanic':'White',\n",
    "                        'Black - Non-Hispanic': 'Black',\n",
    "                        'White - Hispanic' : 'Hispanic',\n",
    "                        'American Indian or Alaska Native - Non-Hispanic' : 'American Native',\n",
    "                        'Asian or Pacific Islander - Non-Hispanic' : 'Asian or Pacific Islander',\n",
    "                        'Black - Hispanic' : 'Black',\n",
    "                        'American Indian or Alaska Native - Hispanic':'American Native',\n",
    "                        'White -' : 'White',\n",
    "                        'Asian or Pacific Islander - Hispanic' : 'Asian or Pacific Islander',\n",
    "                        'N/A -' : np.nan,\n",
    "                        'Black -':'Black'}\n",
    "\n",
    "df['race_ethnicity'] = df['race_ethnicity'].map(race_ethnicity_map)\n",
    "df['race_ethnicity'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-16T05:01:50.144Z"
    }
   },
   "outputs": [],
   "source": [
    "df['crime_class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-16T05:01:50.147Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remapping\n",
    "crime_class_map = {'Other Felony (Old Code)': np.nan ,#or other felony\n",
    "                  'Other Misdemeanor':np.nan,\n",
    "                   'Felony - Mandatory Minimum':np.nan, # if minimum then lowest sentence ==  D Felony\n",
    "                   'Special Sentence 2005': 'Sex Offender',\n",
    "                   'Other Felony' : np.nan ,\n",
    "                   'Sexual Predator Community Supervision' : 'Sex Offender',\n",
    "                   'D Felony': 'D Felony',\n",
    "                   'C Felony' :'C Felony',\n",
    "                   'B Felony' : 'B Felony',\n",
    "                   'A Felony' : 'A Felony',\n",
    "                   'Aggravated Misdemeanor':'Aggravated Misdemeanor',\n",
    "                   'Felony - Enhancement to Original Penalty':'Felony - Enhanced',\n",
    "                   'Felony - Enhanced':'Felony - Enhanced' ,\n",
    "                   'Serious Misdemeanor':'Serious Misdemeanor',\n",
    "                   'Simple Misdemeanor':'Simple Misdemeanor'}\n",
    "\n",
    "df['crime_class'] = df['crime_class'].map(crime_class_map)\n",
    "df['crime_class'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-16T05:01:50.149Z"
    }
   },
   "outputs": [],
   "source": [
    "df['age_released'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-16T05:01:50.151Z"
    }
   },
   "outputs": [],
   "source": [
    "# Encoding age groups as ordinal\n",
    "age_ranges = ('Under 25','25-34', '35-44','45-54','55 and Older')\n",
    "age_codes = (0,1,2,3,4) \n",
    "# Zipping into Dictionary to Map onto Column\n",
    "age_map = dict(zip(age_ranges,age_codes))\n",
    "age_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-16T05:01:50.165Z"
    }
   },
   "outputs": [],
   "source": [
    "df['age_enc'] = df['age_released'].map(age_map)\n",
    "df['age_enc'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-16T05:01:50.156Z"
    }
   },
   "outputs": [],
   "source": [
    "# Mapping age_map onto 'age_released'\n",
    "# Encoding age groups as ordinal\n",
    "age_ranges = ('Under 25','25-34', '35-44','45-54','55 and Older')\n",
    "age_numbers = (20,30,40,50,70) \n",
    "age_num_map = dict(zip(age_ranges,age_numbers))\n",
    "age_num_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-16T05:01:50.158Z"
    }
   },
   "outputs": [],
   "source": [
    "df['age_number'] = df['age_released'].map(age_num_map)\n",
    "df['age_number'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-16T05:01:50.160Z"
    }
   },
   "outputs": [],
   "source": [
    "## Drop Nulls \n",
    "print(df.isna().sum().divide(len(df))*100)\n",
    "drop_cols  = [col for col in df.drop(columns=['super_dist','release_type']).columns]\n",
    "drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-16T05:01:50.163Z"
    }
   },
   "outputs": [],
   "source": [
    "df.dropna(subset=drop_cols, inplace=True)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-16T05:01:50.165Z"
    }
   },
   "outputs": [],
   "source": [
    "df[['super_dist','release_type']] = df[['super_dist','release_type']].fillna('Missing')\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-16T05:01:50.167Z"
    }
   },
   "outputs": [],
   "source": [
    "drop_cols = ['age_released']\n",
    "df = df.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-16T05:01:50.169Z"
    }
   },
   "outputs": [],
   "source": [
    "# one_hot_cols = ['race_ethnicity','crime_class\n",
    "df = pd.get_dummies(df)#,dummy_na=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-16T05:01:50.172Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop('recidivist_No',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-16T05:01:50.174Z"
    }
   },
   "outputs": [],
   "source": [
    "y = df.pop('recidivist_Yes')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-16T05:01:50.176Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Train test split\n",
    "X_train, X_test, y_train,y_test  = train_test_split(df,y,test_size=.3)\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-16T05:01:50.178Z"
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE()\n",
    "X_train, y_train = smote.fit_sample(X_train, y_train)\n",
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-16T05:01:50.182Z"
    }
   },
   "outputs": [],
   "source": [
    "# mpl.rcParams['figure.figsize'] =(20,20)\n",
    "# pd.plotting.scatter_matrix(df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-16T05:01:50.185Z"
    }
   },
   "outputs": [],
   "source": [
    "# px.scatter_matrix(df, color='recidivist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-16T05:01:50.187Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pd.plotting.scatter_matrix(df,diagonal='kde');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-16T05:01:50.190Z"
    }
   },
   "outputs": [],
   "source": [
    "# px.scatter_matrix(df,color='recidivist',\n",
    "#                  dimensions=['race_ethnicity','super_dist','age_released',\n",
    "#                             'target_pop','crime_class','crime_class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForests Revisited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-16T05:01:50.193Z"
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRFClassifier,XGBClassifier\n",
    "## Fit and Evaluate\n",
    "\n",
    "xgb_rf = XGBRFClassifier()\n",
    "xgb_rf.fit(X_train, y_train)\n",
    "\n",
    "print(xgb_rf.score(X_train,y_train))\n",
    "print(xgb_rf.score(X_test,y_test))\n",
    "\n",
    "y_hat_test = xgb_rf.predict(X_test)\n",
    "\n",
    "evaluate_model(y_test,y_hat_test,X_test,xgb_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-16T05:01:50.195Z"
    }
   },
   "outputs": [],
   "source": [
    "importance = pd.Series(xgb_rf.feature_importances_,index=X_train.columns).sort_values(ascending=False)\n",
    "display(importance.head(20))\n",
    "top_cols = list(importance.head(20).index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-16T05:01:50.198Z"
    }
   },
   "outputs": [],
   "source": [
    "import os,glob,sys\n",
    "folder ='../py_files/'\n",
    "sys.path.append(os.path.abspath(folder))\n",
    "import mod_5_functions as m5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-16T05:01:50.200Z"
    }
   },
   "outputs": [],
   "source": [
    "# X_train[top_cols]\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-16T05:01:50.202Z"
    }
   },
   "outputs": [],
   "source": [
    "# np.logspace(0.01, 10,0.1)\n",
    "np.logspace(-1,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T05:01:38.831883Z",
     "start_time": "2020-04-16T05:01:38.828300Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC,LinearSVC,NuSVC\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "pipe = Pipeline([('sca',MinMaxScaler()),\n",
    "                ('clf',SVC(probability=True))])\n",
    "\n",
    "\n",
    "# Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "param_grid = {\n",
    "    'sca__feature_range': [(0,1)],#,(-1,1)],\n",
    "    'clf__C':[0.1,0.5,10],#list( np.logspace(-2,1,2)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-16T05:01:53.720Z"
    }
   },
   "outputs": [],
   "source": [
    "timer=Timer()\n",
    "timer.start()\n",
    "search = RandomizedSearchCV(pipe, param_grid,n_iter=2)#, n_jobs=-1)\n",
    "search.fit(X_train, y_train)\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)\n",
    "timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T03:41:52.080298Z",
     "start_time": "2020-04-16T03:41:52.077603Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "svc = SVC(probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T03:45:58.105927Z",
     "start_time": "2020-04-16T03:44:09.861531Z"
    }
   },
   "outputs": [],
   "source": [
    "timer = Timer()\n",
    "timer.start()\n",
    "svc.fit(X_train[top_cols],y_train)\n",
    "\n",
    "print(svc.score(X_test[top_cols],y_test))\n",
    "timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T03:49:12.962657Z",
     "start_time": "2020-04-16T03:49:01.198355Z"
    }
   },
   "outputs": [],
   "source": [
    "y_hat_test = svc.predict(X_test[top_cols])\n",
    "evaluate_model(y_test,y_hat_test,X_test[top_cols], svc)#,conf_matrix_kws={'normalize':True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn has a class called [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) that is very logical and versatile. We can break up the steps within a full process. But it'll help if we define what the different parts are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is any object in the pipeline that can can take in data and *estimate* (or **learn**) some parameters. \n",
    "\n",
    "This means regression and classification models are estimators but so are objects that transform the original dataset ([Transformers](pipeline_intro.ipynb#Transformer)) such as a standard scaling.\n",
    "\n",
    "### Usage (Methods)\n",
    "\n",
    "#### `fit`\n",
    "\n",
    "All estimators estimate/learn by calling the `fit()` method by passing in the dataset. Other parameters can be passed in to \"help\" the estimator to learn. These are called **hyperparameters**, parameters used to tweak the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some estimators can change the original data to something new, a **transformation**. You can think of examples of these **transformers** when you do scaling, data cleaning, or expanding/reducing on a dataset.\n",
    "\n",
    "### Usage (Methods)\n",
    "\n",
    "#### `transform`\n",
    "\n",
    "Transformers will call the `transform()` method to apply the transformation to a dataset.\n",
    "\n",
    "####  `fit_transform`\n",
    "\n",
    "Remember that all estimators have a `fit()` method, so a transformer can use the `fit()` method to learn something about the given dataset. After learning with `fit()`, a transformation on the dataset can be made with the `transform()` method. \n",
    "\n",
    "An example of this would be a function that performs normalization on the dataset; the `fit()` method would learn the minimum and maximum of the dataset and the `transform()` method will scale the dataset.\n",
    "\n",
    "When you call `fit` and `transform` with the same dataset, you can simply call the `fit_transform()` method. This essentially has the same results as calling `fit()` and then `transform()` on the dataset but possibly with some optimization and efficiencies baked in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've been using **predictors** whenever we've been making predictions with a classifier or regressor. We would use the `fit()` method to train our predictor object and then feed in new data to make predictions (based on what it learned in the fitting stage).\n",
    "\n",
    "### Usage (Methods)\n",
    "\n",
    "#### `predict`\n",
    "\n",
    "As you probably can guess, the `predict()` method predicts results from a dataset given to it after being trained with a `fit()` method\n",
    "\n",
    "#### `score`\n",
    "\n",
    "Predictors also have a `score()` method that can be used to evaluate how well the predictor performed on a dataset (such as the test set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Using a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can imagine doing the full steps planned out for a dataset. We _technically_ don't need to use the Pipeline class but it makes it much more manageable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T03:02:47.977051Z",
     "start_time": "2020-04-16T03:02:47.974320Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "param_grid = {\n",
    "    'pca__n_components': [5, 15, 30, 45, 64],\n",
    "    'logistic__C': np.logspace(-4, 4, 4),\n",
    "}\n",
    "search = GridSearchCV(pipe, param_grid, n_jobs=-1)\n",
    "search.fit(X_digits, y_digits)\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T03:02:48.222378Z",
     "start_time": "2020-04-16T03:02:48.171596Z"
    }
   },
   "outputs": [],
   "source": [
    "# Getting some data\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without the Pipeline class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T17:42:03.564354Z",
     "start_time": "2019-11-14T17:42:03.550201Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define transformers (will adjust/massage the data)\n",
    "imputer = SimpleImputer(strategy=\"median\") # replaces missing values\n",
    "std_scaler = StandardScaler() # scales the data\n",
    "pca = PCA()\n",
    "\n",
    "# Define the classifier (predictor) to train\n",
    "rf_clf = RandomForestClassifier()\n",
    "\n",
    "# Have the classifer (and full pipeline) learn/train/fit from the data\n",
    "X_train_filled = imputer.fit_transform(X_train)\n",
    "X_train_scaled = std_scaler.fit_transform(X_train_filled)\n",
    "X_train_reduce = pca.fit_transform(X_train_scaled)\n",
    "rf_clf.fit(X_train_reduce, y_train)\n",
    "\n",
    "# Predict using the trained classifier (still need to do the transformations)\n",
    "X_test_filled = imputer.transform(X_test)\n",
    "X_test_scaled = std_scaler.transform(X_test_filled)\n",
    "X_test_reduce = pca.fit_transform(X_test_scaled)\n",
    "y_pred = rf_clf.predict(X_test_reduce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that if we were to add more steps in this process, we'd have to change both the *training* and *testing* processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With the Pipeline class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T03:02:52.127999Z",
     "start_time": "2020-04-16T03:02:51.970312Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")), \n",
    "        ('std_scaler', StandardScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('rf_clf', RandomForestClassifier()),\n",
    "])\n",
    "\n",
    "\n",
    "# Train the pipeline (tranformations & predictor)\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the pipeline (includes the transfomers & trained predictor)\n",
    "predicted = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T03:03:14.715243Z",
     "start_time": "2020-04-16T03:03:14.712755Z"
    }
   },
   "outputs": [],
   "source": [
    "model = pipeline.named_steps['rf_clf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T03:03:20.683890Z",
     "start_time": "2020-04-16T03:03:19.924378Z"
    }
   },
   "outputs": [],
   "source": [
    "m5.evaluate_model(y_test,predicted,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SHAP and Shapely Values for Model Interpretation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- White Paper on Shapely Values:\n",
    "    - https://arxiv.org/abs/1705.07874\n",
    "    \n",
    "- Blog Posts:\n",
    "    - https://towardsdatascience.com/explain-your-model-with-the-shap-values-bc36aac4de3d\n",
    "\n",
    "    - https://towardsdatascience.com/explain-any-models-with-the-shap-values-use-the-kernelexplainer-79de9464897a\n",
    "\n",
    "\n",
    "- Videos/Talks:\n",
    "    - [\"Open the Black Box: an intro to Model Interpretability with LIME and SHAP](https://youtu.be/C80SQe16Rao)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Uses game theory to explain feature importance and how a feature steered a model's prediction(s) by removing each feature and seeing the effect on the error.\n",
    "\n",
    "- SHAP has:\n",
    "    - `TreeExplainer`:\n",
    "        - compatible with sckit learn, xgboost, Catboost\n",
    "    - `KernelExplainer`:\n",
    "        - compatible with \"any\" model\n",
    "        \n",
    "\n",
    "\n",
    "- See [this blog post](https://towardsdatascience.com/explain-your-model-with-the-shap-values-bc36aac4de3d) for intro to topic and how to use with trees\n",
    "\n",
    "- For non-tree/random forest models [see this follow up post]( https://towardsdatascience.com/explain-any-models-with-the-shap-values-use-the-kernelexplainer-79de9464897a)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Get Expanations for Trees:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Import and initialize javascript:\n",
    "\n",
    "```python\n",
    "import shap \n",
    "shap.initjs()\n",
    "```\n",
    "1. Create a shap explainer using your fit model.\n",
    "\n",
    "```python\n",
    "explainer = shap.TreeExplainer(xgb_clf)\n",
    "```\n",
    "\n",
    "2. Get shapely values from explainer for your training data\n",
    "\n",
    "```python\n",
    "shap_values = explainer.shap_values(X_train,y_train)\n",
    "```            \n",
    "\n",
    "3. Select which type of the available plots you'd like to visualize\n",
    "\n",
    "    \n",
    "- **Types of Plots:**\n",
    "    - `summary_plot()`\n",
    "    - `dependence_plot()`\n",
    "    - `force_plot()` for a given observation\n",
    "    - `force_plot()` for all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "## For normal bar graph of importance:\n",
    "shap.summary_plot(shap_values,X_train,plot_type='bar')\n",
    "\n",
    "## For detail Shapely value visuals:\n",
    "shap.summary_plot(shap_values, X_train)\n",
    "```\n",
    "\n",
    "**`shap.summary_plot`**\n",
    "> - Feature importance: Variables are ranked in descending order.\n",
    "- Impact: The horizontal location shows whether the effect of that value is associated with a higher or lower prediction.\n",
    "- Original value: Color shows whether that variable is high (in red) or low (in blue) for that observation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`shap.dependence_plot`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "## To Auto-Select Feature Most correlated with a specific feature, just pass the desired feature's column name.\n",
    "\n",
    "shap.dependence_plot('super_dist', shap_values, X_train)\n",
    "\n",
    "## There is a way to specifically call out multiple features but I wasn't able to summarize it quickly for this nb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`shap.force_plot`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show an individual data point's prediction and the factors pushing it towards one class or another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "## Just using np to randomly select a row\n",
    "\n",
    "row = np.random.choice(range(len(X_train))\n",
    "                       \n",
    "shap.force_plot(explainer.expected_value, shap_values[row,:], X_train.iloc[row,:])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T02:16:06.915927Z",
     "start_time": "2020-01-22T02:16:06.912246Z"
    }
   },
   "source": [
    "# Let's peak inside this black box\n",
    "[SHAP Values](https://towardsdatascience.com/explain-your-model-with-the-shap-values-bc36aac4de3d)\n",
    "\n",
    "[Explain Any Model with SHAP KernelExplaibner](https://towardsdatascience.com/explain-any-models-with-the-shap-values-use-the-kernelexplainer-79de9464897a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T02:36:21.986852Z",
     "start_time": "2020-04-16T02:36:21.788809Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install -U shap\n",
    "import shap\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T02:42:18.719371Z",
     "start_time": "2020-04-16T02:40:31.735316Z"
    }
   },
   "outputs": [],
   "source": [
    "X_shap = shap.sample(X_test,nsamples=5)\n",
    "explainer = shap.KernelExplainer(svc.predict,X_shap)#.shap_values()\n",
    "shap_values = explainer.shap_values(X_shap)\n",
    "# # # rf_shap_values = shap.KernelExplainer(rf.predict,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T02:42:55.627958Z",
     "start_time": "2020-04-16T02:42:55.254340Z"
    }
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values,X_shap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T02:42:55.639878Z",
     "start_time": "2020-04-16T02:42:55.629410Z"
    }
   },
   "outputs": [],
   "source": [
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T02:42:55.913366Z",
     "start_time": "2020-04-16T02:42:55.902018Z"
    }
   },
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value, shap_values, X_shap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T02:44:17.615049Z",
     "start_time": "2020-04-16T02:44:17.606697Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value, shap_values, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Using a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Check out Aurélien Geron's notebook of an [end-to-end ml project](https://github.com/ageron/handson-ml2/blob/master/02_end_to_end_machine_learning_project.ipynb) on his GitHub repo based around his book [_Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems (2nd ed)_](https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "279px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "3176a925b1da4812b6b9a08eaa9a47e5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "6cdfcbcf7d9e46788a093ca3e89884cf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7f45943170bc4076966bdad19d1993bb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "907fbf82d8fd4b0aa4bd1e20f61cd86c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "b1e379b187b447b6a38deec60ef877de": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b2c21f049d1c447ab6bdb2eb160fc01c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_b1e379b187b447b6a38deec60ef877de",
       "style": "IPY_MODEL_907fbf82d8fd4b0aa4bd1e20f61cd86c",
       "value": " 0/6497 [00:00&lt;?, ?it/s]"
      }
     },
     "b83511e5360247068a26da1552bce1b5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_fca5f875a10349d59c154faa80e49698",
        "IPY_MODEL_b2c21f049d1c447ab6bdb2eb160fc01c"
       ],
       "layout": "IPY_MODEL_7f45943170bc4076966bdad19d1993bb"
      }
     },
     "fca5f875a10349d59c154faa80e49698": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "description": "  0%",
       "layout": "IPY_MODEL_6cdfcbcf7d9e46788a093ca3e89884cf",
       "max": 6497,
       "style": "IPY_MODEL_3176a925b1da4812b6b9a08eaa9a47e5"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
