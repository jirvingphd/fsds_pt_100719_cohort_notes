{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sect 43: Operationalizing Code & AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- online-ds-ft-100719\n",
    "- 04/07/20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Udemy Course: Deployment of Machine Learning Models](https://www.udemy.com/share/101Y5KAEYbdVdWRXQ=/)\n",
    "- [Amazon Web Services](https://aws.amazon.com/)\n",
    "    - [Getting Started Resource Center](https://aws.amazon.com/getting-started/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Productionizing Models as a Career Skill\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Many data scientists don't know how to put machine learning models into production.  \n",
    "2. Putting a model into production is a mandatory skill for data scientists at most small to medium-sized companies.\n",
    "3. Being able to productionize models will make you a much more attractive candidate to employers, and give you a competitive advantage!\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-students/dsc-data-science-and-machine-learning-engineering-online-ds-ft-100719/master/images/new-venn-diagram.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  A decade ago, productionizing a machine learning model would have meant building your own web server with something like [Flask](http://flask.pocoo.org/) or [Django](https://www.djangoproject.com/) and hosting somewhere, just like you would with any web app. \n",
    "- Now, we don't even need to worry about things like server code -- instead, we can use preexisting services from AWS that were created specifically to simplify the process of productionizing machine learning solutions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Eco System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/fsds_100719_cohort_notes/master/images/awscloud.png\">\n",
    "\n",
    "> * AWS is a **_Cloud-Computing Platform_** which we can use for a variety of use cases in data science.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **To Manage your account use the AWS Console**\n",
    "    - http://console.aws.amazon.com/\n",
    "    - Sign into console using Root User\n",
    "    \n",
    "- **Resources**\n",
    "    - https://aws.amazon.com/getting-started/\n",
    "\n",
    "- **AWS Components:**\n",
    "    - S3: storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sign up for AWS\n",
    "- [Follow Learn lesson steps to set up account](https://learn.co/tracks/data-science-career-v2/module-6-natural-language-processing-and-deep-learning/section-50-operationalizing-code-and-aws/the-aws-ecosystem)\n",
    "\n",
    "- [Amazon Web Services](https://aws.amazon.com/)\n",
    "\n",
    "AWS has data centers all over the world, and they are **not** interchangeable when it comes to your projects. Click on the \"Region\" tab in the top right corner of the navigation bar, and you should see a dropdown of all the different data centers you can choose from. It is **_very important_** that you always choose the same region to connect to with your projects.\n",
    "\n",
    "- Create an AWS account\n",
    "- Sign into console using Root User"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***SageMaker is a platform created by Amazon to centralize all the various services related to Data Science and Machine Learning. If you're a data scientist working on AWS, chances are that you'll be spending most (if not all) of your time in SageMaker getting things done.***\n",
    "\n",
    "\n",
    "> * Amazon has centralized all of the major data science services inside **_Amazon SageMaker_**. SageMaker provides numerous services for things such as:\n",
    "    * Data Labeling\n",
    "    * Cloud-based Notebooks\n",
    "    * Training and Model Tuning\n",
    "    * Inference\n",
    "    \n",
    "#### SageMaker Components\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-students/dsc-introduction-to-aws-sagemaker-online-ds-ft-100719/master/images/use_cases.png\">\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Productionizing Models with SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **SAVE THESE 2 RESOURCES:**\n",
    "    - [Learn Lesson: Productionizing Models with SageMaker](https://learn.co/tracks/data-science-career-v2/module-6-natural-language-processing-and-deep-learning/section-50-operationalizing-code-and-aws/productionizing-models-with-sagemaker)\n",
    "        - [Lesson Repo](https://github.com/learn-co-students/dsc-productionizing-models-with-sagemaker-online-ds-ft-100719)\n",
    "\n",
    "    - [Official SageMaker Tutorial](https://github.com/aws-samples/amazon-sagemaker-keras-text-classification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When productionizing a machine learning model using AWS, you'll typically use the following workflow:\n",
    "\n",
    "1. Explore and preprocess data\n",
    "2. Build SageMaker container (Docker)\n",
    "3. Test training and inference code on your local machine \n",
    "4. Train and deploy model with SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps for Deploying A Model (from lesson)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Complete AWS training notebooks\n",
    "\n",
    "- https://github.com/aws-samples/amazon-sagemaker-keras-text-classification\n",
    "- Do labs 1,2,& 3\n",
    "    - These cover all of the set up required\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Build and Register the container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In Learn lesson repo, there is a `container` folder that has Docker images\n",
    "\n",
    "- The Code Below uses the container folder contens to create and register the docker image needed on AWS.\n",
    "- It is best to \"upload this notebook into your notebook to your AWS Jupyter environment\n",
    "\n",
    "\n",
    "\n",
    "\"After you have successfully uploaded this notebook, if you are asked to choose a kernel, use the same kernel which runs the `sagemaker_keras_text_classification.ipynb` notebook. \n",
    "\n",
    "\n",
    "> NOTE: If you deactivated this process (stopped your Jupyter instance, like _Step 8_ below) and then came back to continue, you'll need to go back and start from [Lab 2](https://github.com/aws-samples/amazon-sagemaker-keras-text-classification#lab-2-building-the-sagemaker-tensorflow-container), because you need the Docker instance running in order to run the following cells. \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Code for Building/Registering containers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```ython\n",
    "%%sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=sagemaker-keras-text-classification\n",
    "\n",
    "cd container\n",
    "\n",
    "chmod +x sagemaker_keras_text_classification/train\n",
    "chmod +x sagemaker_keras_text_classification/serve\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "# On a SageMaker Notebook Instance, the docker daemon may need to be restarted in order\n",
    "# to detect your network configuration correctly.  (This is a known issue.)\n",
    "if [ -d \"/home/ec2-user/SageMaker\" ]; then\n",
    "  sudo service docker restart\n",
    "fi\n",
    "\n",
    "docker build  -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Setting up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Once we've created the container, we'll need to set up the environment. The cell below contains more boilerplate code, which is used to handle a couple sticking points in order to set up the environment. \n",
    "\n",
    "\n",
    "```python \n",
    "# S3 prefix\n",
    "prefix = 'sagemaker-keras-text-classification'\n",
    "\n",
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Creating the Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created the container and set up our environment, the next step is to create a SageMaker session. \n",
    "```python\n",
    "import sagemaker as sage\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sess = sage.Session()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Upload the Data for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps 4 and 5 are where you'll add the code unique to your project.In this step, make sure have a folder called `'data'` that contains the data you'll be working with. The actual structure of the data is up to you, as you'll be the one consuming it to train your model in step 5. \n",
    "\n",
    "```python \n",
    "WORK_DIRECTORY = 'data'\n",
    "\n",
    "data_location = sess.upload_data(WORK_DIRECTORY, key_prefix=prefix)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Fitting the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is the part where you'll do the brunt of the work. \n",
    "- You'll train your own model on the data you uploaded in the previous step. \n",
    "\n",
    "- Note that in the sample code below, the first 3 lines are boilerplate code. \n",
    "    \n",
    "- The actual creation and training of the model happen on the last two lines of code, where `tree` is instantiated and used. \n",
    "\n",
    "> **_NOTE_**: You may have noticed that the code in the cell below uses an `Estimator` from `sage` (which is just an alias we set for `sagemaker` up above), the SageMaker library for python, rather than a model from scikit-learn. The `sagemaker` library contains a massive amount of useful models that we can use directly. Under the hood, the `sagemaker` library wraps in the same open-source frameworks such as scikit-learn, Keras, and TensorFlow that you're used to using. The code below is an example from AWS of how to use one of their `Estimator` objects for training. If you read the output of the cell when you run everything, you'll notice that much of it is warning messages or other printouts from sklearn and keras!\n",
    "\n",
    "For more information on the models and other tools included in the aws sagemaker library, check out [Amazon SageMaker Python SDK Documentation](https://sagemaker.readthedocs.io/en/stable/)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = sess.boto_session.region_name\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/sagemaker-keras-text-classification'.format(account, region)\n",
    "\n",
    "tree = sage.estimator.Estimator(image,\n",
    "                       role, 1, 'ml.c5.2xlarge',\n",
    "                       output_path=\"s3://{}/output\".format(sess.default_bucket()),\n",
    "                       sagemaker_session=sess)\n",
    "\n",
    "tree.fit(data_location)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Deploying the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This is where the magic happens -- we have a trained model, and now we need to actually **_deploy_** it to the AWS cloud! Notice how during this step, we include a `json_serializer` -- this is so that the model can serialize and deserialize data as needed when taking data in as input. \n",
    "\n",
    "Running the cell below will create an endpoint for your trained model. \n",
    "\n",
    "```python\n",
    "from sagemaker.predictor import json_serializer\n",
    "predictor = tree.deploy(1, 'ml.t2.medium', serializer=json_serializer)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Cleanup (IMPORTANT!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As a final step for this exercise, **be sure to run the following line of code to delete your endpoint!** Although you are running this lab on the free tier, you don't want to leave it running, because that is how costs can accrue. Run the cell below to delete your endpoint.\n",
    "\n",
    "```python \n",
    "sess.delete_endpoint(predictor.endpoint)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Deactivate Everything in AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In AWS, you pay for usage. **This means that anything left running is being used.  While the AWS Free Tier we've signed up for allows us to do small things for free for prototyping or learning, leaving some things running may take us past the usage limits for the AWS Free Tier.** In order to avoid getting charged, you'll need to do the following steps: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 8.1: Deactivate the notebook in Sagemaker\n",
    "\n",
    "First, you'll need to deactivate your notebook in SageMaker. When you enter the SageMaker platform, you'll always see the number of open notebooks you have up and running highlighted in green under the 'Recent Activity' section. \n",
    "\n",
    "<img src='https://raw.githubusercontent.com/jirvingphd/dsc-productionizing-models-with-sagemaker-online-ds-ft-100719/master/images/create-notebook-7.png'>\n",
    "\n",
    "\n",
    "To deactivate a running notebook, select it and then go to the 'Actions' tab and select stop. Stopping the notebook instance will take a minute or two. You'll know it's done when you see the 'Status' column for the highlighted notebook change from 'InService' to 'Stopped'. \n",
    "\n",
    "<img src='https://raw.githubusercontent.com/jirvingphd/dsc-productionizing-models-with-sagemaker-online-ds-ft-100719/master/images/create-notebook-8.png'>\n",
    "\n",
    "### 8.2: Keep an Eye on Cost Explorer\n",
    "\n",
    "As you've seen from this lab, getting a handle on all the different services in AWS and how they interact with one another can be a bit daunting until you have some experience. It's very important that you don't leave services running when you aren't using them, because you will be charged for that. If you want to make sure that you haven't left anything running, the easiest thing to do is to check the 'Costs Explorer' page inside AWS. You can find this by searching for 'AWS Cost Explorer' in the search bar on the main page for the AWS Console. This service will show you what your usage is for everything that you can be charged for. It's quite intuitive and easy to use, and should make it easy to see if you are accruing charges because you left something running that you didn't realize. If you left something running that you aren't aware of, you'll see it here -- once you've noticed it, just navigate to the service in question and deactivate it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
