{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 29: Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- online-ds-pt-100719\n",
    "- 04/08/20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANNOUNCEMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Thursday's Study Group -  need to shift to Friday \n",
    "- We will have our study group on Support Vector Machines on Saturday 04/11/20 at 3 PM EST\n",
    "    - https://learn.co/study-groups/sect-31-support-vector-machines\n",
    "\n",
    "- Next week, we have to shift our Tuesday (04/14/20) project-kickoff study group to either Monday or Wednesday (preference?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions/Comments?:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data Partitioning Activity\n",
    "- Decision Tree Visual Walkthrough\n",
    "- Entropy and Information Gain\n",
    "- [Hyperparamtere Tuning and Pruning Decision Trees](https://learn.co/tracks/data-science-career-v2/module-5-machine-learning-and-big-data/section-34-decision-trees/hyperparameter-tuning-and-pruning-in-decision-trees-lab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Data Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T22:45:45.562859Z",
     "start_time": "2020-04-08T22:45:38.433408Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U fsds_100719\n",
    "from fsds_100719.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T22:45:45.626304Z",
     "start_time": "2020-04-08T22:45:45.564439Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "# plt.rcParams['figure.figsize'] = (10,5)\n",
    "np.random.seed(27)\n",
    "plt.style.use('seaborn-talk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T22:45:45.642979Z",
     "start_time": "2020-04-08T22:45:45.629735Z"
    },
    "code_folding": [
     0,
     36
    ]
   },
   "outputs": [],
   "source": [
    "def helper_create_plot(n=300):\n",
    "    '''\n",
    "    Create a plot to practice how a decision tree makes its cuts/decisions.\n",
    "    '''\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(n):\n",
    "        # Generate a random number\n",
    "        nx = np.random.random()*10\n",
    "        ny = np.random.random()*10\n",
    "        X.append((nx,ny))\n",
    "\n",
    "        if nx > 5:\n",
    "            if ny > 1:\n",
    "                y.append(0)\n",
    "            else:\n",
    "                y.append(1)\n",
    "        else:\n",
    "            if ny > 7:\n",
    "                y.append(0)\n",
    "            else:\n",
    "                y.append(1)\n",
    "\n",
    "    X = np.array(X)\n",
    "\n",
    "    f, ax = plt.subplots(1)\n",
    "\n",
    "    ax.scatter(X[:,0], X[:,1], c=y, s=20, cmap='Set1');\n",
    "    plt.xticks(range(11));\n",
    "    plt.xlabel('X1');\n",
    "    plt.yticks(range(11));\n",
    "    plt.ylabel('X2');\n",
    "    \n",
    "    return f, ax\n",
    "\n",
    "def create_line(ax, direction, threshold, x_range=(0,10), y_range=(0,10), color='blue'):\n",
    "    '''\n",
    "    Creates a vertical or horizontal cut at threshold\n",
    "    '''\n",
    "    if direction == 'vertical':\n",
    "        cut = lambda t: ax.vlines(t,y_range[0], y_range[1], colors=color)\n",
    "    elif direction == 'horizontal':\n",
    "        cut = lambda t: ax.hlines(t,x_range[0], x_range[1], colors=color)\n",
    "    else:\n",
    "        print('Direction does not exist')\n",
    "        return\n",
    "    \n",
    "    cut(threshold)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T22:45:45.855247Z",
     "start_time": "2020-04-08T22:45:45.645051Z"
    }
   },
   "outputs": [],
   "source": [
    "f,x = helper_create_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Looking at the example above, would a **vertical** or a **horizontal** cut better split the classes?\n",
    "\n",
    "Also, what threshold should we use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T22:45:45.858544Z",
     "start_time": "2020-04-08T22:45:45.856437Z"
    }
   },
   "outputs": [],
   "source": [
    "# 'horizontal' or 'vertical'\n",
    "q1_direction = None\n",
    "# Between 0 and 10\n",
    "q1_threshold = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T22:45:46.068799Z",
     "start_time": "2020-04-08T22:45:45.860088Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test your answer by running this cell\n",
    "f,ax = helper_create_plot()\n",
    "create_line(ax,q1_direction,q1_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **vertical** cut/line would do the best to split with a threshold at about **5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T22:45:46.261215Z",
     "start_time": "2020-04-08T22:45:46.070048Z"
    }
   },
   "outputs": [],
   "source": [
    "q1_direction = 'vertical'\n",
    "q1_threshold = 5\n",
    "\n",
    "f,ax = helper_create_plot();\n",
    "create_line(ax,q1_direction, q1_threshold);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Splitting further, what would be the next line & threshold to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T22:45:46.264882Z",
     "start_time": "2020-04-08T22:45:46.263012Z"
    }
   },
   "outputs": [],
   "source": [
    "# 'horizontal' or 'vertical'\n",
    "q2_direction = None\n",
    "# Between 0 and 10\n",
    "q2_threshold = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T22:45:46.436744Z",
     "start_time": "2020-04-08T22:45:46.266242Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test your answer by running this cell\n",
    "f,ax = helper_create_plot()\n",
    "create_line(ax,q1_direction, q1_threshold)\n",
    "create_line(ax,q2_direction, q2_threshold, x_range=(0, q1_threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **horizontal** cut/line would do the best to split with a threshold at about **7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T22:45:46.616885Z",
     "start_time": "2020-04-08T22:45:46.437822Z"
    }
   },
   "outputs": [],
   "source": [
    "q2_direction = 'horizontal'\n",
    "q2_threshold = 7\n",
    "\n",
    "f,ax = helper_create_plot()\n",
    "create_line(ax,q1_direction,q1_threshold)\n",
    "create_line(ax,q2_direction, q2_threshold, x_range=(0, q1_threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3:  again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T22:45:46.620464Z",
     "start_time": "2020-04-08T22:45:46.618245Z"
    }
   },
   "outputs": [],
   "source": [
    "# 'horizontal' or 'vertical'\n",
    "q3_direction = None\n",
    "# Between 0 and 10\n",
    "q3_threshold = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T22:45:46.808426Z",
     "start_time": "2020-04-08T22:45:46.622050Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test your answer by running this cell\n",
    "f,ax = helper_create_plot()\n",
    "create_line(ax, q1_direction, q1_threshold)\n",
    "create_line(ax, q2_direction, q2_threshold, x_range=(0, q1_threshold))\n",
    "create_line(ax, q3_direction, q3_threshold, x_range=(q1_threshold, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **horizontal** cut/line would do the best to split with a threshold at about **1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T22:45:46.978518Z",
     "start_time": "2020-04-08T22:45:46.809536Z"
    }
   },
   "outputs": [],
   "source": [
    "q3_direction = 'horizontal'\n",
    "q3_threshold = 1\n",
    "\n",
    "f,ax = helper_create_plot()\n",
    "create_line(ax, q1_direction, q1_threshold)\n",
    "create_line(ax, q2_direction, q2_threshold, x_range=(0, q1_threshold))\n",
    "create_line(ax, q3_direction, q3_threshold, x_range=(q1_threshold, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Supervised Learning\n",
    "- Classification OR Regression\n",
    "\n",
    "- Interactive Visual Demonstration:\n",
    "    - http://www.r2d3.us/visual-intro-to-machine-learning-part-1/\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/fsds_100719_cohort_notes/master/images/ex-decision-tree.png\" width=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **ADVANTAGES**\n",
    "    - Interpretability \n",
    "    - Render feature importance\n",
    "    - Less data pre-processing needed\n",
    "    \n",
    "- **DISADVANTAGES**\n",
    "    - Do not predict a continuous output (for regression)\n",
    "    - Does not predict beyond range of the training data\n",
    "    - Overfits easily\n",
    "\n",
    "<!---<img src=\"https://raw.githubusercontent.com/learn-co-students/dsc-intro-to-supervised-learning-online-ds-ft-100719/master/images/new_ml-hierarchy.png\" width=60%>--->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direction Acyclic Graphs (DAG) Definition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Direction Acyclic Graph**\n",
    "> A decision tree is a DAG type of classifier where each branch node represents a choice between a number of alternatives and each leaf node represents a classification. An unknown (or test) instance is routed down the tree according to the values of the attributes in the successive nodes. When the instance reaches a leaf, it is classified according to the label assigned to the corresponded leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://raw.githubusercontent.com/jirvingphd/dsc-introduction-to-decision-trees-online-ds-pt-100719/master/images/dt1.png' width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.r2d3.us/visual-intro-to-machine-learning-part-1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. There are features and a target (either class or value)\n",
    "\n",
    "\n",
    "2. Train the tree to make a *decision* (a split) about which feature best separates the data, based on some *metric* \n",
    "    - Data are split into partitions/branches\n",
    "    - Metrics include 'Gini Index', 'Information Gain'\n",
    "    \n",
    "    \n",
    "3. Continue growing each branch of the tree until a stopping criterion is reached.\n",
    "\n",
    "\n",
    "4. Keep doing that until a **stopping condition** is hit.\n",
    "    - Number of data points in a final partition\n",
    "    - Layers deep\n",
    "    \n",
    "5. Test the trees decisions using previously unseen data.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-students/dsc-introduction-to-decision-trees-online-ds-ft-100719/master/images/dt3.png\" width=65%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this procedure, all the features are considered and different split points are tried and tested using some __cost function__. The split with the lowest cost is selected. \n",
    "\n",
    "There are a couple of algorithms used to build a decision tree:\n",
    "\n",
    "* __CART (Classification and Regression Trees)__ uses the Gini Index as a metric\n",
    "* __ID3 (Iterative Dichotomiser 3)__ uses the entropy function and information gain as metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**We need to determine the attribute that __best__ classifies the training data, and use this attribute at the root of the tree.** At each node, we repeat this process creating further splits, until a leaf node is achieved, i.e., all data gets classified.  \n",
    "\n",
    "> ***This means we are performing a top-down, greedy search through the space of possible decision trees***\n",
    "\n",
    "- For ID3 classification trees, we use the \"information gain\" criteria.  \n",
    "    - Information gain (IG) measures how much \"information\" a feature gives us about the class. \n",
    "\n",
    "\n",
    "- Decision trees always try to maximize information gain. So, the attribute with the highest information gain will be split on first.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy and Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy and decision trees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Decision trees aim to tidy the data by separating the samples and re-grouping them in the classes they belong to.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-students/dsc-entropy-and-information-gain-online-ds-ft-100719/master/images/split_fs.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Shannon's Entropy\n",
    "\n",
    "> __Entropy is a measure of disorder or uncertainty.__\n",
    "> \n",
    "> The entropy of a variable is the \"amount of information\" contained in the variable. \n",
    ">\n",
    "> - We can informally describe entropy as an indicator of how messy your data is.  A high degree of entropy always reflects \"messed-up\" data with low/no information content. \n",
    "\n",
    "$$\\large H(S) = -\\sum (P_i . log_2(P_i))$$\n",
    "\n",
    "When  $H(S) = 0$, this means that the set $S$ is perfectly classified, meaning that there is no disorganization in our data because all of our data in S is the exact same class. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO https://learn.co/tracks/module-3-data-science-career-2-1/machine-learning/section-29-decision-trees/entropy-and-information-gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$Information~Gain  = Entropy_{parent} - Entropy_{child}.[child ~weighted ~average]$$\n",
    "\n",
    "- measuring the difference in entropy from before the split (an untidy sock drawer) to after the split (a group of white socks and underwear, and a group of non-white socks and underwear).\n",
    "\n",
    "- Information gain allows us to put a number to exactly how much we've reduced our _uncertainty_ after splitting a dataset $S$ on some attribute, $A$.  The equation for information gain is:\n",
    "\n",
    "$$\\large IG(A, S) = H(S) - \\sum{}{p(t)H(t)}  $$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $H(S)$ is the entropy of set $S$\n",
    "* $t$ is a subset of the attributes contained in $A$ (we represent all subsets $t$ as $T$)\n",
    "* $p(t)$ is the proportion of the number of elements in $t$ to the number of elements in $S$\n",
    "* $H(t)$ is the entropy of a given subset $t$ \n",
    "\n",
    "In the ID3 algorithm, we use entropy to calculate information gain, and then pick the attribute with the largest possible information gain to split our data on at each iteration. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T22:45:47.258613Z",
     "start_time": "2020-04-08T22:45:46.979575Z"
    }
   },
   "outputs": [],
   "source": [
    "url=\"https://raw.githubusercontent.com/jirvingphd/dsc-decision-trees-with-sklearn-codealong-online-ds-pt-100719/master/tennis.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T22:45:47.282345Z",
     "start_time": "2020-04-08T22:45:47.272001Z"
    }
   },
   "outputs": [],
   "source": [
    "df['windy'] = df['windy'].astype(int)\n",
    "df['play'] = df['play'].map({'no':0,'yes':1})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T22:45:47.289883Z",
     "start_time": "2020-04-08T22:45:47.283745Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T22:45:47.301013Z",
     "start_time": "2020-04-08T22:45:47.291236Z"
    }
   },
   "outputs": [],
   "source": [
    "y = df['play'].copy()\n",
    "X =  df.drop(columns=['play']).copy()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T22:45:47.307640Z",
     "start_time": "2020-04-08T22:45:47.302378Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T22:47:38.320297Z",
     "start_time": "2020-04-08T22:47:38.311052Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoder.fit(X_train)\n",
    "\n",
    "X_train_ohe = encoder.transform(X_train).toarray()\n",
    "X_test_ohe = encoder.transform(X_test).toarray()\n",
    "\n",
    "X_train_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T22:45:47.324146Z",
     "start_time": "2020-04-08T22:45:47.320379Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T22:50:35.796648Z",
     "start_time": "2020-04-08T22:50:35.776916Z"
    }
   },
   "outputs": [],
   "source": [
    "def remake_df(X_split, X_df,encoder):\n",
    "    return pd.DataFrame(X_split, columns=encoder.get_feature_names(X_df.columns), index=X_df.index)\n",
    "\n",
    "X_train_ohe = remake_df(X_train_ohe,X_train,encoder)\n",
    "X_test_ohe = remake_df(X_test_ohe,X_test,encoder)\n",
    "\n",
    "display(X_train_ohe.head(),X_test_ohe.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T22:50:36.275989Z",
     "start_time": "2020-04-08T22:50:36.273884Z"
    }
   },
   "outputs": [],
   "source": [
    "# X_test_ohe.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T22:50:36.801605Z",
     "start_time": "2020-04-08T22:50:36.798758Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "from pydotplus import graph_from_dot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T22:50:37.187022Z",
     "start_time": "2020-04-08T22:50:37.169775Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "clf = DecisionTreeClassifier(criterion='entropy')\n",
    "clf.fit(X_train_ohe,y_train)\n",
    "\n",
    "y_hat_test = clf.predict(X_test_ohe)\n",
    "\n",
    "print(metrics.classification_report(y_test,y_hat_test))\n",
    "metrics.plot_confusion_matrix(clf,X_test_ohe,y_test,cmap='Blues',\n",
    "                              normalize='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the decision tree \n",
    "\n",
    "You can see what rules the tree learned by plotting this decision tree. To do this, you need to use additional packages such as `pytdotplus`. \n",
    "\n",
    "> **Note:** If you are run into errors while generating the plot, you probably need to install `python-graphviz` in your machine using `conda install python-graphviz`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T22:53:05.593600Z",
     "start_time": "2020-04-03T22:53:05.087352Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create DOT data\n",
    "dot_data = export_graphviz(clf, out_file=None, \n",
    "                           feature_names=X_train_ohe.columns,  \n",
    "                           class_names=np.unique(y).astype('str'), \n",
    "                           filled=True, rounded=True, special_characters=True)\n",
    "\n",
    "# Draw graph\n",
    "graph = graph_from_dot_data(dot_data)  \n",
    "\n",
    "# Show graph\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T22:53:17.517301Z",
     "start_time": "2020-04-03T22:53:17.512152Z"
    }
   },
   "outputs": [],
   "source": [
    "metrics.accuracy_score(y_test,y_hat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameter Tuning with Trees\n",
    "## TO DO: \n",
    "https://learn.co/tracks/module-3-data-science-career-2-1/machine-learning/section-29-decision-trees/hyperparameter-tuning-and-pruning-in-decision-trees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "- Useful to consider to make sure you don't overfit or underfit\n",
    "\n",
    "Check out the scikit-learn documentation: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `max-depth`\n",
    "- `min_samples_leaf`: The smallest number of samples that can be in a leaf (node)\n",
    "- `min_samples_split`: The smallest number of samples in a leaf (node) before splitting it\n",
    "- `max_features`: Most features to consider when splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEE LABS FROM CLASS FOLDER FOR HYPERPARAMETER TUNING NOTEBOOK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions / Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
