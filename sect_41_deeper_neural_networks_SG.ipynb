{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sect 41:  Deeper Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Study Group: 05/26/20\n",
    "- online-ds-pt-100719"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Review basics from last class about an individual neuron. \n",
    "- Use `repo folder > references  > ANN diagrams.pptx` to talk through inputs and outputs\n",
    "\n",
    "- Revisit bio neural networks PowerPoint with and discuss \"How Neurons Learn\" to review biological inspiration of deeper ANNs\n",
    "    - `repo folder> references > bio_neural_networks.pptx`\n",
    "\n",
    "- **Discuss details about deep neural networks:**\n",
    "    - what makes an ANN \"deep\"?\n",
    "    - what are the different activation functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Announcements/Choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What to cover next week before kicking off the project (and if we want to cover Appdendix'd Sections on Deep NLP and CNNs?\n",
    "    - Save section 43 until post-break?\n",
    "    - Cover section 35 (Big data/pyspark) until post-break?\n",
    "    - Cover recommendation systems  from section 36 (but save pyspark versions until post-break)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Neural Network Terminology from Last Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlvqq-TbP5zO"
   },
   "source": [
    "### Processing within a Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlvqq-TbP5zO"
   },
   "source": [
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-40-02-introduction-to-neural-networks-online-ds-ft-021119/master/figures/log_reg.png\">\n",
    "\n",
    "- **Activation functions** control the output of a neuron.($\\hat y =f_{activation}(x)$ )\n",
    "    - Most basic activation function is sigmoid functin ($\\hat y =\\sigma(x)$)\n",
    "    - Choice of activation function controls the size/range of the output.\n",
    "- **Linear transformations** ( $z = w^T x + b$ ) are used control the output of the activation function .\n",
    "    - where $w^T $ is the weight(/coefficient), $x$ is the input, and  $b$ is a bias. \n",
    "        - weights: \n",
    "        - bias:\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlvqq-TbP5zO"
   },
   "source": [
    "### How Neurons Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlvqq-TbP5zO"
   },
   "source": [
    "- **Loss functions** ($\\mathcal{L}(\\hat y, y) $)  measure inconsistency between predicted ($\\hat y$) and actual $y$\n",
    "    - will be optimized using gradient descent\n",
    "    - defined over 1 traning sample\n",
    "- **Cost functions** takes the average loss over all of the samples.\n",
    "    - $J(w,b) = \\displaystyle\\frac{1}{l}\\displaystyle\\sum^l_{i=1}\\mathcal{L}(\\hat y^{(i)}, y^{(i)})$\n",
    "    - where $l$ is the number of samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlvqq-TbP5zO"
   },
   "source": [
    "#### How weights and biases are updated\n",
    "- **Forward propagation** is the calculating  loss and cost functions.\n",
    "- **Back propagation** involves using gradient descent to update the values for  $w$ and $b$.\n",
    "    - $w := w- \\alpha\\displaystyle \\frac{dJ(w)}{dw}$ <br><br>\n",
    "    - $b := b- \\alpha\\displaystyle \\frac{dJ(b)}{db}$\n",
    "\n",
    "        - where $ \\displaystyle \\frac{dJ(w)}{dw}$ and $\\displaystyle \\frac{dJ(b)}{db}$ represent the *slope* of the function $J$ with respect to $w$ and $b$ respectively\n",
    "        - $\\alpha$ denote the *learning rate*. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "- https://dashee87.github.io/deep%20learning/visualising-activation-functions-in-neural-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T22:19:24.028394Z",
     "start_time": "2020-05-26T22:19:23.444176Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# q\n",
    "def sigmoid(x, derivative=False):\n",
    "    f = 1 / (1 + np.exp(-x))\n",
    "    if (derivative == True):\n",
    "        return f * (1 - f)\n",
    "    return f\n",
    "\n",
    "def tanh(x, derivative=False):\n",
    "    f = np.tanh(x)\n",
    "    if (derivative == True):\n",
    "        return (1 - (f ** 2))\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x, derivative=False):\n",
    "    f = np.zeros(len(x))\n",
    "    if (derivative == True):\n",
    "        for i in range(0, len(x)):\n",
    "            if x[i] > 0:\n",
    "                f[i] = 1  \n",
    "            else:\n",
    "                f[i] = 0\n",
    "        return f\n",
    "    for i in range(0, len(x)):\n",
    "        if x[i] > 0:\n",
    "            f[i] = x[i]  \n",
    "        else:\n",
    "            f[i] = 0\n",
    "    return f\n",
    "\n",
    "def leaky_relu(x, leakage = 0.05, derivative=False):\n",
    "    f = np.zeros(len(x))\n",
    "    if (derivative == True):\n",
    "        for i in range(0, len(x)):\n",
    "            if x[i] > 0:\n",
    "                f[i] = 1  \n",
    "            else:\n",
    "                f[i] = leakage\n",
    "        return f\n",
    "    for i in range(0, len(x)):\n",
    "        if x[i] > 0:\n",
    "            f[i] = x[i]  \n",
    "        else:\n",
    "            f[i] = x[i]* leakage\n",
    "    return f\n",
    "\n",
    "def arctan(x, derivative=False):\n",
    "    if (derivative == True):\n",
    "        return 1/(1+np.square(x))\n",
    "    return np.arctan(x)\n",
    "\n",
    "\n",
    "\n",
    "def plot_activation(fn,return_=True):\n",
    "    z = np.arange(-10, 10, 0.2)\n",
    "    y = fn(z)\n",
    "    dy = fn(z, derivative=True)\n",
    "    \n",
    "    fig,ax=plt.subplots(figsize=(6,4))\n",
    "    ax.set_title(f'{fn.__name__}')\n",
    "    ax.set(xlabel='Input',ylabel='Output')\n",
    "    ax.axhline(color='gray', linewidth=1,)\n",
    "    ax.axvline(color='gray', linewidth=1,)\n",
    "    ax.plot(z, y, 'r', label='original (y)')\n",
    "    ax.plot(z, dy, 'b', label='derivative (dy)')\n",
    "    ax.legend();\n",
    "    \n",
    "    if return_:\n",
    "        return fig\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T22:19:24.034581Z",
     "start_time": "2020-05-26T22:19:24.030170Z"
    }
   },
   "outputs": [],
   "source": [
    "sigmoid.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T22:19:24.964665Z",
     "start_time": "2020-05-26T22:19:24.036271Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "## Plot activation functions\n",
    "activation_functions = {}\n",
    "act_funcs = [sigmoid,tanh,arctan,relu,leaky_relu]\n",
    "\n",
    "for func in act_funcs:\n",
    "    fig = plot_activation(func,return_=True);\n",
    "    activation_functions[func.__name__] = fig\n",
    "clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T22:19:25.117680Z",
     "start_time": "2020-05-26T22:19:24.966398Z"
    }
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "@interact\n",
    "def show_plot(function = list(activation_functions.keys())):\n",
    "    display(activation_functions[function])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sect 41:  Deeper Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisit `bio_neural_networks.pptx`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `repo folder> references > bio_neural_networks.pptx`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/learn-co-students/dsc-image-classification-with-mlps-online-ds-ft-100719/master/images/Deeper_network.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why deeper networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Advantages:**\n",
    "    - largely eliminates need for feature engineering\n",
    "    - multiple levels of information processing in one networking.\n",
    "        - Ex: for images:\n",
    "            - First layer detects edges\n",
    "            - second layer gorups edges and detects patterns\n",
    "            - more layers group even bigger parts together\n",
    "        - Ex: for audio:\n",
    "            - first layer: low level wave features\n",
    "            - second: basic units of sounds (\"phonemes\")\n",
    "            - third: word recognition\n",
    "            - fourth: sentence recognition\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo: Revisit Tensorflow Playground Spiral \n",
    "\n",
    "- [Tensorflow Playground - Spiral Task](https://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=spiral&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.58541&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)\n",
    "    - [Potenial Solution](https://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=spiral&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,5,5,2&seed=0.58541&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many layers/units?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Two schools of thought on how how many layers:**\n",
    "    - Start with a single layer with few neurons\n",
    "        - Add additional, add additional\n",
    "    - Start with a fully fleshed out network that we then prune until we see a dropoff in in performance (then restore the last changes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Visualization of a Convolutional Neural Network - maybe?](https://www.cs.ryerson.ca/~aharley/vis/conv/flat.html)\n",
    "\n",
    "- [Blog post: How many Hidden Layers?](https://towardsdatascience.com/beginners-ask-how-many-hidden-layers-neurons-to-use-in-artificial-neural-networks-51466afa0d3e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math Notation for Deeper Networks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Networks are comprised of sequential layers of neurons/nodes.\n",
    "    - \\# of layers = hidden+output layer\n",
    "        - The input layer is not counted as formal layer.\n",
    "    - All layers except the final are _hidden layers_.\n",
    "    \n",
    "    <img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-40-04-deeper-neural-networks-online-ds-ft-021119/master/figures/small_deeper.png\">\n",
    "    \n",
    "    \n",
    "**The output of a layer $j$ is denoted as $a^{[j]}$.**\n",
    "\n",
    "**For our 2-layer neural network above, this means that:**\n",
    "\n",
    "- $x = a^{[0]}$  as x is what comes out of the input layer\n",
    "- $a^{[1]} = \\begin{bmatrix} a^{[1]}_1  \\\\ a^{[1]}_2 \\\\ a^{[1]}_3  \\\\\\end{bmatrix}$ is the value generated by the hidden layer\n",
    "- $\\hat y =  a^{[2]}$, the output layer will generate a value $a^{[2]}$, which is equal to $\\hat y$.\n",
    "\n",
    "\n",
    "\n",
    "<br>For the **first node** in the hidden layer:\n",
    "- The linear transformation that occurs is:  $ z^{[1]}_1 = w^{[1]}_1 x +b^{[1]}_1$,\n",
    "    - Where $w$ = the weight, and $b$ = bias\n",
    "\n",
    "- For **all nodes** in the hidden layer:\n",
    "    - $ z^{[1]}_1 = w^{[1]}_1 x +b^{[1]}_1$ and  $a^{[1]}_1= f(z^{[1]}_1)$\n",
    "\n",
    "    - $ z^{[1]}_2 = w^{[1]}_2 x +b^{[1]}_2$ and $a^{[1]}_2= f(z^{[1]}_2)$\n",
    "\n",
    "    - $ z^{[1]}_3 = w^{[1]}_3 x +b^{[1]}_3$ and $a^{[1]}_3= f(z^{[1]}_3)$\n",
    "\n",
    "The **dimensions** of the elements:\n",
    "\n",
    "- $w^{[1]} = \\begin{bmatrix} w^{[1]}_{1,1}  & w^{[1]}_{2,1} & w^{[1]}_{3,1}  \\\\ w^{[1]}_{1,2}  & w^{[1]}_{2,2} & w^{[1]}_{3,2}\\end{bmatrix}$\n",
    "    - where, eg. $w^{[1]}_{1,2}$ denotes the weight of the arrow going **from $x_2$ into the first node** of the hidden layer. \n",
    "\n",
    "\n",
    "- When multiplying the transpose of this matrix (making it a 2 x 3 matrix) \n",
    "    - $w^{[1]T}_1$ with $x = \\begin{bmatrix} x_1  \\\\x_2\\end{bmatrix}$ and add $b^{[1]} = \\begin{bmatrix} b^{[1]}_1  \\\\b^{[1]}_2 \\\\ b^{[1]}_3 \\end{bmatrix}$,\n",
    "    - we obtain $z^{[1]} = \\begin{bmatrix} z^{[1]}_1  \\\\z^{[1]}_2 \\\\ z^{[1]}_3 \\end{bmatrix}$.\n",
    "\n",
    "----\n",
    "\n",
    "- The activation function is   $a^{[1]}_1= f(z^{[1]}_1)$.\n",
    "$w^{[1]}_{1,2}$\n",
    "\n",
    "$w^{[1]} = \\begin{bmatrix} w^{[1]}_{1,1}  & w^{[1]}_{2,1} & w^{[1]}_{3,1}  \\\\ w^{[1]}_{1,2}  & w^{[1]}_{2,2} & w^{[1]}_{3,2}\\end{bmatrix}$ \n",
    "\n",
    "[Reminder: $x = \\begin{bmatrix} x_1  \\\\x_2\\end{bmatrix} \\equiv a^{[0]}$ and that $a^{[2]} = \\hat y$ ]\n",
    "\n",
    "- Then, given input $x$:\n",
    "\n",
    "    - $z^{[1]} = w^{[1]T} a^{[0]} + b^{[1]}$\n",
    "\n",
    "    - $a^{[1]} = f(z^{[1]})$\n",
    "\n",
    "    - $z^{[2]} = w^{[2]T} a^{[1]} + b^{[2]}$\n",
    "\n",
    "    - $a^{[2]} = f(z^{[1]})$\n",
    "    \n",
    "    \n",
    "- When adding in several training samples ($i$), these become:\n",
    "    - $z^{[1](i)} = w^{[1]T} a^{[0](i)} + b^{[0]}$\n",
    "\n",
    "    - $a^{[1](i)} = f(z^{[1](i)})$\n",
    "\n",
    "    - $z^{[2](i)} = w^{[2]T} a^{[1](i)} + b^{[2]}$\n",
    "\n",
    "    - $a^{[2](i)} = f(z^{[1](i)})$\n",
    "    \n",
    "<!---    \n",
    "### Process Summary\n",
    "- We begin by defining a model architecture which includes the number of hidden layers, activation functions (sigmoid or relu) and the number of units in each of these.  \n",
    "- We then initialize parameters for each of these layers (typically randomly). After the initial parameters are set, forward propagation evaluates the model giving a prediction, which is then used to evaluate a cost function. Forward propogation involves evaluating each layer and then piping this output into the next layer. \n",
    "- Each layer consists of a linear transformation and an activation function.  The parameters for the linear transformation in **each** layer include $W^l$ and $b^l$. The output of this linear transformation is represented by $Z^l$. This is then fed through the activation function (again, for each layer) giving us an output $A^l$ which is the input for the next layer of the model.  \n",
    "- After forward propogation is completed and the cost function is evaluated, backpropogation is used to calculate gradients of the initial parameters with respect to this cost function. Finally, these gradients are then used in an optimization algorithm, such as gradient descent, to make small adjustments to the parameters and the entire process of forward propogation, back propogation and parameter adjustments is repeated until the modeller is satisfied with the results.--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Summary (Deep Networks Lesson):\n",
    "Notation for when there are $L$ layers present (and $l$ is current layer)\n",
    "\n",
    "**Parameters for the linear transformation: **  \n",
    "\n",
    "$W^{[l]}: (n^{[l]}, n^{[l-1]})$\n",
    "\n",
    "$b^{[l]}: (n^{[l]}, 1)$\n",
    "\n",
    "$dW^{[l]}: (n^{[l]}, n^{[l-1]})$\n",
    "\n",
    "$db^{[l]}: (n^{[l]}, 1)$\n",
    "\n",
    "**Parameters for the activation function**  \n",
    "\n",
    "$ a^{[l]}, z^{[l]}: (n^{[l]}, 1)$\n",
    "\n",
    "$ Z^{[l]}, A^{[l]}: (n^{[l]}, m)$\n",
    "\n",
    "$ dZ^{[l]}, dA^{[l]}: (n^{[l]}, m)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bu64jh4K3oKt"
   },
   "source": [
    "# Basics of Building a Neural Network with Keras:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TNk4Uz27wVZc"
   },
   "source": [
    "**Basics of Building a Neural Network with Keras:**\n",
    "1. **Import required modules**\n",
    "    - **For general neural network**\n",
    "        - `from keras import models, layers,optimizers`\n",
    "    - **For text:**\n",
    "        - `from keras.preprocessing.text import Tokenizer`\n",
    "        - `from keras.utils import to_categorical`\n",
    "    - **For images:**\n",
    "        - `from keras.preprocessing.image import ImageDataGenerator, img_to_array, array_to_img`\n",
    "    - **For relocating image files:**\n",
    "        - `import os, shutil`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TNk4Uz27wVZc"
   },
   "source": [
    "2. **Decide on a network architecture (have only discussed sequential thus far)**\n",
    "    - `model = models.Sequential()`\n",
    "\n",
    "3. **Adding layers - specifying layer type, number of neurons, activation functions, and, optionally, the input shape.**\n",
    "    - `model.add(layers.Dense(units, activation='relu', input_shape))`\n",
    "    - `model.add(layers.Dense(units, activation='relu',input_shape))`\n",
    "    - **3B. Final layer choice:**\n",
    "        - Want to have as many neurons as classes you are trying to predict\n",
    "        -  Final activation function:\n",
    "            - For binary classificaiton, use `activation='sigmoid'`\n",
    "            - For multi classificaiton, use `activation='softmax'`\n",
    "        - For regression tasks, have a single final neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TNk4Uz27wVZc"
   },
   "source": [
    "4. **Compile the model:**\n",
    "    - Specify optimiziers\n",
    "        - `RMSprop`, `SGD`\n",
    "    - specify loss functions\n",
    "        - for binary classification: `'binary_crossentropy'`\n",
    "        - for multi classification: `'categorical_crossentropy'`\n",
    "    - specify metrics\n",
    "        -usually 'acc'\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TNk4Uz27wVZc"
   },
   "source": [
    "5. **Training the model**\n",
    "    - `model.fit(X_train, y_train, epochs=20,batch_size=512,validation_data=(x_val,y_val))`\n",
    "        - Note: if using images with ImageDataGenerator, use `model.fit_generator()`\n",
    "    \n",
    "    - **batches:**\n",
    "        - a set of N samples, processed independently in parallel\n",
    "        - a batch determines how many samples are fed through before back-propagation. \n",
    "        - model only updates after a batch is complete.\n",
    "        - ideally have as large of a batch as your hardware can handle without going out of memory.\n",
    "            - larger batches usually run faster than smaller ones for evaluation/prediction. \n",
    "    - **epoch:**\n",
    "        - arbitrary cutoff / \"one pass over the entire dataset\", useful for logging and periodic evaluation\n",
    "        - when using kera's `model.fit` parameters `validation_data` or `validation_split`, these evaluations run at the end of every epoch.\n",
    "        - Within Keras can add callbacksto be run at the end of an epoch. Examples of these are learning rate changes and model checkpointing (saving).\n",
    "        \n",
    "\n",
    "6. **Evaluation / Predictions**\n",
    "    - To get predicted results:\n",
    "        - `y_hat_test = model.predict(test)`\n",
    "    - To get evaluation metrics:\n",
    "        - `results_test = model.evaluate(test, label_test)`\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TNk4Uz27wVZc"
   },
   "source": [
    "7. **Visualization**\n",
    "    - **`history =  model.fit()` creates history object with .history attribute.**\n",
    "        - `history.history()` returns a dictionary of metrics from each epoch. \n",
    "            - `history.history['loss']` and `history.history['acc']` \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/learn-co-students/dsc-image-classification-with-mlps-online-ds-ft-100719/master/images/Deeper_network.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Image Classification with MLPs - Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T22:19:28.747811Z",
     "start_time": "2020-05-26T22:19:25.119135Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from fsds_100719.imports import *\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "from keras import models, layers,optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense#,Dropout\n",
    "\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T22:39:22.917191Z",
     "start_time": "2020-05-26T22:39:22.648619Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T22:19:29.176916Z",
     "start_time": "2020-05-26T22:19:29.173783Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Print out image shapes and data shapes\n",
    "print(sample_image.shape)\n",
    "print(f\"X_train.shape={X_train.shape}\")\n",
    "print(f\"X_test.shape={X_test.shape}\")\n",
    "print(sample_image.shape[0]*sample_image.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T22:19:29.171671Z",
     "start_time": "2020-05-26T22:19:29.021976Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Visualize random image\n",
    "i = np.random.choice(list(range(len(X_train))))\n",
    "sample_image = X_train[i]\n",
    "sample_label = y_train[i]\n",
    "\n",
    "title = f\"Image #{i}:  Label={sample_label}\"\n",
    "plt.imshow(sample_image, cmap='gray')\n",
    "\n",
    "ax= plt.gca()\n",
    "ax.set(title=title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T22:43:47.016601Z",
     "start_time": "2020-05-26T22:43:47.014145Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## SAVE SHAPES FOR EASY ACCESS LATE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> ***We can interpret these numbers as saying \"X_train consists of 60,000 images that are 28x28\". We'll need to reshape them from (28, 28), a 28x28 matrix, to (784,), a 784-element vector. However, we need to make sure that the first number in our reshape call for both X_train and X_test still correspond to the number of observations we have in each.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T22:19:29.187971Z",
     "start_time": "2020-05-26T22:19:29.184692Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T22:19:29.193501Z",
     "start_time": "2020-05-26T22:19:29.189437Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T22:44:04.824819Z",
     "start_time": "2020-05-26T22:44:04.822643Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#  reshape and convert astype('float32') so that we convert our data from type uint8 to float32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T22:44:08.100714Z",
     "start_time": "2020-05-26T22:44:08.098568Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## normalizing data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Preparing Targets\n",
    "\n",
    "- This is a **Multiclass Classification** problem.\n",
    "    - we need to One-Hot Encode our labels\n",
    "    - `keras.utils.to_categorical`\n",
    "    \n",
    "- For multi classification:\n",
    "    - good final activation function is softmax\n",
    "    - categorical_crossentytropu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T22:19:29.385480Z",
     "start_time": "2020-05-26T22:19:29.381048Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "##Y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T22:44:26.512788Z",
     "start_time": "2020-05-26T22:44:26.510779Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## build network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T22:44:29.845908Z",
     "start_time": "2020-05-26T22:44:29.843798Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## compile model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T22:44:36.675045Z",
     "start_time": "2020-05-26T22:44:36.672984Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Train network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T22:44:52.504737Z",
     "start_time": "2020-05-26T22:44:52.502701Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## visualize training results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T22:44:56.675263Z",
     "start_time": "2020-05-26T22:44:56.673285Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## MODEL 2: Try a bigger model\n",
    "## build network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T22:45:05.828364Z",
     "start_time": "2020-05-26T22:45:05.826230Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## compile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T22:45:16.096908Z",
     "start_time": "2020-05-26T22:45:16.094964Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## fit and visualize results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T22:19:40.198818Z",
     "start_time": "2020-05-26T22:19:40.104637Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Model 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T22:19:45.982403Z",
     "start_time": "2020-05-26T22:19:40.200200Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Sidebar: Augmenting Image Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "### ðŸ’¡ Data Augmentation (not covered in class)\n",
    "- Simplest way to reduce overfitting is to increase the size of the training data.\n",
    "- Difficult to do with large datasets, but can be implemented with images as shown below:\n",
    "- **For augmenting image data:**\n",
    "    - Can alter the images already present in the training data by shifting, shearing, scaling, rotating.<br><br> <img src =\"https://www.dropbox.com/s/9i1hl3quwo294jr/data_augmentation_example.png?raw=1\" width=300>\n",
    "    - This usually provides a big leap in improving the accuracy of the model. It can be considered as a mandatory trick in order to improve our predictions.\n",
    "\n",
    "- **In Keras:**\n",
    "    - `ImageDataGenerator` contains several augmentations available.\n",
    "    - Example below:\n",
    "    \n",
    "```python\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(horizontal flip=True)\n",
    "datagen.fit(train)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of James' Study Group Notes - Section 41-42 (condensed version).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
