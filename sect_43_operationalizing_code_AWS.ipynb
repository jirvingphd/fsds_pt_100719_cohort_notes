{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sect 43: Operationalizing Code & AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- online-ds-pt-100719\n",
    "- 08/06/20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Udemy Course: Deployment of Machine Learning Models](https://www.udemy.com/share/101Y5KAEYbdVdWRXQ=/)\n",
    "- [Amazon Web Services](https://aws.amazon.com/)\n",
    "    - [Getting Started Resource Center](https://aws.amazon.com/getting-started/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **To Manage your account use the AWS Console**\n",
    "    - http://console.aws.amazon.com/\n",
    "    - Sign into console using Root User\n",
    "    \n",
    "- **Resources**\n",
    "    - https://aws.amazon.com/getting-started/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Productionizing Models as a Career Skill\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Many data scientists don't know how to put machine learning models into production.  \n",
    "2. Putting a model into production is a mandatory skill for data scientists at most small to medium-sized companies.\n",
    "3. Being able to productionize models will make you a much more attractive candidate to employers, and give you a competitive advantage!\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-students/dsc-data-science-and-machine-learning-engineering-online-ds-ft-100719/master/images/new-venn-diagram.png\" width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> -  A decade ago, productionizing a machine learning model would have meant building your own web server with something like [Flask](http://flask.pocoo.org/) or [Django](https://www.djangoproject.com/) and hosting somewhere, just like you would with any web app. \n",
    "> - Now, we don't even need to worry about things like server code -- instead, we can use preexisting services from AWS that were created specifically to simplify the process of productionizing machine learning solutions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/fsds_pt_100719_cohort_notes/master/Instructor%20Notebooks/sect_43/image/cloud_PNG27.png\" alt=\"image3\" style = \"width:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives:\n",
    "- define buckets, credentials, and regions in relation to AWS cloud computing services\n",
    "- create an S3 bucket through the AWS console\n",
    "- upload dats to our new S3 bucket\n",
    "- create a Jupyter Notebook Instance through Sagemaker\n",
    "- ensure Sagemaker can access S3 data\n",
    "- clone a git repository into your Sagemaker Jupyter Lab environment\n",
    "\n",
    "## Prework:\n",
    "- sign up for an AWS account\n",
    "- check your email for a confirmation email\n",
    "- click that email\n",
    "\n",
    "If you don't do that, you **_will not be able_** to do this material"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Cloud** computing is an **umbrella** term for many solutions\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/fsds_pt_100719_cohort_notes/master/Instructor%20Notebooks/sect_43/image/umbrella_PNG69231.png\" style = \"width:200px\">\n",
    "\n",
    "### The common theme in cloud computing:\n",
    "#### _Part_ of the solution is not located on premesis, but instead provided by an external _vendor_. \n",
    "\n",
    "\n",
    "<img src=\"https://images.pexels.com/photos/1181354/pexels-photo-1181354.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=750&w=1260\" alt=\"server\" style = \"width:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top three cloud services vendors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/fsds_pt_100719_cohort_notes/master/Instructor%20Notebooks/sect_43//image/cloud_providers.png\" alt=\"provider\" style = \"width:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provides any of the following as a *service*: \n",
    "\n",
    "Can you think of examples of each?\n",
    "- Infrastructure (IaaS)\n",
    "- Software (SaaS)\n",
    "- Platforms (PaaS)\n",
    "\n",
    "![microsoft example](https://miro.medium.com/max/624/1*QmV2VDvgIquNx-Daxcdddw.png)\n",
    "\n",
    "[image sourced from here](https://medium.com/@kumarshivam_66534/a-walk-through-on-iaas-paas-and-saas-7e8a4e4793fb)\n",
    "\n",
    "### Solves many problems:\n",
    "\n",
    "- How can I keep my data secure yet accessible remotely?\n",
    "- How can I pay less for software licenses?\n",
    "- What if I need more server space in the future?\n",
    "- I have more data to analyze than can fit on my computer. What can I do?\n",
    "- My model has taken three days to run. Is there a faster way?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS can be intimidating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/fsds_pt_100719_cohort_notes/master/Instructor%20Notebooks/sect_43//image/aws_offerings.png\" alt=\"offerings\" style = \"width:400px\">\n",
    "\n",
    "### Focus on the last two questions:\n",
    "\n",
    "- How can I keep my data secure yet accessible remotely?\n",
    "- How can I pay less for software licenses?\n",
    "- What if I need more server space in the future?\n",
    "- **I have more data to analyze than can fit on my computer. What can I do?**\n",
    "- **My model has taken three days to run. Is there a faster way?**\n",
    "\n",
    "So we will only be using **S3**, **Sagemaker**, and **IAM**. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/fsds_pt_100719_cohort_notes/master/Instructor%20Notebooks/sect_43//image/aws_focus.png\" alt=\"foci\" style = \"width:90%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions:\n",
    "\n",
    "- Working from a Jupyter notebook locally\n",
    "- Want to keep your analysis in a Jupyter notebook\n",
    "- Store your work on git as well\n",
    "- Not concerned about access or keeping data private\n",
    "- Want the easiest and fastest solution to getting our notebook in the cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get your data to the cloud!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New vocab: Bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://pngimg.com/uploads/bucket/bucket_PNG7777.png\" alt=\"bucket\" style = \"text-align:left;width:200px;float:none\">\n",
    "\n",
    "#### Buckets defined\n",
    "\n",
    "[by PC Mag](https://www.pcmag.com/encyclopedia/term/bucket)\n",
    "\n",
    "> A customer-defined storage area in a cloud-based storage system such as Amazon's S3 or Google Storage. Each bucket can be divided into folders. Customers are not charged for the buckets themselves, only when data reside within them. See S3 cloud storage and Google Storage.\n",
    "\n",
    "#### S3 stands for _Amazon Simple Storage Service_\n",
    "Amazon uses [S3 buckets](https://aws.amazon.com/s3/) for the most general form of object storage.\n",
    "\n",
    "<!---\n",
    "<img src=\"https://cdn.worldvectorlogo.com/logos/aws-s3.svg\"></br>--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New vocab: Credential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![credentials](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRvaB5OvGWguYHBlVyagwofOP9kX0h5HqtbcIa02MyAVs_XS90McA&s)\n",
    "\n",
    "#### Credentials Defined:\n",
    "\n",
    "[From AWS](https://docs.aws.amazon.com/general/latest/gr/aws-security-credentials.html)\n",
    "\n",
    "> When you interact with AWS, you specify your AWS security credentials to verify who you are and whether you have permission to access the resources that you are requesting. AWS uses the security credentials to authenticate and authorize your requests.\n",
    "\n",
    ">For example, if you want to download a specific file from an Amazon Simple Storage Service (Amazon S3) bucket, your credentials must allow that access. If your credentials aren't authorized to download the file, AWS denies your request.\n",
    "\n",
    "#### Our approach to credentials:\n",
    "\n",
    "Make everything public. </br>\n",
    "But we will still have to work with **IAM** a bit to make things talk to each other. \n",
    "\n",
    "<img src=\"https://a0.awsstatic.com/libra-css/images/logos/aws_logo_smile_1200x630.png\" alt=\"aws\" style =\"text-align:center;width:250px;float:none\" ></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New vocab: Region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.concurrencylabs.com/img/posts/9-choose-region-wisely/choose-your-aws-region.png\" wiodth=30%></br>\n",
    "\n",
    "#### Regions Defined:\n",
    "[from AWS documentation](https://aws.amazon.com/about-aws/global-infrastructure/regions_az/):\n",
    ">AWS has the concept of a Region, which is a physical location around the world where we cluster data centers. We call each group of logical data centers an Availability Zone. Each AWS Region consists of multiple, isolated, and physically separate AZ's within a geographic area...\n",
    "\n",
    ">Each AZ has independent power, cooling, and physical security and is connected via redundant, ultra-low-latency networks. AWS customers focused on high availability can design their applications to run in multiple AZ's to achieve even greater fault-tolerance. AWS infrastructure Regions meet the highest levels of security, compliance, and data protection.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/fsds_pt_100719_cohort_notes/master/Instructor%20Notebooks/sect_43//image/aws_regions_facts.png\" alt=\"aws_regions\" style =\"text-align:center;width:500px;float:none\" ></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---#### Task:\n",
    "- go to the website where I sourced the regions map [https://aws.amazon.com/about-aws/global-infrastructure/](https://aws.amazon.com/about-aws/global-infrastructure/)\n",
    "- find the two newest AWS regions --->\n",
    "\n",
    "__Why do we care?__\n",
    "- Each time you create a new \"service\" in AWS, you need to define its region.\n",
    "- Each region is a separate geographic area and is completely independent\n",
    "- Each Amazon region is designed to be completely isolated from the other regions & helps achieve the greatest possible fault tolerance and stability\n",
    "- Communication between regions is across the public Internet and appropriate measures should be taken to protect the data using encryption\n",
    "- Data transfer between regions is charged at the Internet data transfer rate for both the sending and the receiving instance\n",
    "- Resources arenâ€™t replicated across regions unless done explicitly\n",
    "\n",
    "Here are some real factors impacted by your choice of region:\n",
    "- Latency \n",
    "- Cost\n",
    "- Legal Compliance\n",
    "- Features \n",
    "\n",
    "**For these exercises:** </br>\n",
    "We are going to use the Northern Virgina region, identified as `us-east-1`\n",
    "(if in a different region than Virginia, please use your relevant region!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Sagemaker\n",
    "<img src=\"https://d2908q01vomqb2.cloudfront.net/77de68daecd823babbb58edb1c8e14d7106e83bb/2018/04/24/SageMaker.jpg\" alt=\"sagemaker\" style =\"text-align:center;width:250px;float:none\" ></br>\n",
    "\n",
    "Amazon Sagemaker includes many services, but we will only be using the [Amazon Sagemaker Notebook Instances](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html)\n",
    "\n",
    ">An Amazon SageMaker notebook instance is a fully managed ML compute instance running the Jupyter Notebook App. Amazon SageMaker manages creating the instance and related resources. Use Jupyter notebooks in your notebook instance to prepare and process data, write code to train models, deploy models to Amazon SageMaker hosting, and test or validate your models.\n",
    "\n",
    "This means AWS creates _containerized_ environments on [_ec2_](https://aws.amazon.com/ec2/) instances from which you can launch higher powered Jupyter Notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walkthrough: Getting Started with AWS & SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **SAVE THESE RESOURCES:**\n",
    "    - **[Learn Lesson for Setting Up AWS: The AWS Ecosystem](https://learn.co/tracks/module-4-data-science-career-2-1/big-data-deep-learning-and-natural-language-processing/section-43-operationalizing-code-and-aws/the-aws-ecosystem)**\n",
    "    - [Learn Lesson: Productionizing Models with SageMaker](https://learn.co/tracks/module-4-data-science-career-2-1/big-data-deep-learning-and-natural-language-processing/section-43-operationalizing-code-and-aws/productionizing-a-model-with-docker-and-sagemaker)\n",
    "        - [Lesson Repo](https://github.com/learn-co-students/dsc-productionizing-models-with-sagemaker-online-ds-pt-100719)\n",
    "\n",
    "    - **[Official SageMaker Tutorial](https://github.com/aws-samples/amazon-sagemaker-keras-text-classification)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/fsds_100719_cohort_notes/master/images/awscloud.png\">\n",
    "\n",
    "> * AWS is a **_Cloud-Computing Platform_** which we can use for a variety of use cases in data science.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sign up for AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **[Follow Learn lesson steps to set up account](https://learn.co/tracks/data-science-career-v2/module-6-natural-language-processing-and-deep-learning/section-50-operationalizing-code-and-aws/the-aws-ecosystem)**\n",
    "\n",
    "- [Amazon Web Services](https://aws.amazon.com/)\n",
    "\n",
    "AWS has data centers all over the world, and they are **not** interchangeable when it comes to your projects. Click on the \"Region\" tab in the top right corner of the navigation bar, and you should see a dropdown of all the different data centers you can choose from. It is **_very important_** that you always choose the same region to connect to with your projects.\n",
    "\n",
    "- Create an AWS account\n",
    "- Sign into console using Root User"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***SageMaker is a platform created by Amazon to centralize all the various services related to Data Science and Machine Learning. If you're a data scientist working on AWS, chances are that you'll be spending most (if not all) of your time in SageMaker getting things done.***\n",
    "\n",
    "\n",
    "> * Amazon has centralized all of the major data science services inside **_Amazon SageMaker_**. SageMaker provides numerous services for things such as:\n",
    "    * Data Labeling\n",
    "    * Cloud-based Notebooks\n",
    "    * Training and Model Tuning\n",
    "    * Inference\n",
    "    \n",
    "#### SageMaker Components\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-students/dsc-introduction-to-aws-sagemaker-online-ds-ft-100719/master/images/use_cases.png\">\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Productionizing Models with SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When productionizing a machine learning model using AWS, you'll typically use the following workflow:\n",
    "\n",
    "1. Explore and preprocess data\n",
    "2. Build SageMaker container (Docker)\n",
    "3. Test training and inference code on your local machine \n",
    "4. Train and deploy model with SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps for Deploying A Model (from lesson)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Complete AWS training notebooks\n",
    "- https://github.com/aws-samples/amazon-sagemaker-keras-text-classification\n",
    "- Do labs 1,2,& 3\n",
    "    - These cover all of the set up required\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Build and Register the container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In Learn lesson repo, there is a `container` folder that has Docker images\n",
    "\n",
    "- The Code Below uses the container folder contens to create and register the docker image needed on AWS.\n",
    "- It is best to \"upload this notebook into your notebook to your AWS Jupyter environment\n",
    "\n",
    "\n",
    "\n",
    "\"After you have successfully uploaded this notebook, if you are asked to choose a kernel, use the same kernel which runs the `sagemaker_keras_text_classification.ipynb` notebook. \n",
    "\n",
    "\n",
    "> NOTE: If you deactivated this process (stopped your Jupyter instance, like _Step 8_ below) and then came back to continue, you'll need to go back and start from [Lab 2](https://github.com/aws-samples/amazon-sagemaker-keras-text-classification#lab-2-building-the-sagemaker-tensorflow-container), because you need the Docker instance running in order to run the following cells. \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#####  Code for Building/Registering containers (reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```ython\n",
    "%%sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=sagemaker-keras-text-classification\n",
    "\n",
    "cd container\n",
    "\n",
    "chmod +x sagemaker_keras_text_classification/train\n",
    "chmod +x sagemaker_keras_text_classification/serve\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "# On a SageMaker Notebook Instance, the docker daemon may need to be restarted in order\n",
    "# to detect your network configuration correctly.  (This is a known issue.)\n",
    "if [ -d \"/home/ec2-user/SageMaker\" ]; then\n",
    "  sudo service docker restart\n",
    "fi\n",
    "\n",
    "docker build  -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 2. Setting up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "Once we've created the container, we'll need to set up the environment. The cell below contains more boilerplate code, which is used to handle a couple sticking points in order to set up the environment. \n",
    "\n",
    "\n",
    "```python \n",
    "# S3 prefix\n",
    "prefix = 'sagemaker-keras-text-classification'\n",
    "\n",
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 3. Creating the Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now that we've created the container and set up our environment, the next step is to create a SageMaker session. \n",
    "```python\n",
    "import sagemaker as sage\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sess = sage.Session()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Upload the Data for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps 4 and 5 are where you'll add the code unique to your project.In this step, make sure have a folder called `'data'` that contains the data you'll be working with. The actual structure of the data is up to you, as you'll be the one consuming it to train your model in step 5. \n",
    "\n",
    "```python \n",
    "WORK_DIRECTORY = 'data'\n",
    "\n",
    "data_location = sess.upload_data(WORK_DIRECTORY, key_prefix=prefix)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Fitting the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is the part where you'll do the brunt of the work. \n",
    "- You'll train your own model on the data you uploaded in the previous step. \n",
    "\n",
    "- Note that in the sample code below, the first 3 lines are boilerplate code. \n",
    "    \n",
    "- The actual creation and training of the model happen on the last two lines of code, where `tree` is instantiated and used. \n",
    "\n",
    "> **_NOTE_**: You may have noticed that the code in the cell below uses an `Estimator` from `sage` (which is just an alias we set for `sagemaker` up above), the SageMaker library for python, rather than a model from scikit-learn. The `sagemaker` library contains a massive amount of useful models that we can use directly. Under the hood, the `sagemaker` library wraps in the same open-source frameworks such as scikit-learn, Keras, and TensorFlow that you're used to using. The code below is an example from AWS of how to use one of their `Estimator` objects for training. If you read the output of the cell when you run everything, you'll notice that much of it is warning messages or other printouts from sklearn and keras!\n",
    "\n",
    "For more information on the models and other tools included in the aws sagemaker library, check out [Amazon SageMaker Python SDK Documentation](https://sagemaker.readthedocs.io/en/stable/)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = sess.boto_session.region_name\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/sagemaker-keras-text-classification'.format(account, region)\n",
    "\n",
    "tree = sage.estimator.Estimator(image,\n",
    "                       role, 1, 'ml.c5.2xlarge',\n",
    "                       output_path=\"s3://{}/output\".format(sess.default_bucket()),\n",
    "                       sagemaker_session=sess)\n",
    "\n",
    "tree.fit(data_location)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Deploying the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This is where the magic happens -- we have a trained model, and now we need to actually **_deploy_** it to the AWS cloud! Notice how during this step, we include a `json_serializer` -- this is so that the model can serialize and deserialize data as needed when taking data in as input. \n",
    "\n",
    "Running the cell below will create an endpoint for your trained model. \n",
    "\n",
    "```python\n",
    "from sagemaker.predictor import json_serializer\n",
    "predictor = tree.deploy(1, 'ml.t2.medium', serializer=json_serializer)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Cleanup (IMPORTANT!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As a final step for this exercise, **be sure to run the following line of code to delete your endpoint!** Although you are running this lab on the free tier, you don't want to leave it running, because that is how costs can accrue. Run the cell below to delete your endpoint.\n",
    "\n",
    "```python \n",
    "sess.delete_endpoint(predictor.endpoint)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Deactivate Everything in AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In AWS, you pay for usage. **This means that anything left running is being used.  While the AWS Free Tier we've signed up for allows us to do small things for free for prototyping or learning, leaving some things running may take us past the usage limits for the AWS Free Tier.** In order to avoid getting charged, you'll need to do the following steps: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 8.1: Deactivate the notebook in Sagemaker\n",
    "\n",
    "First, you'll need to deactivate your notebook in SageMaker. When you enter the SageMaker platform, you'll always see the number of open notebooks you have up and running highlighted in green under the 'Recent Activity' section. \n",
    "\n",
    "<img src='https://raw.githubusercontent.com/jirvingphd/dsc-productionizing-models-with-sagemaker-online-ds-ft-100719/master/images/create-notebook-7.png'>\n",
    "\n",
    "\n",
    "To deactivate a running notebook, select it and then go to the 'Actions' tab and select stop. Stopping the notebook instance will take a minute or two. You'll know it's done when you see the 'Status' column for the highlighted notebook change from 'InService' to 'Stopped'. \n",
    "\n",
    "<img src='https://raw.githubusercontent.com/jirvingphd/dsc-productionizing-models-with-sagemaker-online-ds-ft-100719/master/images/create-notebook-8.png'>\n",
    "\n",
    "### 8.2: Keep an Eye on Cost Explorer\n",
    "\n",
    "As you've seen from this lab, getting a handle on all the different services in AWS and how they interact with one another can be a bit daunting until you have some experience. It's very important that you don't leave services running when you aren't using them, because you will be charged for that. If you want to make sure that you haven't left anything running, the easiest thing to do is to check the 'Costs Explorer' page inside AWS. You can find this by searching for 'AWS Cost Explorer' in the search bar on the main page for the AWS Console. This service will show you what your usage is for everything that you can be charged for. It's quite intuitive and easy to use, and should make it easy to see if you are accruing charges because you left something running that you didn't realize. If you left something running that you aren't aware of, you'll see it here -- once you've noticed it, just navigate to the service in question and deactivate it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Getting  files into S3 via the AWS console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "(Will start here, will do it through code later)\n",
    "\n",
    "### Steps:\n",
    "- have an amazon account\n",
    "- open s3\n",
    "- make buckets\n",
    "- upload files\n",
    "- set all permissions to open to the public\n",
    "\n",
    "#### **Tasks**:\n",
    "- Create bucket named something unique (eg `ae-fis1118-demo` or `flatiron-apm-example`)\n",
    "- Upload the `wine.csv` and the `carseats.csv` to that bucket\n",
    "- Make all things public (AWS will ask you if YOU ARE SURE a million times, you are sure)\n",
    "- Send your url for `wine.csv` to a friend and have them import it using `pandas.read_csv()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Setting up a Jupyter Notebook in Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Have an amazon account\n",
    "- Ideally, have s3 buckets created first\n",
    "- Find sagemaker in the offerings\n",
    "- Create a notebook instance (you can only **have two** on this tier)\n",
    "- When setting it up, set up user profille permissions to have full access to Sagemaker AND S3 (ignore most other settings)\n",
    "  - this might mean setting up a new admin \"Group\" in IAM, we will go through that together\n",
    "- Wait for it to be created, will take a few minutes, should say \"pending\"\n",
    "- Open Jupyter Lab\n",
    "- Use git symbol above file directory to git clone a file into the environment\n",
    "- Commit your work the same way you would with any other project\n",
    "- **TURN OFF THE RESOURCE WHEN DONE**\n",
    "\n",
    "#### **Tasks**:\n",
    "- go to Sagemaker within AWS\n",
    "- create a notebook instance\n",
    "- set user permissions to have access to Sagemaker & S3, wait for it to be created\n",
    "- go to github and create your own fork of this directory: https://github.com/learn-co-students/dsc-accessing-data-in-aws/\n",
    "- go back to your jupyterlab, use the git symbol to clone in your forked version of the the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
