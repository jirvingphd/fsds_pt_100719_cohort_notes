{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ByfUTZodQsND"
   },
   "source": [
    "# Sect 40: Neural Networks - Intro to Deep Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 05/21/20\n",
    "- online-ds-pt-100719"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions/ Comments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Start By Discussing Biological Neural Networks (powerpoint)\n",
    "    - `Repo Folder > references >bio_neural_networks.pptx`\n",
    "- Connect back to introduction from Learn\n",
    "- Discuss the steps involved in training an ANN\n",
    "    - Forward propagation\n",
    "    - Back\n",
    "- Demonstrate / play with Neural Network with Tensorflow Playground\n",
    "- Activity: Finding Trump with Keras (if there's time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uceu7mnytDno"
   },
   "source": [
    "# Artificial Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlvqq-TbP5zO"
   },
   "source": [
    "    \n",
    "- **The purpose of a neural network is to model $\\hat y \\approx y$ by minimizing loss/cost functions using gradient descent.**\n",
    "\n",
    "- Neural networks are very good with unstructured data. (images, audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-introduction-to-neural-networks-online-ds-ft-100719/master/images/new_first_network_num.png\" width=50%%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlvqq-TbP5zO"
   },
   "source": [
    "- **Networks are comprised of sequential layers of neurons/nodes.**\n",
    "    - Each neuron applies a **linear transformation** and an **activation function** and outputs its results to all neurons in the next layer.\n",
    "    - Minimizing Loss functions by adjusting parameters (weights and bias) of each connection using gradient descent (forward and back propagation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlvqq-TbP5zO"
   },
   "source": [
    "- **Activation functions** control the output of a neuron.($\\hat y =f_{activation}(x)$ )\n",
    "    - Most basic activation function is sigmoid functin ($\\hat y =\\sigma(x)$)\n",
    "    - Choice of activation function controls the size/range of the output.\n",
    "- **Linear transformations** ( $z = w^T x + b$ ) are used control the output of the activation function .\n",
    "    - where $w^T $ is the weight(/coefficient), $x$ is the input, and  $b$ is a bias. \n",
    "        - weights: \n",
    "        - bias:\n",
    "        \n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-40-02-introduction-to-neural-networks-online-ds-ft-021119/master/figures/log_reg.png\">\n",
    "\n",
    "\n",
    "\n",
    "- **Loss functions** ($\\mathcal{L}(\\hat y, y) $)  measure inconsistency between predicted ($\\hat y$) and actual $y$\n",
    "    - will be optimized using gradient descent\n",
    "    - defined over 1 traning sample\n",
    "- **Cost functions** takes the average loss over all of the samples.\n",
    "    - $J(w,b) = \\displaystyle\\frac{1}{l}\\displaystyle\\sum^l_{i=1}\\mathcal{L}(\\hat y^{(i)}, y^{(i)})$\n",
    "    - where $l$ is the number of samples\n",
    "\n",
    "\n",
    "- **Forward propagation** is the calculating  loss and cost functions.\n",
    "- **Back propagation** involves using gradient descent to update the values for  $w$ and $b$.\n",
    "    - $w := w- \\alpha\\displaystyle \\frac{dJ(w)}{dw}$ <br><br>\n",
    "    - $b := b- \\alpha\\displaystyle \\frac{dJ(b)}{db}$\n",
    "\n",
    "        - where $ \\displaystyle \\frac{dJ(w)}{dw}$ and $\\displaystyle \\frac{dJ(b)}{db}$ represent the *slope* of the function $J$ with respect to $w$ and $b$ respectively\n",
    "        - $\\alpha$ denote the *learning rate*. \n",
    "        \n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/fsds_100719_cohort_notes/master/images/neural_network_steps.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Note On Shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inputs:\n",
    "    - $n$: Number of inputs (columns) in the feature vector \n",
    "    - $l$: Number of items (rows) in the training set \n",
    "    - $m$: Number of items (rows) in the test set\n",
    "    \n",
    "- Input X:\n",
    "    - Will have shape $n$ x $l$ (number of features x number of training data points/rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlvqq-TbP5zO"
   },
   "source": [
    "## Activation Functions (will call $f_a$ here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlvqq-TbP5zO"
   },
   "source": [
    "- **sigmoid:**<br><img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-40-04-deeper-neural-networks-online-ds-ft-021119/master/index_files/index_33_1.png\" width=200>\n",
    "    - $ f_a=\\dfrac{1}{1+ \\exp(-z)}$\n",
    "    - outputs 0 to +1\n",
    "    \n",
    "- **tanh (hyperbolic tan):**<br><img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-40-04-deeper-neural-networks-online-ds-ft-021119/master/index_files/index_36_1.png\" width=200> <br>(Note:title is incorrect)\n",
    "    - $f_a = =\\dfrac{\\exp(z)- \\exp(-z)}{\\exp(z)+ \\exp(-z)}$\n",
    "    - outputs -1 to +1\n",
    "    - Generally works well in intermediate layers\n",
    "    - one of most popular functions\n",
    "    \n",
    "- **arctan**<br><img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-40-04-deeper-neural-networks-online-ds-ft-021119/master/index_files/index_40_1.png\" width=200>\n",
    "    -  similar qualities as tanh, but slope is more gentle than tanh\n",
    "    - outputs ~ 1.6 to 1.6\n",
    "    \n",
    "-  **Rectified Linear Unit (relu):**<br><img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-40-04-deeper-neural-networks-online-ds-ft-021119/master/index_files/index_43_1.png\" width=200>\n",
    "    - most popular activation function\n",
    "    - Activation is exactly 0 when Z <0\n",
    "    - Makes taking directives slightly cumbersome\n",
    "    - $f_a=\\max(0,z)$\n",
    "- **leaky_relu:**<br><img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-40-04-deeper-neural-networks-online-ds-ft-021119/master/index_files/index_46_1.png\" width=200>\n",
    "    -  altered version of relu where the activatiom is slightly negative when $z<0$\n",
    "    - $f_a=\\max(0.001*z,z)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Finding Trump with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Repo Folder > labs_from_class > sect_40_neural_networks`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlvqq-TbP5zO"
   },
   "source": [
    "### Using the chain rule for updating parameters with sigmoid activation function example:\n",
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-04-40-02-introduction-to-neural-networks-online-ds-ft-021119/master/figures/log_reg_deriv.png\" >\n",
    "- $\\displaystyle \\frac{dJ(w,b)}{dw_i} = \\displaystyle\\frac{1}{l}\\displaystyle\\sum^l_{i=1} \\frac{d\\mathcal{L}(\\hat y^{(i)}, y^{(i)})}{dw_i}$\n",
    " \n",
    " \n",
    "- For each training sample $1,...,l$ you'll need to compute:\n",
    "\n",
    "    - $ z^{(i)} = w^T x^ {(i)} +b $\n",
    "\n",
    "    - $\\hat y^{(i)} = \\sigma (z^{(i)})$\n",
    "\n",
    "    - $dz^{(i)} = \\hat y^{(i)}- y^{(i)}$\n",
    "\n",
    "- Then, you'll need to make update:\n",
    "\n",
    "    - $J_{+1} = - [y^{(i)} \\log (\\hat y^{(i)}) + (1-y^{(i)}) \\log(1-\\hat y^{(i)})$ (for the sigmoid function)\n",
    "\n",
    "    - $dw_{1, +1}^{(i)} = x_1^{(i)} * dz^{(i)}$\n",
    "\n",
    "    - $dw_{2, +1}^{(i)} = x_2^{(i)} * dz^{(i)}$\n",
    "\n",
    "    - $db_{+1}^{(i)} =  dz^{(i)}$\n",
    "\n",
    "    - $\\dfrac{J}{m}$, $\\dfrac{dw_1}{m}$, $\\dfrac{dw_1}{m}$, $\\dfrac{db}{m}$\n",
    "\n",
    "- After that, update: \n",
    "\n",
    "    $w_1 := w_1 - \\alpha dw_1$\n",
    "\n",
    "    $w_2 := w_2 - \\alpha dw_2$\n",
    "\n",
    "    $b := b - \\alpha db$\n",
    "\n",
    "    repeat until convergence!\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T22:40:27.163071Z",
     "start_time": "2020-05-21T22:40:27.159937Z"
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# # q\n",
    "# def sigmoid(x, derivative=False):\n",
    "#     f = 1 / (1 + np.exp(-x))\n",
    "#     if (derivative == True):\n",
    "#         return f * (1 - f)\n",
    "#     return f\n",
    "\n",
    "# def tanh(x, derivative=False):\n",
    "#     f = np.tanh(x)\n",
    "#     if (derivative == True):\n",
    "#         return (1 - (f ** 2))\n",
    "#     return np.tanh(x)\n",
    "\n",
    "# def relu(x, derivative=False):\n",
    "#     f = np.zeros(len(x))\n",
    "#     if (derivative == True):\n",
    "#         for i in range(0, len(x)):\n",
    "#             if x[i] > 0:\n",
    "#                 f[i] = 1  \n",
    "#             else:\n",
    "#                 f[i] = 0\n",
    "#         return f\n",
    "#     for i in range(0, len(x)):\n",
    "#         if x[i] > 0:\n",
    "#             f[i] = x[i]  \n",
    "#         else:\n",
    "#             f[i] = 0\n",
    "#     return f\n",
    "\n",
    "# def leaky_relu(x, leakage = 0.05, derivative=False):\n",
    "#     f = np.zeros(len(x))\n",
    "#     if (derivative == True):\n",
    "#         for i in range(0, len(x)):\n",
    "#             if x[i] > 0:\n",
    "#                 f[i] = 1  \n",
    "#             else:\n",
    "#                 f[i] = leakage\n",
    "#         return f\n",
    "#     for i in range(0, len(x)):\n",
    "#         if x[i] > 0:\n",
    "#             f[i] = x[i]  \n",
    "#         else:\n",
    "#             f[i] = x[i]* leakage\n",
    "#     return f\n",
    "\n",
    "# def arctan(x, derivative=False):\n",
    "#     if (derivative == True):\n",
    "#         return 1/(1+np.square(x))\n",
    "#     return np.arctan(x)\n",
    "\n",
    "\n",
    "\n",
    "# def plot_activation(fn):\n",
    "#     z = np.arange(-10, 10, 0.2)\n",
    "#     y = fn(z)\n",
    "#     dy = fn(z, derivative=True)\n",
    "#     fig,ax=plt.subplots(figsize=(6,4))\n",
    "#     ax.set_title(f'{fn.__name__}')\n",
    "#     ax.set(xlabel='Input',ylabel='Output')\n",
    "#     ax.axhline(color='gray', linewidth=1,)\n",
    "#     ax.axvline(color='gray', linewidth=1,)\n",
    "#     ax.plot(z, y, 'r', label='original (y)')\n",
    "#     ax.plot(z, dy, 'b', label='derivative (dy)')\n",
    "#     ax.legend();\n",
    "#     plt.show()\n",
    "# ## Plot activation functions\n",
    "# act_funcs = [sigmoid,tanh,arctan,relu,leaky_relu]\n",
    "# [plot_activation(fn) for fn in act_funcs]\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "My Flatiron Bootcamp Notes - Mod 4.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "325px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
